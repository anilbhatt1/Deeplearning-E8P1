{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HowoVBNLCBfS",
        "outputId": "b4897d77-295b-43b8-ad8a-26677b16f0a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Mar 13 11:50:02 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   53C    P0    25W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms\n",
        "import torchvision"
      ],
      "metadata": {
        "id": "VgvmFNNCCCgl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W4OMBJ6mxsjB"
      },
      "outputs": [],
      "source": [
        "DATA_DIR='./data'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WZejmg6qxsjB",
        "outputId": "5d975769-606b-4fc6-c20e-a22e84cf0d73"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "device: cuda\n"
          ]
        }
      ],
      "source": [
        "DEVICE = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "print(\"device:\", DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "171uPlqXxsjD"
      },
      "outputs": [],
      "source": [
        "class Residual(nn.Module):\n",
        "    def __init__(self, *layers):\n",
        "        super().__init__()\n",
        "        self.residual = nn.Sequential(*layers)\n",
        "        self.gamma = nn.Parameter(torch.zeros(1))\n",
        "    \n",
        "    def forward(self, x):\n",
        "        print(f'Residual x.shape : {x.shape}')        \n",
        "        print(f'Residual self.residual(x).shape : {self.residual(x).shape}')\n",
        "        out = x + self.gamma * self.residual(x)\n",
        "        print(f'Residual out.shape : {out.shape}') \n",
        "        return x + self.gamma * self.residual(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "39-F1AytxsjD"
      },
      "outputs": [],
      "source": [
        "class LayerNormChannels(nn.Module):\n",
        "    def __init__(self, channels):\n",
        "        super().__init__()\n",
        "        self.norm = nn.LayerNorm(channels)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = x.transpose(1, -1)\n",
        "        x = self.norm(x)\n",
        "        x = x.transpose(-1, 1)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bGt11ksHxsjE"
      },
      "outputs": [],
      "source": [
        "class SelfAttention2d(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, head_channels, shape):\n",
        "        super().__init__()\n",
        "        self.heads = out_channels // head_channels\n",
        "        self.head_channels = head_channels\n",
        "        self.scale = head_channels**-0.5\n",
        "        \n",
        "        self.to_keys = nn.Conv2d(in_channels, out_channels, 1)\n",
        "        self.to_queries = nn.Conv2d(in_channels, out_channels, 1)\n",
        "        self.to_values = nn.Conv2d(in_channels, out_channels, 1)\n",
        "        self.unifyheads = nn.Conv2d(out_channels, out_channels, 1)\n",
        "        \n",
        "        height, width = shape\n",
        "        self.pos_enc = nn.Parameter(torch.Tensor(self.heads, (2 * height - 1) * (2 * width - 1)))\n",
        "        self.register_buffer(\"relative_indices\", self.get_indices(height, width))\n",
        "    \n",
        "    def forward(self, x):\n",
        "        b, _, h, w = x.shape\n",
        "        print(f'SelfAttention2d fwd : b, _, h, w : {x.shape}')\n",
        "        \n",
        "        keys = self.to_keys(x).view(b, self.heads, self.head_channels, -1)\n",
        "        values = self.to_values(x).view(b, self.heads, self.head_channels, -1)\n",
        "        queries = self.to_queries(x).view(b, self.heads, self.head_channels, -1)\n",
        "        print(f'SelfAttention2d fwd : k v q shapes : {keys.shape} , {values.shape}, {queries.shape}')\n",
        "        \n",
        "        att = keys.transpose(-2, -1) @ queries\n",
        "        print(f'SelfAttention2d fwd att.shape: {att.shape}')\n",
        "        \n",
        "        indices = self.relative_indices.expand(self.heads, -1)\n",
        "        print(f'SelfAttention2d fwd indices.shape: {indices.shape}, self.relative_indices.shape : {self.relative_indices.shape}, self.heads : {self.heads}')\n",
        "        rel_pos_enc = self.pos_enc.gather(-1, indices)\n",
        "        print(f'SelfAttention2d fwd self.pos_enc.shape : {self.pos_enc.shape}, rel_pos_enc.shape : {rel_pos_enc.shape}')\n",
        "        rel_pos_enc = rel_pos_enc.unflatten(-1, (h * w, h * w))\n",
        "        print(f'SelfAttention2d fwd flat rel_pos_enc : {rel_pos_enc.shape}')\n",
        "        \n",
        "        att = att * self.scale + rel_pos_enc\n",
        "        print(f'SelfAttention2d fwd attn.shape after adding rel_pos_enc : {att.shape}')\n",
        "        att = F.softmax(att, dim=-2)\n",
        "        \n",
        "        out = values @ att\n",
        "        print(f'SelfAttention2d fwd out.shape : {out.shape}')\n",
        "        out = out.view(b, -1, h, w)\n",
        "        print(f'SelfAttention2d fwd out.view(b, -1, h, w) : {out.shape}')\n",
        "        out = self.unifyheads(out)\n",
        "        print(f'SelfAttention2d fwd self.unifyheads(out): {out.shape}')\n",
        "        return out\n",
        "    \n",
        "    @staticmethod\n",
        "    def get_indices(h, w):\n",
        "        y = torch.arange(h, dtype=torch.long)\n",
        "        x = torch.arange(w, dtype=torch.long)\n",
        "        \n",
        "        y1, x1, y2, x2 = torch.meshgrid(y, x, y, x, indexing='ij')\n",
        "        indices = (y1 - y2 + h - 1) * (2 * w - 1) + x1 - x2 + w - 1\n",
        "        print(f'SelfAttention2d get_indices indices : {indices.shape}')\n",
        "        indices = indices.flatten()\n",
        "        print(f'SelfAttention2d get_indices flat indices : {indices.shape}')\n",
        "        \n",
        "        return indices"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cvz8Ro-_xsjF"
      },
      "source": [
        "![img](https://i.stack.imgur.com/8Mbig.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qaslNgohxsjG"
      },
      "outputs": [],
      "source": [
        "class FeedForward(nn.Sequential):\n",
        "    def __init__(self, in_channels, out_channels, mult=4):\n",
        "        hidden_channels = in_channels * mult\n",
        "        super().__init__(\n",
        "            nn.Conv2d(in_channels, hidden_channels, 1),\n",
        "            nn.GELU(),\n",
        "            nn.Conv2d(hidden_channels, out_channels, 1)   \n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VOLfx7w9xsjH"
      },
      "outputs": [],
      "source": [
        "class TransformerBlock(nn.Sequential):\n",
        "    def __init__(self, channels, head_channels, shape, p_drop=0.):\n",
        "        super().__init__(\n",
        "            Residual(\n",
        "                LayerNormChannels(channels),\n",
        "                SelfAttention2d(channels, channels, head_channels, shape),\n",
        "                nn.Dropout(p_drop)\n",
        "            ),\n",
        "            Residual(\n",
        "                LayerNormChannels(channels),\n",
        "                FeedForward(channels, channels),\n",
        "                nn.Dropout(p_drop)\n",
        "            )\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DcKbq_4OxsjH"
      },
      "outputs": [],
      "source": [
        "class TransformerStack(nn.Sequential):\n",
        "    def __init__(self, num_blocks, channels, head_channels, shape, p_drop=0.):\n",
        "        layers = [TransformerBlock(channels, head_channels, shape, p_drop) for _ in range(num_blocks)]\n",
        "        super().__init__(*layers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f4L_qq4bxsjH"
      },
      "outputs": [],
      "source": [
        "class ToPatches(nn.Sequential):\n",
        "    def __init__(self, in_channels, channels, patch_size, hidden_channels=32):\n",
        "        super().__init__()\n",
        "        self.tp1 = nn.Sequential(nn.Conv2d(in_channels, hidden_channels, 3, padding=1),\n",
        "                                 nn.GELU())\n",
        "        self.tp2 = nn.Conv2d(hidden_channels, channels, patch_size, stride=patch_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        print(f'ToPatches x.shape : {x.shape}')\n",
        "        out = self.tp1(x)\n",
        "        print(f'ToPatches self.tp1 output : {out.shape}')\n",
        "        out = self.tp2(out)\n",
        "        print(f'ToPatches self.tp2 output : {out.shape}')\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8pLs0h3HxsjI"
      },
      "outputs": [],
      "source": [
        "class AddPositionEmbedding(nn.Module):\n",
        "    def __init__(self, channels, shape):\n",
        "        super().__init__()\n",
        "        self.pos_embedding = nn.Parameter(torch.Tensor(channels, *shape))\n",
        "    \n",
        "    def forward(self, x):\n",
        "        print(f'AddPositionEmbedding x.shape : {x.shape}, self.pos_embedding.shape : {self.pos_embedding.shape}')\n",
        "        return x + self.pos_embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_KKeZVf1xsjI"
      },
      "outputs": [],
      "source": [
        "class ToEmbedding(nn.Sequential):\n",
        "    def __init__(self, in_channels, channels, patch_size, shape, p_drop=0.):\n",
        "        super().__init__(\n",
        "            ToPatches(in_channels, channels, patch_size),\n",
        "            AddPositionEmbedding(channels, shape),\n",
        "            nn.Dropout(p_drop)\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wYpeuAVWxsjI"
      },
      "outputs": [],
      "source": [
        "class Head(nn.Sequential):\n",
        "    def __init__(self, in_channels, classes, p_drop=0.):\n",
        "        super().__init__(\n",
        "            LayerNormChannels(in_channels),\n",
        "            nn.GELU(),\n",
        "            nn.AdaptiveAvgPool2d(1),\n",
        "            nn.Flatten(),\n",
        "            nn.Dropout(p_drop),\n",
        "            nn.Linear(in_channels, classes)\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qwEBJV4IxsjJ"
      },
      "outputs": [],
      "source": [
        "class ViT(nn.Sequential):\n",
        "    def __init__(self, classes, image_size, channels, head_channels, num_blocks, patch_size,\n",
        "                 in_channels=3, emb_p_drop=0., trans_p_drop=0., head_p_drop=0.):\n",
        "        reduced_size = image_size // patch_size\n",
        "        shape = (reduced_size, reduced_size)\n",
        "        super().__init__(\n",
        "            ToEmbedding(in_channels, channels, patch_size, shape, emb_p_drop),\n",
        "            TransformerStack(num_blocks, channels, head_channels, shape, trans_p_drop),\n",
        "            Head(channels, classes, head_p_drop)\n",
        "        )\n",
        "        self.reset_parameters()\n",
        "    \n",
        "    def reset_parameters(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
        "                nn.init.kaiming_normal_(m.weight)\n",
        "                if m.bias is not None: nn.init.zeros_(m.bias)\n",
        "            elif isinstance(m, nn.LayerNorm):\n",
        "                nn.init.constant_(m.weight, 1.)\n",
        "                nn.init.zeros_(m.bias)\n",
        "            elif isinstance(m, AddPositionEmbedding):\n",
        "                nn.init.normal_(m.pos_embedding, mean=0.0, std=0.02)\n",
        "            elif isinstance(m, SelfAttention2d):\n",
        "                nn.init.normal_(m.pos_enc, mean=0.0, std=0.02)\n",
        "            elif isinstance(m, Residual):\n",
        "                nn.init.zeros_(m.gamma)\n",
        "    \n",
        "    def separate_parameters(self):\n",
        "        parameters_decay = set()\n",
        "        parameters_no_decay = set()\n",
        "        modules_weight_decay = (nn.Linear, nn.Conv2d)\n",
        "        modules_no_weight_decay = (nn.LayerNorm,)\n",
        "\n",
        "        for m_name, m in self.named_modules():\n",
        "            for param_name, param in m.named_parameters():\n",
        "                full_param_name = f\"{m_name}.{param_name}\" if m_name else param_name\n",
        "\n",
        "                if isinstance(m, modules_no_weight_decay):\n",
        "                    parameters_no_decay.add(full_param_name)\n",
        "                elif param_name.endswith(\"bias\"):\n",
        "                    parameters_no_decay.add(full_param_name)\n",
        "                elif isinstance(m, Residual) and param_name.endswith(\"gamma\"):\n",
        "                    parameters_no_decay.add(full_param_name)\n",
        "                elif isinstance(m, AddPositionEmbedding) and param_name.endswith(\"pos_embedding\"):\n",
        "                    parameters_no_decay.add(full_param_name)\n",
        "                elif isinstance(m, SelfAttention2d) and param_name.endswith(\"pos_enc\"):\n",
        "                    parameters_no_decay.add(full_param_name)\n",
        "                elif isinstance(m, modules_weight_decay):\n",
        "                    parameters_decay.add(full_param_name)\n",
        "\n",
        "        # sanity check\n",
        "        # assert len(parameters_decay & parameters_no_decay) == 0\n",
        "        # assert len(parameters_decay) + len(parameters_no_decay) == len(list(model.parameters()))\n",
        "\n",
        "        return parameters_decay, parameters_no_decay"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "93TUs4ERxsjJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "730d078f-a633-4b86-a012-dc144a61ee88"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SelfAttention2d get_indices indices : torch.Size([16, 16, 16, 16])\n",
            "SelfAttention2d get_indices flat indices : torch.Size([65536])\n",
            "SelfAttention2d get_indices indices : torch.Size([16, 16, 16, 16])\n",
            "SelfAttention2d get_indices flat indices : torch.Size([65536])\n",
            "SelfAttention2d get_indices indices : torch.Size([16, 16, 16, 16])\n",
            "SelfAttention2d get_indices flat indices : torch.Size([65536])\n",
            "SelfAttention2d get_indices indices : torch.Size([16, 16, 16, 16])\n",
            "SelfAttention2d get_indices flat indices : torch.Size([65536])\n"
          ]
        }
      ],
      "source": [
        "NUM_CLASSES, IMAGE_SIZE = 10, 32\n",
        "model = ViT(NUM_CLASSES, IMAGE_SIZE, channels=32, head_channels=8, num_blocks=4, patch_size=2,\n",
        "               emb_p_drop=0., trans_p_drop=0., head_p_drop=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mu4DG-twxsjJ",
        "outputId": "f1d622be-ada2-4f71-dfdd-aa2f8c086ef6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ViT(\n",
              "  (0): ToEmbedding(\n",
              "    (0): ToPatches(\n",
              "      (tp1): Sequential(\n",
              "        (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (1): GELU(approximate='none')\n",
              "      )\n",
              "      (tp2): Conv2d(32, 32, kernel_size=(2, 2), stride=(2, 2))\n",
              "    )\n",
              "    (1): AddPositionEmbedding()\n",
              "    (2): Dropout(p=0.0, inplace=False)\n",
              "  )\n",
              "  (1): TransformerStack(\n",
              "    (0): TransformerBlock(\n",
              "      (0): Residual(\n",
              "        (residual): Sequential(\n",
              "          (0): LayerNormChannels(\n",
              "            (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (1): SelfAttention2d(\n",
              "            (to_keys): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (to_queries): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (to_values): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (unifyheads): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
              "          )\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (1): Residual(\n",
              "        (residual): Sequential(\n",
              "          (0): LayerNormChannels(\n",
              "            (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (1): FeedForward(\n",
              "            (0): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (1): GELU(approximate='none')\n",
              "            (2): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
              "          )\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (1): TransformerBlock(\n",
              "      (0): Residual(\n",
              "        (residual): Sequential(\n",
              "          (0): LayerNormChannels(\n",
              "            (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (1): SelfAttention2d(\n",
              "            (to_keys): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (to_queries): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (to_values): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (unifyheads): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
              "          )\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (1): Residual(\n",
              "        (residual): Sequential(\n",
              "          (0): LayerNormChannels(\n",
              "            (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (1): FeedForward(\n",
              "            (0): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (1): GELU(approximate='none')\n",
              "            (2): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
              "          )\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (2): TransformerBlock(\n",
              "      (0): Residual(\n",
              "        (residual): Sequential(\n",
              "          (0): LayerNormChannels(\n",
              "            (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (1): SelfAttention2d(\n",
              "            (to_keys): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (to_queries): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (to_values): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (unifyheads): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
              "          )\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (1): Residual(\n",
              "        (residual): Sequential(\n",
              "          (0): LayerNormChannels(\n",
              "            (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (1): FeedForward(\n",
              "            (0): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (1): GELU(approximate='none')\n",
              "            (2): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
              "          )\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (3): TransformerBlock(\n",
              "      (0): Residual(\n",
              "        (residual): Sequential(\n",
              "          (0): LayerNormChannels(\n",
              "            (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (1): SelfAttention2d(\n",
              "            (to_keys): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (to_queries): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (to_values): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (unifyheads): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
              "          )\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (1): Residual(\n",
              "        (residual): Sequential(\n",
              "          (0): LayerNormChannels(\n",
              "            (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (1): FeedForward(\n",
              "            (0): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (1): GELU(approximate='none')\n",
              "            (2): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
              "          )\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (2): Head(\n",
              "    (0): LayerNormChannels(\n",
              "      (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (1): GELU(approximate='none')\n",
              "    (2): AdaptiveAvgPool2d(output_size=1)\n",
              "    (3): Flatten(start_dim=1, end_dim=-1)\n",
              "    (4): Dropout(p=0.1, inplace=False)\n",
              "    (5): Linear(in_features=32, out_features=10, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "model.to(DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cFntUhVVxsjK",
        "outputId": "d1aa14cd-b5f8-495f-c9bb-9ee60eb94d2c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of parameters: 79,810\n"
          ]
        }
      ],
      "source": [
        "print(\"Number of parameters: {:,}\".format(sum(p.numel() for p in model.parameters())))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchsummary\n",
        "from torchsummary import summary\n",
        "summary(model, input_size=(3, 32, 32))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qhj52m4k8wJs",
        "outputId": "03370346-a6b5-4c44-88ad-41d77f5063e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchsummary in /usr/local/lib/python3.9/dist-packages (1.5.1)\n",
            "ToPatches x.shape : torch.Size([2, 3, 32, 32])\n",
            "ToPatches self.tp1 output : torch.Size([2, 32, 32, 32])\n",
            "ToPatches self.tp2 output : torch.Size([2, 32, 16, 16])\n",
            "AddPositionEmbedding x.shape : torch.Size([2, 32, 16, 16]), self.pos_embedding.shape : torch.Size([32, 16, 16])\n",
            "Residual x.shape : torch.Size([2, 32, 16, 16])\n",
            "SelfAttention2d fwd : b, _, h, w : torch.Size([2, 32, 16, 16])\n",
            "SelfAttention2d fwd : k v q shapes : torch.Size([2, 4, 8, 256]) , torch.Size([2, 4, 8, 256]), torch.Size([2, 4, 8, 256])\n",
            "SelfAttention2d fwd att.shape: torch.Size([2, 4, 256, 256])\n",
            "SelfAttention2d fwd indices.shape: torch.Size([4, 65536]), self.relative_indices.shape : torch.Size([65536]), self.heads : 4\n",
            "SelfAttention2d fwd self.pos_enc.shape : torch.Size([4, 961]), rel_pos_enc.shape : torch.Size([4, 65536])\n",
            "SelfAttention2d fwd flat rel_pos_enc : torch.Size([4, 256, 256])\n",
            "SelfAttention2d fwd attn.shape after adding rel_pos_enc : torch.Size([2, 4, 256, 256])\n",
            "SelfAttention2d fwd out.shape : torch.Size([2, 4, 8, 256])\n",
            "SelfAttention2d fwd out.view(b, -1, h, w) : torch.Size([2, 32, 16, 16])\n",
            "SelfAttention2d fwd self.unifyheads(out): torch.Size([2, 32, 16, 16])\n",
            "Residual self.residual(x).shape : torch.Size([2, 32, 16, 16])\n",
            "SelfAttention2d fwd : b, _, h, w : torch.Size([2, 32, 16, 16])\n",
            "SelfAttention2d fwd : k v q shapes : torch.Size([2, 4, 8, 256]) , torch.Size([2, 4, 8, 256]), torch.Size([2, 4, 8, 256])\n",
            "SelfAttention2d fwd att.shape: torch.Size([2, 4, 256, 256])\n",
            "SelfAttention2d fwd indices.shape: torch.Size([4, 65536]), self.relative_indices.shape : torch.Size([65536]), self.heads : 4\n",
            "SelfAttention2d fwd self.pos_enc.shape : torch.Size([4, 961]), rel_pos_enc.shape : torch.Size([4, 65536])\n",
            "SelfAttention2d fwd flat rel_pos_enc : torch.Size([4, 256, 256])\n",
            "SelfAttention2d fwd attn.shape after adding rel_pos_enc : torch.Size([2, 4, 256, 256])\n",
            "SelfAttention2d fwd out.shape : torch.Size([2, 4, 8, 256])\n",
            "SelfAttention2d fwd out.view(b, -1, h, w) : torch.Size([2, 32, 16, 16])\n",
            "SelfAttention2d fwd self.unifyheads(out): torch.Size([2, 32, 16, 16])\n",
            "Residual out.shape : torch.Size([2, 32, 16, 16])\n",
            "SelfAttention2d fwd : b, _, h, w : torch.Size([2, 32, 16, 16])\n",
            "SelfAttention2d fwd : k v q shapes : torch.Size([2, 4, 8, 256]) , torch.Size([2, 4, 8, 256]), torch.Size([2, 4, 8, 256])\n",
            "SelfAttention2d fwd att.shape: torch.Size([2, 4, 256, 256])\n",
            "SelfAttention2d fwd indices.shape: torch.Size([4, 65536]), self.relative_indices.shape : torch.Size([65536]), self.heads : 4\n",
            "SelfAttention2d fwd self.pos_enc.shape : torch.Size([4, 961]), rel_pos_enc.shape : torch.Size([4, 65536])\n",
            "SelfAttention2d fwd flat rel_pos_enc : torch.Size([4, 256, 256])\n",
            "SelfAttention2d fwd attn.shape after adding rel_pos_enc : torch.Size([2, 4, 256, 256])\n",
            "SelfAttention2d fwd out.shape : torch.Size([2, 4, 8, 256])\n",
            "SelfAttention2d fwd out.view(b, -1, h, w) : torch.Size([2, 32, 16, 16])\n",
            "SelfAttention2d fwd self.unifyheads(out): torch.Size([2, 32, 16, 16])\n",
            "Residual x.shape : torch.Size([2, 32, 16, 16])\n",
            "Residual self.residual(x).shape : torch.Size([2, 32, 16, 16])\n",
            "Residual out.shape : torch.Size([2, 32, 16, 16])\n",
            "Residual x.shape : torch.Size([2, 32, 16, 16])\n",
            "SelfAttention2d fwd : b, _, h, w : torch.Size([2, 32, 16, 16])\n",
            "SelfAttention2d fwd : k v q shapes : torch.Size([2, 4, 8, 256]) , torch.Size([2, 4, 8, 256]), torch.Size([2, 4, 8, 256])\n",
            "SelfAttention2d fwd att.shape: torch.Size([2, 4, 256, 256])\n",
            "SelfAttention2d fwd indices.shape: torch.Size([4, 65536]), self.relative_indices.shape : torch.Size([65536]), self.heads : 4\n",
            "SelfAttention2d fwd self.pos_enc.shape : torch.Size([4, 961]), rel_pos_enc.shape : torch.Size([4, 65536])\n",
            "SelfAttention2d fwd flat rel_pos_enc : torch.Size([4, 256, 256])\n",
            "SelfAttention2d fwd attn.shape after adding rel_pos_enc : torch.Size([2, 4, 256, 256])\n",
            "SelfAttention2d fwd out.shape : torch.Size([2, 4, 8, 256])\n",
            "SelfAttention2d fwd out.view(b, -1, h, w) : torch.Size([2, 32, 16, 16])\n",
            "SelfAttention2d fwd self.unifyheads(out): torch.Size([2, 32, 16, 16])\n",
            "Residual self.residual(x).shape : torch.Size([2, 32, 16, 16])\n",
            "SelfAttention2d fwd : b, _, h, w : torch.Size([2, 32, 16, 16])\n",
            "SelfAttention2d fwd : k v q shapes : torch.Size([2, 4, 8, 256]) , torch.Size([2, 4, 8, 256]), torch.Size([2, 4, 8, 256])\n",
            "SelfAttention2d fwd att.shape: torch.Size([2, 4, 256, 256])\n",
            "SelfAttention2d fwd indices.shape: torch.Size([4, 65536]), self.relative_indices.shape : torch.Size([65536]), self.heads : 4\n",
            "SelfAttention2d fwd self.pos_enc.shape : torch.Size([4, 961]), rel_pos_enc.shape : torch.Size([4, 65536])\n",
            "SelfAttention2d fwd flat rel_pos_enc : torch.Size([4, 256, 256])\n",
            "SelfAttention2d fwd attn.shape after adding rel_pos_enc : torch.Size([2, 4, 256, 256])\n",
            "SelfAttention2d fwd out.shape : torch.Size([2, 4, 8, 256])\n",
            "SelfAttention2d fwd out.view(b, -1, h, w) : torch.Size([2, 32, 16, 16])\n",
            "SelfAttention2d fwd self.unifyheads(out): torch.Size([2, 32, 16, 16])\n",
            "Residual out.shape : torch.Size([2, 32, 16, 16])\n",
            "SelfAttention2d fwd : b, _, h, w : torch.Size([2, 32, 16, 16])\n",
            "SelfAttention2d fwd : k v q shapes : torch.Size([2, 4, 8, 256]) , torch.Size([2, 4, 8, 256]), torch.Size([2, 4, 8, 256])\n",
            "SelfAttention2d fwd att.shape: torch.Size([2, 4, 256, 256])\n",
            "SelfAttention2d fwd indices.shape: torch.Size([4, 65536]), self.relative_indices.shape : torch.Size([65536]), self.heads : 4\n",
            "SelfAttention2d fwd self.pos_enc.shape : torch.Size([4, 961]), rel_pos_enc.shape : torch.Size([4, 65536])\n",
            "SelfAttention2d fwd flat rel_pos_enc : torch.Size([4, 256, 256])\n",
            "SelfAttention2d fwd attn.shape after adding rel_pos_enc : torch.Size([2, 4, 256, 256])\n",
            "SelfAttention2d fwd out.shape : torch.Size([2, 4, 8, 256])\n",
            "SelfAttention2d fwd out.view(b, -1, h, w) : torch.Size([2, 32, 16, 16])\n",
            "SelfAttention2d fwd self.unifyheads(out): torch.Size([2, 32, 16, 16])\n",
            "Residual x.shape : torch.Size([2, 32, 16, 16])\n",
            "Residual self.residual(x).shape : torch.Size([2, 32, 16, 16])\n",
            "Residual out.shape : torch.Size([2, 32, 16, 16])\n",
            "Residual x.shape : torch.Size([2, 32, 16, 16])\n",
            "SelfAttention2d fwd : b, _, h, w : torch.Size([2, 32, 16, 16])\n",
            "SelfAttention2d fwd : k v q shapes : torch.Size([2, 4, 8, 256]) , torch.Size([2, 4, 8, 256]), torch.Size([2, 4, 8, 256])\n",
            "SelfAttention2d fwd att.shape: torch.Size([2, 4, 256, 256])\n",
            "SelfAttention2d fwd indices.shape: torch.Size([4, 65536]), self.relative_indices.shape : torch.Size([65536]), self.heads : 4\n",
            "SelfAttention2d fwd self.pos_enc.shape : torch.Size([4, 961]), rel_pos_enc.shape : torch.Size([4, 65536])\n",
            "SelfAttention2d fwd flat rel_pos_enc : torch.Size([4, 256, 256])\n",
            "SelfAttention2d fwd attn.shape after adding rel_pos_enc : torch.Size([2, 4, 256, 256])\n",
            "SelfAttention2d fwd out.shape : torch.Size([2, 4, 8, 256])\n",
            "SelfAttention2d fwd out.view(b, -1, h, w) : torch.Size([2, 32, 16, 16])\n",
            "SelfAttention2d fwd self.unifyheads(out): torch.Size([2, 32, 16, 16])\n",
            "Residual self.residual(x).shape : torch.Size([2, 32, 16, 16])\n",
            "SelfAttention2d fwd : b, _, h, w : torch.Size([2, 32, 16, 16])\n",
            "SelfAttention2d fwd : k v q shapes : torch.Size([2, 4, 8, 256]) , torch.Size([2, 4, 8, 256]), torch.Size([2, 4, 8, 256])\n",
            "SelfAttention2d fwd att.shape: torch.Size([2, 4, 256, 256])\n",
            "SelfAttention2d fwd indices.shape: torch.Size([4, 65536]), self.relative_indices.shape : torch.Size([65536]), self.heads : 4\n",
            "SelfAttention2d fwd self.pos_enc.shape : torch.Size([4, 961]), rel_pos_enc.shape : torch.Size([4, 65536])\n",
            "SelfAttention2d fwd flat rel_pos_enc : torch.Size([4, 256, 256])\n",
            "SelfAttention2d fwd attn.shape after adding rel_pos_enc : torch.Size([2, 4, 256, 256])\n",
            "SelfAttention2d fwd out.shape : torch.Size([2, 4, 8, 256])\n",
            "SelfAttention2d fwd out.view(b, -1, h, w) : torch.Size([2, 32, 16, 16])\n",
            "SelfAttention2d fwd self.unifyheads(out): torch.Size([2, 32, 16, 16])\n",
            "Residual out.shape : torch.Size([2, 32, 16, 16])\n",
            "SelfAttention2d fwd : b, _, h, w : torch.Size([2, 32, 16, 16])\n",
            "SelfAttention2d fwd : k v q shapes : torch.Size([2, 4, 8, 256]) , torch.Size([2, 4, 8, 256]), torch.Size([2, 4, 8, 256])\n",
            "SelfAttention2d fwd att.shape: torch.Size([2, 4, 256, 256])\n",
            "SelfAttention2d fwd indices.shape: torch.Size([4, 65536]), self.relative_indices.shape : torch.Size([65536]), self.heads : 4\n",
            "SelfAttention2d fwd self.pos_enc.shape : torch.Size([4, 961]), rel_pos_enc.shape : torch.Size([4, 65536])\n",
            "SelfAttention2d fwd flat rel_pos_enc : torch.Size([4, 256, 256])\n",
            "SelfAttention2d fwd attn.shape after adding rel_pos_enc : torch.Size([2, 4, 256, 256])\n",
            "SelfAttention2d fwd out.shape : torch.Size([2, 4, 8, 256])\n",
            "SelfAttention2d fwd out.view(b, -1, h, w) : torch.Size([2, 32, 16, 16])\n",
            "SelfAttention2d fwd self.unifyheads(out): torch.Size([2, 32, 16, 16])\n",
            "Residual x.shape : torch.Size([2, 32, 16, 16])\n",
            "Residual self.residual(x).shape : torch.Size([2, 32, 16, 16])\n",
            "Residual out.shape : torch.Size([2, 32, 16, 16])\n",
            "Residual x.shape : torch.Size([2, 32, 16, 16])\n",
            "SelfAttention2d fwd : b, _, h, w : torch.Size([2, 32, 16, 16])\n",
            "SelfAttention2d fwd : k v q shapes : torch.Size([2, 4, 8, 256]) , torch.Size([2, 4, 8, 256]), torch.Size([2, 4, 8, 256])\n",
            "SelfAttention2d fwd att.shape: torch.Size([2, 4, 256, 256])\n",
            "SelfAttention2d fwd indices.shape: torch.Size([4, 65536]), self.relative_indices.shape : torch.Size([65536]), self.heads : 4\n",
            "SelfAttention2d fwd self.pos_enc.shape : torch.Size([4, 961]), rel_pos_enc.shape : torch.Size([4, 65536])\n",
            "SelfAttention2d fwd flat rel_pos_enc : torch.Size([4, 256, 256])\n",
            "SelfAttention2d fwd attn.shape after adding rel_pos_enc : torch.Size([2, 4, 256, 256])\n",
            "SelfAttention2d fwd out.shape : torch.Size([2, 4, 8, 256])\n",
            "SelfAttention2d fwd out.view(b, -1, h, w) : torch.Size([2, 32, 16, 16])\n",
            "SelfAttention2d fwd self.unifyheads(out): torch.Size([2, 32, 16, 16])\n",
            "Residual self.residual(x).shape : torch.Size([2, 32, 16, 16])\n",
            "SelfAttention2d fwd : b, _, h, w : torch.Size([2, 32, 16, 16])\n",
            "SelfAttention2d fwd : k v q shapes : torch.Size([2, 4, 8, 256]) , torch.Size([2, 4, 8, 256]), torch.Size([2, 4, 8, 256])\n",
            "SelfAttention2d fwd att.shape: torch.Size([2, 4, 256, 256])\n",
            "SelfAttention2d fwd indices.shape: torch.Size([4, 65536]), self.relative_indices.shape : torch.Size([65536]), self.heads : 4\n",
            "SelfAttention2d fwd self.pos_enc.shape : torch.Size([4, 961]), rel_pos_enc.shape : torch.Size([4, 65536])\n",
            "SelfAttention2d fwd flat rel_pos_enc : torch.Size([4, 256, 256])\n",
            "SelfAttention2d fwd attn.shape after adding rel_pos_enc : torch.Size([2, 4, 256, 256])\n",
            "SelfAttention2d fwd out.shape : torch.Size([2, 4, 8, 256])\n",
            "SelfAttention2d fwd out.view(b, -1, h, w) : torch.Size([2, 32, 16, 16])\n",
            "SelfAttention2d fwd self.unifyheads(out): torch.Size([2, 32, 16, 16])\n",
            "Residual out.shape : torch.Size([2, 32, 16, 16])\n",
            "SelfAttention2d fwd : b, _, h, w : torch.Size([2, 32, 16, 16])\n",
            "SelfAttention2d fwd : k v q shapes : torch.Size([2, 4, 8, 256]) , torch.Size([2, 4, 8, 256]), torch.Size([2, 4, 8, 256])\n",
            "SelfAttention2d fwd att.shape: torch.Size([2, 4, 256, 256])\n",
            "SelfAttention2d fwd indices.shape: torch.Size([4, 65536]), self.relative_indices.shape : torch.Size([65536]), self.heads : 4\n",
            "SelfAttention2d fwd self.pos_enc.shape : torch.Size([4, 961]), rel_pos_enc.shape : torch.Size([4, 65536])\n",
            "SelfAttention2d fwd flat rel_pos_enc : torch.Size([4, 256, 256])\n",
            "SelfAttention2d fwd attn.shape after adding rel_pos_enc : torch.Size([2, 4, 256, 256])\n",
            "SelfAttention2d fwd out.shape : torch.Size([2, 4, 8, 256])\n",
            "SelfAttention2d fwd out.view(b, -1, h, w) : torch.Size([2, 32, 16, 16])\n",
            "SelfAttention2d fwd self.unifyheads(out): torch.Size([2, 32, 16, 16])\n",
            "Residual x.shape : torch.Size([2, 32, 16, 16])\n",
            "Residual self.residual(x).shape : torch.Size([2, 32, 16, 16])\n",
            "Residual out.shape : torch.Size([2, 32, 16, 16])\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 32, 32, 32]             896\n",
            "              GELU-2           [-1, 32, 32, 32]               0\n",
            "            Conv2d-3           [-1, 32, 16, 16]           4,128\n",
            "AddPositionEmbedding-4           [-1, 32, 16, 16]               0\n",
            "           Dropout-5           [-1, 32, 16, 16]               0\n",
            "         LayerNorm-6           [-1, 16, 16, 32]              64\n",
            " LayerNormChannels-7           [-1, 32, 16, 16]               0\n",
            "            Conv2d-8           [-1, 32, 16, 16]           1,056\n",
            "            Conv2d-9           [-1, 32, 16, 16]           1,056\n",
            "           Conv2d-10           [-1, 32, 16, 16]           1,056\n",
            "           Conv2d-11           [-1, 32, 16, 16]           1,056\n",
            "  SelfAttention2d-12           [-1, 32, 16, 16]               0\n",
            "          Dropout-13           [-1, 32, 16, 16]               0\n",
            "        LayerNorm-14           [-1, 16, 16, 32]              64\n",
            "LayerNormChannels-15           [-1, 32, 16, 16]               0\n",
            "           Conv2d-16           [-1, 32, 16, 16]           1,056\n",
            "           Conv2d-17           [-1, 32, 16, 16]           1,056\n",
            "           Conv2d-18           [-1, 32, 16, 16]           1,056\n",
            "           Conv2d-19           [-1, 32, 16, 16]           1,056\n",
            "  SelfAttention2d-20           [-1, 32, 16, 16]               0\n",
            "          Dropout-21           [-1, 32, 16, 16]               0\n",
            "        LayerNorm-22           [-1, 16, 16, 32]              64\n",
            "LayerNormChannels-23           [-1, 32, 16, 16]               0\n",
            "           Conv2d-24           [-1, 32, 16, 16]           1,056\n",
            "           Conv2d-25           [-1, 32, 16, 16]           1,056\n",
            "           Conv2d-26           [-1, 32, 16, 16]           1,056\n",
            "           Conv2d-27           [-1, 32, 16, 16]           1,056\n",
            "  SelfAttention2d-28           [-1, 32, 16, 16]               0\n",
            "          Dropout-29           [-1, 32, 16, 16]               0\n",
            "         Residual-30           [-1, 32, 16, 16]               0\n",
            "        LayerNorm-31           [-1, 16, 16, 32]              64\n",
            "LayerNormChannels-32           [-1, 32, 16, 16]               0\n",
            "           Conv2d-33          [-1, 128, 16, 16]           4,224\n",
            "             GELU-34          [-1, 128, 16, 16]               0\n",
            "           Conv2d-35           [-1, 32, 16, 16]           4,128\n",
            "          Dropout-36           [-1, 32, 16, 16]               0\n",
            "        LayerNorm-37           [-1, 16, 16, 32]              64\n",
            "LayerNormChannels-38           [-1, 32, 16, 16]               0\n",
            "           Conv2d-39          [-1, 128, 16, 16]           4,224\n",
            "             GELU-40          [-1, 128, 16, 16]               0\n",
            "           Conv2d-41           [-1, 32, 16, 16]           4,128\n",
            "          Dropout-42           [-1, 32, 16, 16]               0\n",
            "        LayerNorm-43           [-1, 16, 16, 32]              64\n",
            "LayerNormChannels-44           [-1, 32, 16, 16]               0\n",
            "           Conv2d-45          [-1, 128, 16, 16]           4,224\n",
            "             GELU-46          [-1, 128, 16, 16]               0\n",
            "           Conv2d-47           [-1, 32, 16, 16]           4,128\n",
            "          Dropout-48           [-1, 32, 16, 16]               0\n",
            "         Residual-49           [-1, 32, 16, 16]               0\n",
            "        LayerNorm-50           [-1, 16, 16, 32]              64\n",
            "LayerNormChannels-51           [-1, 32, 16, 16]               0\n",
            "           Conv2d-52           [-1, 32, 16, 16]           1,056\n",
            "           Conv2d-53           [-1, 32, 16, 16]           1,056\n",
            "           Conv2d-54           [-1, 32, 16, 16]           1,056\n",
            "           Conv2d-55           [-1, 32, 16, 16]           1,056\n",
            "  SelfAttention2d-56           [-1, 32, 16, 16]               0\n",
            "          Dropout-57           [-1, 32, 16, 16]               0\n",
            "        LayerNorm-58           [-1, 16, 16, 32]              64\n",
            "LayerNormChannels-59           [-1, 32, 16, 16]               0\n",
            "           Conv2d-60           [-1, 32, 16, 16]           1,056\n",
            "           Conv2d-61           [-1, 32, 16, 16]           1,056\n",
            "           Conv2d-62           [-1, 32, 16, 16]           1,056\n",
            "           Conv2d-63           [-1, 32, 16, 16]           1,056\n",
            "  SelfAttention2d-64           [-1, 32, 16, 16]               0\n",
            "          Dropout-65           [-1, 32, 16, 16]               0\n",
            "        LayerNorm-66           [-1, 16, 16, 32]              64\n",
            "LayerNormChannels-67           [-1, 32, 16, 16]               0\n",
            "           Conv2d-68           [-1, 32, 16, 16]           1,056\n",
            "           Conv2d-69           [-1, 32, 16, 16]           1,056\n",
            "           Conv2d-70           [-1, 32, 16, 16]           1,056\n",
            "           Conv2d-71           [-1, 32, 16, 16]           1,056\n",
            "  SelfAttention2d-72           [-1, 32, 16, 16]               0\n",
            "          Dropout-73           [-1, 32, 16, 16]               0\n",
            "         Residual-74           [-1, 32, 16, 16]               0\n",
            "        LayerNorm-75           [-1, 16, 16, 32]              64\n",
            "LayerNormChannels-76           [-1, 32, 16, 16]               0\n",
            "           Conv2d-77          [-1, 128, 16, 16]           4,224\n",
            "             GELU-78          [-1, 128, 16, 16]               0\n",
            "           Conv2d-79           [-1, 32, 16, 16]           4,128\n",
            "          Dropout-80           [-1, 32, 16, 16]               0\n",
            "        LayerNorm-81           [-1, 16, 16, 32]              64\n",
            "LayerNormChannels-82           [-1, 32, 16, 16]               0\n",
            "           Conv2d-83          [-1, 128, 16, 16]           4,224\n",
            "             GELU-84          [-1, 128, 16, 16]               0\n",
            "           Conv2d-85           [-1, 32, 16, 16]           4,128\n",
            "          Dropout-86           [-1, 32, 16, 16]               0\n",
            "        LayerNorm-87           [-1, 16, 16, 32]              64\n",
            "LayerNormChannels-88           [-1, 32, 16, 16]               0\n",
            "           Conv2d-89          [-1, 128, 16, 16]           4,224\n",
            "             GELU-90          [-1, 128, 16, 16]               0\n",
            "           Conv2d-91           [-1, 32, 16, 16]           4,128\n",
            "          Dropout-92           [-1, 32, 16, 16]               0\n",
            "         Residual-93           [-1, 32, 16, 16]               0\n",
            "        LayerNorm-94           [-1, 16, 16, 32]              64\n",
            "LayerNormChannels-95           [-1, 32, 16, 16]               0\n",
            "           Conv2d-96           [-1, 32, 16, 16]           1,056\n",
            "           Conv2d-97           [-1, 32, 16, 16]           1,056\n",
            "           Conv2d-98           [-1, 32, 16, 16]           1,056\n",
            "           Conv2d-99           [-1, 32, 16, 16]           1,056\n",
            " SelfAttention2d-100           [-1, 32, 16, 16]               0\n",
            "         Dropout-101           [-1, 32, 16, 16]               0\n",
            "       LayerNorm-102           [-1, 16, 16, 32]              64\n",
            "LayerNormChannels-103           [-1, 32, 16, 16]               0\n",
            "          Conv2d-104           [-1, 32, 16, 16]           1,056\n",
            "          Conv2d-105           [-1, 32, 16, 16]           1,056\n",
            "          Conv2d-106           [-1, 32, 16, 16]           1,056\n",
            "          Conv2d-107           [-1, 32, 16, 16]           1,056\n",
            " SelfAttention2d-108           [-1, 32, 16, 16]               0\n",
            "         Dropout-109           [-1, 32, 16, 16]               0\n",
            "       LayerNorm-110           [-1, 16, 16, 32]              64\n",
            "LayerNormChannels-111           [-1, 32, 16, 16]               0\n",
            "          Conv2d-112           [-1, 32, 16, 16]           1,056\n",
            "          Conv2d-113           [-1, 32, 16, 16]           1,056\n",
            "          Conv2d-114           [-1, 32, 16, 16]           1,056\n",
            "          Conv2d-115           [-1, 32, 16, 16]           1,056\n",
            " SelfAttention2d-116           [-1, 32, 16, 16]               0\n",
            "         Dropout-117           [-1, 32, 16, 16]               0\n",
            "        Residual-118           [-1, 32, 16, 16]               0\n",
            "       LayerNorm-119           [-1, 16, 16, 32]              64\n",
            "LayerNormChannels-120           [-1, 32, 16, 16]               0\n",
            "          Conv2d-121          [-1, 128, 16, 16]           4,224\n",
            "            GELU-122          [-1, 128, 16, 16]               0\n",
            "          Conv2d-123           [-1, 32, 16, 16]           4,128\n",
            "         Dropout-124           [-1, 32, 16, 16]               0\n",
            "       LayerNorm-125           [-1, 16, 16, 32]              64\n",
            "LayerNormChannels-126           [-1, 32, 16, 16]               0\n",
            "          Conv2d-127          [-1, 128, 16, 16]           4,224\n",
            "            GELU-128          [-1, 128, 16, 16]               0\n",
            "          Conv2d-129           [-1, 32, 16, 16]           4,128\n",
            "         Dropout-130           [-1, 32, 16, 16]               0\n",
            "       LayerNorm-131           [-1, 16, 16, 32]              64\n",
            "LayerNormChannels-132           [-1, 32, 16, 16]               0\n",
            "          Conv2d-133          [-1, 128, 16, 16]           4,224\n",
            "            GELU-134          [-1, 128, 16, 16]               0\n",
            "          Conv2d-135           [-1, 32, 16, 16]           4,128\n",
            "         Dropout-136           [-1, 32, 16, 16]               0\n",
            "        Residual-137           [-1, 32, 16, 16]               0\n",
            "       LayerNorm-138           [-1, 16, 16, 32]              64\n",
            "LayerNormChannels-139           [-1, 32, 16, 16]               0\n",
            "          Conv2d-140           [-1, 32, 16, 16]           1,056\n",
            "          Conv2d-141           [-1, 32, 16, 16]           1,056\n",
            "          Conv2d-142           [-1, 32, 16, 16]           1,056\n",
            "          Conv2d-143           [-1, 32, 16, 16]           1,056\n",
            " SelfAttention2d-144           [-1, 32, 16, 16]               0\n",
            "         Dropout-145           [-1, 32, 16, 16]               0\n",
            "       LayerNorm-146           [-1, 16, 16, 32]              64\n",
            "LayerNormChannels-147           [-1, 32, 16, 16]               0\n",
            "          Conv2d-148           [-1, 32, 16, 16]           1,056\n",
            "          Conv2d-149           [-1, 32, 16, 16]           1,056\n",
            "          Conv2d-150           [-1, 32, 16, 16]           1,056\n",
            "          Conv2d-151           [-1, 32, 16, 16]           1,056\n",
            " SelfAttention2d-152           [-1, 32, 16, 16]               0\n",
            "         Dropout-153           [-1, 32, 16, 16]               0\n",
            "       LayerNorm-154           [-1, 16, 16, 32]              64\n",
            "LayerNormChannels-155           [-1, 32, 16, 16]               0\n",
            "          Conv2d-156           [-1, 32, 16, 16]           1,056\n",
            "          Conv2d-157           [-1, 32, 16, 16]           1,056\n",
            "          Conv2d-158           [-1, 32, 16, 16]           1,056\n",
            "          Conv2d-159           [-1, 32, 16, 16]           1,056\n",
            " SelfAttention2d-160           [-1, 32, 16, 16]               0\n",
            "         Dropout-161           [-1, 32, 16, 16]               0\n",
            "        Residual-162           [-1, 32, 16, 16]               0\n",
            "       LayerNorm-163           [-1, 16, 16, 32]              64\n",
            "LayerNormChannels-164           [-1, 32, 16, 16]               0\n",
            "          Conv2d-165          [-1, 128, 16, 16]           4,224\n",
            "            GELU-166          [-1, 128, 16, 16]               0\n",
            "          Conv2d-167           [-1, 32, 16, 16]           4,128\n",
            "         Dropout-168           [-1, 32, 16, 16]               0\n",
            "       LayerNorm-169           [-1, 16, 16, 32]              64\n",
            "LayerNormChannels-170           [-1, 32, 16, 16]               0\n",
            "          Conv2d-171          [-1, 128, 16, 16]           4,224\n",
            "            GELU-172          [-1, 128, 16, 16]               0\n",
            "          Conv2d-173           [-1, 32, 16, 16]           4,128\n",
            "         Dropout-174           [-1, 32, 16, 16]               0\n",
            "       LayerNorm-175           [-1, 16, 16, 32]              64\n",
            "LayerNormChannels-176           [-1, 32, 16, 16]               0\n",
            "          Conv2d-177          [-1, 128, 16, 16]           4,224\n",
            "            GELU-178          [-1, 128, 16, 16]               0\n",
            "          Conv2d-179           [-1, 32, 16, 16]           4,128\n",
            "         Dropout-180           [-1, 32, 16, 16]               0\n",
            "        Residual-181           [-1, 32, 16, 16]               0\n",
            "       LayerNorm-182           [-1, 16, 16, 32]              64\n",
            "LayerNormChannels-183           [-1, 32, 16, 16]               0\n",
            "            GELU-184           [-1, 32, 16, 16]               0\n",
            "AdaptiveAvgPool2d-185             [-1, 32, 1, 1]               0\n",
            "         Flatten-186                   [-1, 32]               0\n",
            "         Dropout-187                   [-1, 32]               0\n",
            "          Linear-188                   [-1, 10]             330\n",
            "================================================================\n",
            "Total params: 157,866\n",
            "Trainable params: 157,866\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 16.38\n",
            "Params size (MB): 0.60\n",
            "Estimated Total Size (MB): 16.99\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156,
          "referenced_widgets": [
            "14b6f6562bc54da19725f7336280a367",
            "ca0a52fee01042e086f1c338f7a7d062",
            "d903733793c745f599e9f57f378b2746",
            "ae0b281403064b54a94ca64bdacfcfbb",
            "fcf4feefa880439eb1bf0edfebb3c52a",
            "3cb95b95b2cd419287437714fc7811cc",
            "fafbf1325d0445079aed3055e83a5d26",
            "89ba26e743a74fbc8a6d34b20c6696cb",
            "ac58ddf43a1140f4a7bcccabdc515b67",
            "77e774963d314b3fa5b8438fc208aedc",
            "ea8c1a28ee9046f7aa71f64282ae2421"
          ]
        },
        "id": "uqAyuVtnxsjK",
        "outputId": "c1fc9587-f55a-4a7f-a1dd-15ce48ae17e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/170498071 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "14b6f6562bc54da19725f7336280a367"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "IMAGE_SIZE = 32\n",
        "\n",
        "NUM_CLASSES = 10\n",
        "NUM_WORKERS = 8\n",
        "BATCH_SIZE = 128\n",
        "EPOCHS = 25\n",
        "\n",
        "LEARNING_RATE = 1e-3\n",
        "WEIGHT_DECAY = 1e-1\n",
        "\n",
        "cifar10_mean = (0.4914, 0.4822, 0.4465)\n",
        "cifar10_std = (0.2471, 0.2435, 0.2616)\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(32, scale=(0.75, 1.0), ratio=(1.0, 1.0)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandAugment(num_ops=1, magnitude=8),\n",
        "    transforms.ColorJitter(0.1, 0.1, 0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(cifar10_mean, cifar10_std),\n",
        "    transforms.RandomErasing(p=0.25)\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(cifar10_mean, cifar10_std)\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=train_transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE,\n",
        "                                          shuffle=True, num_workers=4)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=test_transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=64,\n",
        "                                         shuffle=False, num_workers=4)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "orP4qguJxsjL",
        "outputId": "9599f0c2-5e93-434b-bf9c-cd3b54a0ccb2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ConvMixer: Epoch: 0 | Train Acc: 0.2119, Test Acc: 0.3734, Time: 81.8, lr: 0.001000\n",
            "ConvMixer: Epoch: 1 | Train Acc: 0.3742, Test Acc: 0.4631, Time: 73.9, lr: 0.002000\n",
            "ConvMixer: Epoch: 2 | Train Acc: 0.4592, Test Acc: 0.5305, Time: 75.9, lr: 0.003000\n",
            "ConvMixer: Epoch: 3 | Train Acc: 0.5039, Test Acc: 0.5686, Time: 75.2, lr: 0.004000\n",
            "ConvMixer: Epoch: 4 | Train Acc: 0.5246, Test Acc: 0.5573, Time: 75.4, lr: 0.005000\n",
            "ConvMixer: Epoch: 5 | Train Acc: 0.5387, Test Acc: 0.5856, Time: 74.9, lr: 0.006000\n",
            "ConvMixer: Epoch: 6 | Train Acc: 0.5541, Test Acc: 0.5705, Time: 74.9, lr: 0.007000\n",
            "ConvMixer: Epoch: 7 | Train Acc: 0.5690, Test Acc: 0.6013, Time: 74.1, lr: 0.008000\n",
            "ConvMixer: Epoch: 8 | Train Acc: 0.5824, Test Acc: 0.6289, Time: 74.1, lr: 0.009000\n",
            "ConvMixer: Epoch: 9 | Train Acc: 0.5869, Test Acc: 0.6189, Time: 74.0, lr: 0.010000\n",
            "ConvMixer: Epoch: 10 | Train Acc: 0.6014, Test Acc: 0.6474, Time: 75.8, lr: 0.009050\n",
            "ConvMixer: Epoch: 11 | Train Acc: 0.6179, Test Acc: 0.6574, Time: 73.9, lr: 0.008100\n",
            "ConvMixer: Epoch: 12 | Train Acc: 0.6370, Test Acc: 0.6776, Time: 74.0, lr: 0.007150\n",
            "ConvMixer: Epoch: 13 | Train Acc: 0.6603, Test Acc: 0.6959, Time: 75.5, lr: 0.006200\n",
            "ConvMixer: Epoch: 14 | Train Acc: 0.6747, Test Acc: 0.7276, Time: 74.1, lr: 0.005250\n",
            "ConvMixer: Epoch: 15 | Train Acc: 0.6884, Test Acc: 0.7332, Time: 74.3, lr: 0.004300\n",
            "ConvMixer: Epoch: 16 | Train Acc: 0.7056, Test Acc: 0.7469, Time: 76.8, lr: 0.003350\n",
            "ConvMixer: Epoch: 17 | Train Acc: 0.7224, Test Acc: 0.7629, Time: 74.7, lr: 0.002400\n",
            "ConvMixer: Epoch: 18 | Train Acc: 0.7395, Test Acc: 0.7682, Time: 73.4, lr: 0.001450\n",
            "ConvMixer: Epoch: 19 | Train Acc: 0.7506, Test Acc: 0.7733, Time: 75.6, lr: 0.000500\n",
            "ConvMixer: Epoch: 20 | Train Acc: 0.7557, Test Acc: 0.7818, Time: 72.8, lr: 0.000400\n",
            "ConvMixer: Epoch: 21 | Train Acc: 0.7636, Test Acc: 0.7834, Time: 73.0, lr: 0.000300\n",
            "ConvMixer: Epoch: 22 | Train Acc: 0.7634, Test Acc: 0.7830, Time: 75.5, lr: 0.000200\n",
            "ConvMixer: Epoch: 23 | Train Acc: 0.7646, Test Acc: 0.7834, Time: 72.4, lr: 0.000100\n",
            "ConvMixer: Epoch: 24 | Train Acc: 0.7688, Test Acc: 0.7846, Time: 73.6, lr: 0.000000\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "clip_norm = True\n",
        "lr_schedule = lambda t: np.interp([t], [0, EPOCHS*2//5, EPOCHS*4//5, EPOCHS], \n",
        "                                  [0, 0.01, 0.01/20.0, 0])[0]\n",
        "\n",
        "model = nn.DataParallel(model, device_ids=[0]).cuda()\n",
        "opt = optim.AdamW(model.parameters(), lr=0.01, weight_decay=0.01)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "    train_loss, train_acc, n = 0, 0, 0\n",
        "    for i, (X, y) in enumerate(trainloader):\n",
        "        model.train()\n",
        "        X, y = X.cuda(), y.cuda()\n",
        "\n",
        "        lr = lr_schedule(epoch + (i + 1)/len(trainloader))\n",
        "        opt.param_groups[0].update(lr=lr)\n",
        "\n",
        "        opt.zero_grad()\n",
        "        with torch.cuda.amp.autocast():\n",
        "            output = model(X)\n",
        "            loss = criterion(output, y)\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        if clip_norm:\n",
        "            scaler.unscale_(opt)\n",
        "            nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        scaler.step(opt)\n",
        "        scaler.update()\n",
        "        \n",
        "        train_loss += loss.item() * y.size(0)\n",
        "        train_acc += (output.max(1)[1] == y).sum().item()\n",
        "        n += y.size(0)\n",
        "        \n",
        "    model.eval()\n",
        "    test_acc, m = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for i, (X, y) in enumerate(testloader):\n",
        "            X, y = X.cuda(), y.cuda()\n",
        "            with torch.cuda.amp.autocast():\n",
        "                output = model(X)\n",
        "            test_acc += (output.max(1)[1] == y).sum().item()\n",
        "            m += y.size(0)\n",
        "\n",
        "    print(f'ConvMixer: Epoch: {epoch} | Train Acc: {train_acc/n:.4f}, Test Acc: {test_acc/m:.4f}, Time: {time.time() - start:.1f}, lr: {lr:.6f}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gbtap_bCxsjM"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "b8fbfcbe0e544000e4ba3d2d9974592a7ba1a2af52205db5302ae41a0c45d995"
      }
    },
    "colab": {
      "provenance": []
    },
    "accelerator": "GPU",
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "14b6f6562bc54da19725f7336280a367": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ca0a52fee01042e086f1c338f7a7d062",
              "IPY_MODEL_d903733793c745f599e9f57f378b2746",
              "IPY_MODEL_ae0b281403064b54a94ca64bdacfcfbb"
            ],
            "layout": "IPY_MODEL_fcf4feefa880439eb1bf0edfebb3c52a"
          }
        },
        "ca0a52fee01042e086f1c338f7a7d062": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3cb95b95b2cd419287437714fc7811cc",
            "placeholder": "​",
            "style": "IPY_MODEL_fafbf1325d0445079aed3055e83a5d26",
            "value": "100%"
          }
        },
        "d903733793c745f599e9f57f378b2746": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_89ba26e743a74fbc8a6d34b20c6696cb",
            "max": 170498071,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ac58ddf43a1140f4a7bcccabdc515b67",
            "value": 170498071
          }
        },
        "ae0b281403064b54a94ca64bdacfcfbb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_77e774963d314b3fa5b8438fc208aedc",
            "placeholder": "​",
            "style": "IPY_MODEL_ea8c1a28ee9046f7aa71f64282ae2421",
            "value": " 170498071/170498071 [00:05&lt;00:00, 33284212.08it/s]"
          }
        },
        "fcf4feefa880439eb1bf0edfebb3c52a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3cb95b95b2cd419287437714fc7811cc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fafbf1325d0445079aed3055e83a5d26": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "89ba26e743a74fbc8a6d34b20c6696cb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ac58ddf43a1140f4a7bcccabdc515b67": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "77e774963d314b3fa5b8438fc208aedc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ea8c1a28ee9046f7aa71f64282ae2421": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}