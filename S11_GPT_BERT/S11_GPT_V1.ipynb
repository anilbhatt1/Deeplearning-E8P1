{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f1734221f0ce47ea91a3d90c553950e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_df76fabd5dbf4c0a8540b2e1ad2278d0",
              "IPY_MODEL_471a588ff53d4e4a91878ad7d19aa405",
              "IPY_MODEL_53a1dcfe433c466a9d8db0d5d103d2c5"
            ],
            "layout": "IPY_MODEL_4061aecc13fe4ebb931fd4f716a71125"
          }
        },
        "df76fabd5dbf4c0a8540b2e1ad2278d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f569d39051714c2aa5c65b148c6e1d1b",
            "placeholder": "​",
            "style": "IPY_MODEL_1826702a4708400aae7c831dd795407e",
            "value": "Downloading (…)okenizer_config.json: 100%"
          }
        },
        "471a588ff53d4e4a91878ad7d19aa405": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_68a927a23ec446fcb40185eb2ca00f73",
            "max": 28,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_439465290c9d464fb969b914cbdcbc98",
            "value": 28
          }
        },
        "53a1dcfe433c466a9d8db0d5d103d2c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_59a2b68fda9440e89be3fd17367f287f",
            "placeholder": "​",
            "style": "IPY_MODEL_b6040093b0aa4858bd4673dd9ebbfecd",
            "value": " 28.0/28.0 [00:00&lt;00:00, 572B/s]"
          }
        },
        "4061aecc13fe4ebb931fd4f716a71125": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f569d39051714c2aa5c65b148c6e1d1b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1826702a4708400aae7c831dd795407e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "68a927a23ec446fcb40185eb2ca00f73": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "439465290c9d464fb969b914cbdcbc98": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "59a2b68fda9440e89be3fd17367f287f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b6040093b0aa4858bd4673dd9ebbfecd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d16a329c62284e248872fa84fe27395f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c9554990701340e682a60f0be4ed084c",
              "IPY_MODEL_7f0967c0aa4445c38c44f2b15b9be554",
              "IPY_MODEL_ffc48c6b431e43d8b2befde1bb95aab5"
            ],
            "layout": "IPY_MODEL_d3f641fb425e4abeb8a84ca1dcf21236"
          }
        },
        "c9554990701340e682a60f0be4ed084c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b949d3113a124b419d109249ce98e4cf",
            "placeholder": "​",
            "style": "IPY_MODEL_61c2bfc494ba48279be734996c3e79e1",
            "value": "Downloading (…)lve/main/config.json: 100%"
          }
        },
        "7f0967c0aa4445c38c44f2b15b9be554": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_da0486816db8427aab939fefc71301c4",
            "max": 570,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fb25ee1bf602451a8504222eb8eab20e",
            "value": 570
          }
        },
        "ffc48c6b431e43d8b2befde1bb95aab5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4bc99f2044ea4e57a1fcac51e0a06fa0",
            "placeholder": "​",
            "style": "IPY_MODEL_b2b0c8ff12bc468d93bff44cda1d6854",
            "value": " 570/570 [00:00&lt;00:00, 6.30kB/s]"
          }
        },
        "d3f641fb425e4abeb8a84ca1dcf21236": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b949d3113a124b419d109249ce98e4cf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "61c2bfc494ba48279be734996c3e79e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "da0486816db8427aab939fefc71301c4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fb25ee1bf602451a8504222eb8eab20e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4bc99f2044ea4e57a1fcac51e0a06fa0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b2b0c8ff12bc468d93bff44cda1d6854": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f61e0820d1034961818383c2b8a7feaf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b56ca11f7f5c4350977c299f414e6f87",
              "IPY_MODEL_4226ca9988c0440fa276029a60d27602",
              "IPY_MODEL_4a8791d1d8c34feda6955583f20fea85"
            ],
            "layout": "IPY_MODEL_970e43783617465ba975b5910910ab7a"
          }
        },
        "b56ca11f7f5c4350977c299f414e6f87": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d573a525e05c4bfd8dc4faa0f0780c4b",
            "placeholder": "​",
            "style": "IPY_MODEL_fdb36e50fdb549c7b65217cd5aa3392f",
            "value": "Downloading (…)solve/main/vocab.txt: 100%"
          }
        },
        "4226ca9988c0440fa276029a60d27602": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_87cb126eb9c7448cb70c2d5349e1c9ac",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_391789a5e05d47d59a0ee8137396b112",
            "value": 231508
          }
        },
        "4a8791d1d8c34feda6955583f20fea85": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e72f31d8de4a4962a66daa7da2c3b1b8",
            "placeholder": "​",
            "style": "IPY_MODEL_46b369d7d9e048f4bc2e3544df9501ca",
            "value": " 232k/232k [00:00&lt;00:00, 333kB/s]"
          }
        },
        "970e43783617465ba975b5910910ab7a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d573a525e05c4bfd8dc4faa0f0780c4b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fdb36e50fdb549c7b65217cd5aa3392f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "87cb126eb9c7448cb70c2d5349e1c9ac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "391789a5e05d47d59a0ee8137396b112": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e72f31d8de4a4962a66daa7da2c3b1b8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "46b369d7d9e048f4bc2e3544df9501ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "27ea7a44d5e34775a2b539a0567edd03": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3f24f8e5bd044a678039894d3ccf9b43",
              "IPY_MODEL_6739a637d84e4403beda846fad417621",
              "IPY_MODEL_a4c1cae22ee14aba9cd34e7f1e513feb"
            ],
            "layout": "IPY_MODEL_d701f87d843a431291237a22bebb1632"
          }
        },
        "3f24f8e5bd044a678039894d3ccf9b43": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b9030829f28e4105a333e810e1d4877e",
            "placeholder": "​",
            "style": "IPY_MODEL_46ab04cc88d84a929f10f86c54080a2c",
            "value": "Downloading (…)/main/tokenizer.json: 100%"
          }
        },
        "6739a637d84e4403beda846fad417621": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cc88b42286e94e26bfa21f9f7104efb0",
            "max": 466062,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_31915eb4dccc4ec980f4345e422b5144",
            "value": 466062
          }
        },
        "a4c1cae22ee14aba9cd34e7f1e513feb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bc5efff4e6014a9ca4ca915dc7aa5d26",
            "placeholder": "​",
            "style": "IPY_MODEL_b5dae1ea798a4346a1bc67e3f7e0428e",
            "value": " 466k/466k [00:00&lt;00:00, 481kB/s]"
          }
        },
        "d701f87d843a431291237a22bebb1632": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b9030829f28e4105a333e810e1d4877e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "46ab04cc88d84a929f10f86c54080a2c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cc88b42286e94e26bfa21f9f7104efb0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "31915eb4dccc4ec980f4345e422b5144": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bc5efff4e6014a9ca4ca915dc7aa5d26": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b5dae1ea798a4346a1bc67e3f7e0428e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RXldlNFZtjU-",
        "outputId": "8d57d2e3-c44c-4249-ff00-70f5c1dbdd7a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Mar 30 03:09:08 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   47C    P8    11W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3WbU7Ltlv8qK",
        "outputId": "ceb384c4-019d-470b-86e3-80930fd7229e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "import torch.nn.functional as F\n",
        "from collections import Counter\n",
        "from os.path import exists\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "import math\n",
        "import re\n",
        "import copy"
      ],
      "metadata": {
        "id": "QSf8_DgUwKMo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r \"/content/gdrive/My Drive/EVA8_S11_Course_Docs/GPT\" \".\""
      ],
      "metadata": {
        "id": "P29kSgfswsZ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/GPT"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rQUuPuyVyHiu",
        "outputId": "5136626c-d6d7-4860-fc02-c74d611cb3e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/GPT\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import model_p\n",
        "import utils_p\n",
        "from model_p import Transformer\n",
        "from utils_p import (\n",
        "    BATCH_SIZE,\n",
        "    BLOCK_SIZE,\n",
        "    DEVICE,\n",
        "    DROPOUT,\n",
        "    LEARNING_RATE,\n",
        "    NUM_EMBED,\n",
        "    NUM_HEAD,\n",
        "    NUM_LAYER,\n",
        "    MAX_ITER,\n",
        "    EVAL_INTER,\n",
        "    encode,\n",
        "    decode,\n",
        "    get_batch,\n",
        "    save_model_to_chekpoint,\n",
        "    estimate_loss,\n",
        ")\n",
        "print(f'    BATCH_SIZE : {BATCH_SIZE}, \\\n",
        "    BLOCK_SIZE : {BLOCK_SIZE}, \\\n",
        "    DEVICE : {DEVICE}, \\\n",
        "    DROPOUT : {DROPOUT}, \\\n",
        "    LEARNING_RATE : {LEARNING_RATE}')\n",
        "print(f'NUM_EMBED : {NUM_EMBED}, \\\n",
        "    NUM_HEAD : {NUM_HEAD}, \\\n",
        "    NUM_LAYER : {NUM_LAYER}, \\\n",
        "    MAX_ITER : {MAX_ITER},\\\n",
        "    EVAL_INTER {EVAL_INTER}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FH-fMoQiw6D0",
        "outputId": "349e8cec-46d9-4858-9e51-7f685a2bf8f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    BATCH_SIZE : 2,     BLOCK_SIZE : 20,     DEVICE : cuda,     DROPOUT : 0.2,     LEARNING_RATE : 0.0003\n",
            "NUM_EMBED : 768,     NUM_HEAD : 6,     NUM_LAYER : 3,     MAX_ITER : 5,    EVAL_INTER 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QOaHTo0JxUap",
        "outputId": "3f302a01-081c-45c9-af61-9c996c619952"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.27.4-py3-none-any.whl (6.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m66.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
            "Collecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.13.3-py3-none-any.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.8/199.8 KB\u001b[0m \u001b[31m28.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.10.7)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m71.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.13.3 tokenizers-0.13.2 transformers-4.27.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer"
      ],
      "metadata": {
        "id": "cWzdE9chyQe6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load model from checkpoint\n",
        "# m = load_model_from_checkpoint(Transformer,vocab_size=vocab_size)\n",
        "\n",
        "# example to decode sequence\n",
        "# enc_sec = m.generate(idx=torch.zeros((1,1), dtype=torch.long),\n",
        "# max_new_tokens=20)[0].tolist()\n",
        "# print(decode(vocab=vocab, enc_sec=enc_sec))"
      ],
      "metadata": {
        "id": "ytjVequkyt1b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# raw data\n",
        "path_do_data = \"./data/english_small.txt\"\n",
        "data_raw = open(path_do_data, encoding=\"utf-8\").read()\n",
        "print(f'len of data_raw : {len(data_raw)}')\n",
        "# we use pretrained BERT tokenizer for performance improvements\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "vocab_size = tokenizer.vocab_size\n",
        "print(vocab_size)\n",
        "# data_raw = data_raw[4000000:] # short dataset\n",
        "\n",
        "# train/val split\n",
        "data = encode(text_seq=data_raw, tokenizer=tokenizer)\n",
        "print(f'len(data) : {len(data)}')\n",
        "n = int(0.9 * len(data))  # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "print(f'len(train_data) : {len(train_data)}')\n",
        "val_data = data[n:]\n",
        "print(f'len(val_data) : {len(val_data)}')\n",
        "\n",
        "# train a new model\n",
        "model = Transformer(\n",
        "    vocab_size=vocab_size,\n",
        "    num_embed=NUM_EMBED,\n",
        "    block_size=BLOCK_SIZE,\n",
        "    num_heads=NUM_HEAD,\n",
        "    num_layers=NUM_LAYER,\n",
        "    dropout=DROPOUT,\n",
        ")\n",
        "\n",
        "# load model to GPU if available"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391,
          "referenced_widgets": [
            "f1734221f0ce47ea91a3d90c553950e4",
            "df76fabd5dbf4c0a8540b2e1ad2278d0",
            "471a588ff53d4e4a91878ad7d19aa405",
            "53a1dcfe433c466a9d8db0d5d103d2c5",
            "4061aecc13fe4ebb931fd4f716a71125",
            "f569d39051714c2aa5c65b148c6e1d1b",
            "1826702a4708400aae7c831dd795407e",
            "68a927a23ec446fcb40185eb2ca00f73",
            "439465290c9d464fb969b914cbdcbc98",
            "59a2b68fda9440e89be3fd17367f287f",
            "b6040093b0aa4858bd4673dd9ebbfecd",
            "d16a329c62284e248872fa84fe27395f",
            "c9554990701340e682a60f0be4ed084c",
            "7f0967c0aa4445c38c44f2b15b9be554",
            "ffc48c6b431e43d8b2befde1bb95aab5",
            "d3f641fb425e4abeb8a84ca1dcf21236",
            "b949d3113a124b419d109249ce98e4cf",
            "61c2bfc494ba48279be734996c3e79e1",
            "da0486816db8427aab939fefc71301c4",
            "fb25ee1bf602451a8504222eb8eab20e",
            "4bc99f2044ea4e57a1fcac51e0a06fa0",
            "b2b0c8ff12bc468d93bff44cda1d6854",
            "f61e0820d1034961818383c2b8a7feaf",
            "b56ca11f7f5c4350977c299f414e6f87",
            "4226ca9988c0440fa276029a60d27602",
            "4a8791d1d8c34feda6955583f20fea85",
            "970e43783617465ba975b5910910ab7a",
            "d573a525e05c4bfd8dc4faa0f0780c4b",
            "fdb36e50fdb549c7b65217cd5aa3392f",
            "87cb126eb9c7448cb70c2d5349e1c9ac",
            "391789a5e05d47d59a0ee8137396b112",
            "e72f31d8de4a4962a66daa7da2c3b1b8",
            "46b369d7d9e048f4bc2e3544df9501ca",
            "27ea7a44d5e34775a2b539a0567edd03",
            "3f24f8e5bd044a678039894d3ccf9b43",
            "6739a637d84e4403beda846fad417621",
            "a4c1cae22ee14aba9cd34e7f1e513feb",
            "d701f87d843a431291237a22bebb1632",
            "b9030829f28e4105a333e810e1d4877e",
            "46ab04cc88d84a929f10f86c54080a2c",
            "cc88b42286e94e26bfa21f9f7104efb0",
            "31915eb4dccc4ec980f4345e422b5144",
            "bc5efff4e6014a9ca4ca915dc7aa5d26",
            "b5dae1ea798a4346a1bc67e3f7e0428e"
          ]
        },
        "id": "gJKcOjd0ykEN",
        "outputId": "f8348d1b-068e-4a9e-8064-c0c2b5bf4c81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "len of data_raw : 3269\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f1734221f0ce47ea91a3d90c553950e4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d16a329c62284e248872fa84fe27395f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f61e0820d1034961818383c2b8a7feaf"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "27ea7a44d5e34775a2b539a0567edd03"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (722 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "30522\n",
            " encode -> len(text_seq) : 3269 len(tokens) : 722\n",
            " encode -> len(token_indices) : 722\n",
            " encode -> token_indices.shape torch.Size([722])\n",
            "len(data) : 722\n",
            "len(train_data) : 649\n",
            "len(val_data) : 73\n",
            " TransformerBlock init : num_embed : 768, block_size : 20, num_heads : 6\n",
            " TransformerBlock init : num_embed : 768, block_size : 20, num_heads : 6\n",
            " TransformerBlock init : num_embed : 768, block_size : 20, num_heads : 6\n",
            " Transformer init self.vocab_size : 30522, self.num_embed : 768                  self.block_size : 20, self.num_heads : 6,                  self.num_layers : 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load model to GPU if available\n",
        "m = model.to('cuda')\n",
        "# print the number of parameters in the model\n",
        "print(\n",
        "    \"Model with {:.2f}M| parameters\".format(sum(p.numel() for p in m.parameters()) / 1e6)\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZWlfSER0y1nG",
        "outputId": "9915de84-ae67-4799-ffdf-80ce49285f0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model with 68.19M| parameters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(m)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bukiecrICgJt",
        "outputId": "c698906b-142c-4ea5-9de0-8a13c16044ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transformer(\n",
            "  (token_embedding_table): Embedding(30522, 768)\n",
            "  (position_embedding_table): Embedding(20, 768)\n",
            "  (blocks): Sequential(\n",
            "    (0): TransformerBlock(\n",
            "      (sa): MultiHeadAttention(\n",
            "        (heads): ModuleList(\n",
            "          (0): AttentionHead(\n",
            "            (key): Linear(in_features=768, out_features=128, bias=False)\n",
            "            (query): Linear(in_features=768, out_features=128, bias=False)\n",
            "            (value): Linear(in_features=768, out_features=128, bias=False)\n",
            "            (dropout): Dropout(p=0.2, inplace=False)\n",
            "          )\n",
            "          (1): AttentionHead(\n",
            "            (key): Linear(in_features=768, out_features=128, bias=False)\n",
            "            (query): Linear(in_features=768, out_features=128, bias=False)\n",
            "            (value): Linear(in_features=768, out_features=128, bias=False)\n",
            "            (dropout): Dropout(p=0.2, inplace=False)\n",
            "          )\n",
            "          (2): AttentionHead(\n",
            "            (key): Linear(in_features=768, out_features=128, bias=False)\n",
            "            (query): Linear(in_features=768, out_features=128, bias=False)\n",
            "            (value): Linear(in_features=768, out_features=128, bias=False)\n",
            "            (dropout): Dropout(p=0.2, inplace=False)\n",
            "          )\n",
            "          (3): AttentionHead(\n",
            "            (key): Linear(in_features=768, out_features=128, bias=False)\n",
            "            (query): Linear(in_features=768, out_features=128, bias=False)\n",
            "            (value): Linear(in_features=768, out_features=128, bias=False)\n",
            "            (dropout): Dropout(p=0.2, inplace=False)\n",
            "          )\n",
            "          (4): AttentionHead(\n",
            "            (key): Linear(in_features=768, out_features=128, bias=False)\n",
            "            (query): Linear(in_features=768, out_features=128, bias=False)\n",
            "            (value): Linear(in_features=768, out_features=128, bias=False)\n",
            "            (dropout): Dropout(p=0.2, inplace=False)\n",
            "          )\n",
            "          (5): AttentionHead(\n",
            "            (key): Linear(in_features=768, out_features=128, bias=False)\n",
            "            (query): Linear(in_features=768, out_features=128, bias=False)\n",
            "            (value): Linear(in_features=768, out_features=128, bias=False)\n",
            "            (dropout): Dropout(p=0.2, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (dropout): Dropout(p=0.2, inplace=False)\n",
            "      )\n",
            "      (ffwd): FeedForward(\n",
            "        (net): Sequential(\n",
            "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (1): ReLU()\n",
            "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (3): Dropout(p=0.2, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "    (1): TransformerBlock(\n",
            "      (sa): MultiHeadAttention(\n",
            "        (heads): ModuleList(\n",
            "          (0): AttentionHead(\n",
            "            (key): Linear(in_features=768, out_features=128, bias=False)\n",
            "            (query): Linear(in_features=768, out_features=128, bias=False)\n",
            "            (value): Linear(in_features=768, out_features=128, bias=False)\n",
            "            (dropout): Dropout(p=0.2, inplace=False)\n",
            "          )\n",
            "          (1): AttentionHead(\n",
            "            (key): Linear(in_features=768, out_features=128, bias=False)\n",
            "            (query): Linear(in_features=768, out_features=128, bias=False)\n",
            "            (value): Linear(in_features=768, out_features=128, bias=False)\n",
            "            (dropout): Dropout(p=0.2, inplace=False)\n",
            "          )\n",
            "          (2): AttentionHead(\n",
            "            (key): Linear(in_features=768, out_features=128, bias=False)\n",
            "            (query): Linear(in_features=768, out_features=128, bias=False)\n",
            "            (value): Linear(in_features=768, out_features=128, bias=False)\n",
            "            (dropout): Dropout(p=0.2, inplace=False)\n",
            "          )\n",
            "          (3): AttentionHead(\n",
            "            (key): Linear(in_features=768, out_features=128, bias=False)\n",
            "            (query): Linear(in_features=768, out_features=128, bias=False)\n",
            "            (value): Linear(in_features=768, out_features=128, bias=False)\n",
            "            (dropout): Dropout(p=0.2, inplace=False)\n",
            "          )\n",
            "          (4): AttentionHead(\n",
            "            (key): Linear(in_features=768, out_features=128, bias=False)\n",
            "            (query): Linear(in_features=768, out_features=128, bias=False)\n",
            "            (value): Linear(in_features=768, out_features=128, bias=False)\n",
            "            (dropout): Dropout(p=0.2, inplace=False)\n",
            "          )\n",
            "          (5): AttentionHead(\n",
            "            (key): Linear(in_features=768, out_features=128, bias=False)\n",
            "            (query): Linear(in_features=768, out_features=128, bias=False)\n",
            "            (value): Linear(in_features=768, out_features=128, bias=False)\n",
            "            (dropout): Dropout(p=0.2, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (dropout): Dropout(p=0.2, inplace=False)\n",
            "      )\n",
            "      (ffwd): FeedForward(\n",
            "        (net): Sequential(\n",
            "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (1): ReLU()\n",
            "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (3): Dropout(p=0.2, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "    (2): TransformerBlock(\n",
            "      (sa): MultiHeadAttention(\n",
            "        (heads): ModuleList(\n",
            "          (0): AttentionHead(\n",
            "            (key): Linear(in_features=768, out_features=128, bias=False)\n",
            "            (query): Linear(in_features=768, out_features=128, bias=False)\n",
            "            (value): Linear(in_features=768, out_features=128, bias=False)\n",
            "            (dropout): Dropout(p=0.2, inplace=False)\n",
            "          )\n",
            "          (1): AttentionHead(\n",
            "            (key): Linear(in_features=768, out_features=128, bias=False)\n",
            "            (query): Linear(in_features=768, out_features=128, bias=False)\n",
            "            (value): Linear(in_features=768, out_features=128, bias=False)\n",
            "            (dropout): Dropout(p=0.2, inplace=False)\n",
            "          )\n",
            "          (2): AttentionHead(\n",
            "            (key): Linear(in_features=768, out_features=128, bias=False)\n",
            "            (query): Linear(in_features=768, out_features=128, bias=False)\n",
            "            (value): Linear(in_features=768, out_features=128, bias=False)\n",
            "            (dropout): Dropout(p=0.2, inplace=False)\n",
            "          )\n",
            "          (3): AttentionHead(\n",
            "            (key): Linear(in_features=768, out_features=128, bias=False)\n",
            "            (query): Linear(in_features=768, out_features=128, bias=False)\n",
            "            (value): Linear(in_features=768, out_features=128, bias=False)\n",
            "            (dropout): Dropout(p=0.2, inplace=False)\n",
            "          )\n",
            "          (4): AttentionHead(\n",
            "            (key): Linear(in_features=768, out_features=128, bias=False)\n",
            "            (query): Linear(in_features=768, out_features=128, bias=False)\n",
            "            (value): Linear(in_features=768, out_features=128, bias=False)\n",
            "            (dropout): Dropout(p=0.2, inplace=False)\n",
            "          )\n",
            "          (5): AttentionHead(\n",
            "            (key): Linear(in_features=768, out_features=128, bias=False)\n",
            "            (query): Linear(in_features=768, out_features=128, bias=False)\n",
            "            (value): Linear(in_features=768, out_features=128, bias=False)\n",
            "            (dropout): Dropout(p=0.2, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (dropout): Dropout(p=0.2, inplace=False)\n",
            "      )\n",
            "      (ffwd): FeedForward(\n",
            "        (net): Sequential(\n",
            "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (1): ReLU()\n",
            "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (3): Dropout(p=0.2, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "  )\n",
            "  (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "  (lm_head): Linear(in_features=768, out_features=30522, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# optimizer takes the model's parameters and the learning rate as input,\n",
        "# and updates the parameters during the training process in order to\n",
        "# minimize the loss function.\n",
        "optimizer = torch.optim.AdamW(m.parameters(), lr=LEARNING_RATE)\n",
        "MAX_ITER = 3\n",
        "for step in range(MAX_ITER):\n",
        "    print(f'***********step : {step}**************')\n",
        "    # every EVAL_INTER evaluate the loss on train and val sets\n",
        "    if step % EVAL_INTER == 0 or step == MAX_ITER - 1:\n",
        "        print(f' Estimating Train losses - EVAL_INTER : {EVAL_INTER }')\n",
        "        loss_train = estimate_loss(\n",
        "            data=train_data, model=m, block_size=BLOCK_SIZE, batch_size=BATCH_SIZE\n",
        "        )\n",
        "        print(f' EstimatingVal losses')\n",
        "        loss_val = estimate_loss(\n",
        "            data=val_data, model=m, block_size=BLOCK_SIZE, batch_size=BATCH_SIZE\n",
        "        )\n",
        "        print(\"step {:10} | train loss {:6.4f} | val loss {:6.4f}\".format(step, loss_train, loss_val))\n",
        "\n",
        "    print(f' Resuming training with backprop')\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch(data=train_data, block_size=BLOCK_SIZE, batch_size=BATCH_SIZE)\n",
        "    logits, loss = m.forward(xb, yb)\n",
        "    # zero_grad() method sets the gradients of all parameters in the optimizer to zero\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    # backward() method on the loss variable calculates the gradients\n",
        "    # of the loss with respect to the model's parameters.\n",
        "    loss.backward()\n",
        "    # step() method on the optimizer updates the model's parameters\n",
        "    # using the calculated gradients, in order to minimize the loss.\n",
        "    optimizer.step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oIPFvtt7y749",
        "outputId": "c057af37-665f-452d-ce9a-030012855a38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "***********step : 0**************\n",
            " Estimating Train losses - EVAL_INTER : 2\n",
            "Estimate_loss model.eval() - len(data) : 649\n",
            "Estimate_loss model.eval() - k : 0\n",
            "get_batch ix.shape : torch.Size([2]), len(data) : 649\n",
            " Transformer fwd idx.shape : torch.Size([2, 20])\n",
            " Transformer fwd token_emb.shape : torch.Size([2, 20, 768])\n",
            " Transformer fwd posit_emb.shape : torch.Size([20, 768])\n",
            " Transformer fwd x.shape after token_emb + posit_emb : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after MHA x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after FFWD x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after MHA x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after FFWD x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after MHA x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after FFWD x.shape : torch.Size([2, 20, 768])\n",
            " Transformer fwd x.shape after TransformerBlock : torch.Size([2, 20, 768])\n",
            " Transformer fwd logits.shape after self.lm_head(x) : torch.Size([2, 20, 30522])\n",
            " Transformer fwd logits.shape before loss calc : torch.Size([2, 20, 30522])\n",
            " Transformer fwd logits.shape after reshape : torch.Size([40, 30522])\n",
            " Transformer fwd targets.shape after self.lm_head(x) : torch.Size([40])\n",
            "Estimate_loss model.eval() - k : 1\n",
            "get_batch ix.shape : torch.Size([2]), len(data) : 649\n",
            " Transformer fwd idx.shape : torch.Size([2, 20])\n",
            " Transformer fwd token_emb.shape : torch.Size([2, 20, 768])\n",
            " Transformer fwd posit_emb.shape : torch.Size([20, 768])\n",
            " Transformer fwd x.shape after token_emb + posit_emb : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after MHA x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after FFWD x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after MHA x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after FFWD x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after MHA x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after FFWD x.shape : torch.Size([2, 20, 768])\n",
            " Transformer fwd x.shape after TransformerBlock : torch.Size([2, 20, 768])\n",
            " Transformer fwd logits.shape after self.lm_head(x) : torch.Size([2, 20, 30522])\n",
            " Transformer fwd logits.shape before loss calc : torch.Size([2, 20, 30522])\n",
            " Transformer fwd logits.shape after reshape : torch.Size([40, 30522])\n",
            " Transformer fwd targets.shape after self.lm_head(x) : torch.Size([40])\n",
            "Estimate_loss model.eval() - k : 2\n",
            "get_batch ix.shape : torch.Size([2]), len(data) : 649\n",
            " Transformer fwd idx.shape : torch.Size([2, 20])\n",
            " Transformer fwd token_emb.shape : torch.Size([2, 20, 768])\n",
            " Transformer fwd posit_emb.shape : torch.Size([20, 768])\n",
            " Transformer fwd x.shape after token_emb + posit_emb : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after MHA x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after FFWD x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after MHA x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after FFWD x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after MHA x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after FFWD x.shape : torch.Size([2, 20, 768])\n",
            " Transformer fwd x.shape after TransformerBlock : torch.Size([2, 20, 768])\n",
            " Transformer fwd logits.shape after self.lm_head(x) : torch.Size([2, 20, 30522])\n",
            " Transformer fwd logits.shape before loss calc : torch.Size([2, 20, 30522])\n",
            " Transformer fwd logits.shape after reshape : torch.Size([40, 30522])\n",
            " Transformer fwd targets.shape after self.lm_head(x) : torch.Size([40])\n",
            "Estimate_loss model.eval() - k : 3\n",
            "get_batch ix.shape : torch.Size([2]), len(data) : 649\n",
            " Transformer fwd idx.shape : torch.Size([2, 20])\n",
            " Transformer fwd token_emb.shape : torch.Size([2, 20, 768])\n",
            " Transformer fwd posit_emb.shape : torch.Size([20, 768])\n",
            " Transformer fwd x.shape after token_emb + posit_emb : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after MHA x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after FFWD x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after MHA x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after FFWD x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after MHA x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after FFWD x.shape : torch.Size([2, 20, 768])\n",
            " Transformer fwd x.shape after TransformerBlock : torch.Size([2, 20, 768])\n",
            " Transformer fwd logits.shape after self.lm_head(x) : torch.Size([2, 20, 30522])\n",
            " Transformer fwd logits.shape before loss calc : torch.Size([2, 20, 30522])\n",
            " Transformer fwd logits.shape after reshape : torch.Size([40, 30522])\n",
            " Transformer fwd targets.shape after self.lm_head(x) : torch.Size([40])\n",
            "Estimate_loss model.eval() - k : 4\n",
            "get_batch ix.shape : torch.Size([2]), len(data) : 649\n",
            " Transformer fwd idx.shape : torch.Size([2, 20])\n",
            " Transformer fwd token_emb.shape : torch.Size([2, 20, 768])\n",
            " Transformer fwd posit_emb.shape : torch.Size([20, 768])\n",
            " Transformer fwd x.shape after token_emb + posit_emb : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after MHA x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after FFWD x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after MHA x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after FFWD x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after MHA x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after FFWD x.shape : torch.Size([2, 20, 768])\n",
            " Transformer fwd x.shape after TransformerBlock : torch.Size([2, 20, 768])\n",
            " Transformer fwd logits.shape after self.lm_head(x) : torch.Size([2, 20, 30522])\n",
            " Transformer fwd logits.shape before loss calc : torch.Size([2, 20, 30522])\n",
            " Transformer fwd logits.shape after reshape : torch.Size([40, 30522])\n",
            " Transformer fwd targets.shape after self.lm_head(x) : torch.Size([40])\n",
            "Estimate_loss model.eval() - k : 5\n",
            "get_batch ix.shape : torch.Size([2]), len(data) : 649\n",
            " Transformer fwd idx.shape : torch.Size([2, 20])\n",
            " Transformer fwd token_emb.shape : torch.Size([2, 20, 768])\n",
            " Transformer fwd posit_emb.shape : torch.Size([20, 768])\n",
            " Transformer fwd x.shape after token_emb + posit_emb : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after MHA x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after FFWD x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after MHA x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after FFWD x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after MHA x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after FFWD x.shape : torch.Size([2, 20, 768])\n",
            " Transformer fwd x.shape after TransformerBlock : torch.Size([2, 20, 768])\n",
            " Transformer fwd logits.shape after self.lm_head(x) : torch.Size([2, 20, 30522])\n",
            " Transformer fwd logits.shape before loss calc : torch.Size([2, 20, 30522])\n",
            " Transformer fwd logits.shape after reshape : torch.Size([40, 30522])\n",
            " Transformer fwd targets.shape after self.lm_head(x) : torch.Size([40])\n",
            "Estimate_loss model.eval() - k : 6\n",
            "get_batch ix.shape : torch.Size([2]), len(data) : 649\n",
            " Transformer fwd idx.shape : torch.Size([2, 20])\n",
            " Transformer fwd token_emb.shape : torch.Size([2, 20, 768])\n",
            " Transformer fwd posit_emb.shape : torch.Size([20, 768])\n",
            " Transformer fwd x.shape after token_emb + posit_emb : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after MHA x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after FFWD x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after MHA x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after FFWD x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after MHA x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after FFWD x.shape : torch.Size([2, 20, 768])\n",
            " Transformer fwd x.shape after TransformerBlock : torch.Size([2, 20, 768])\n",
            " Transformer fwd logits.shape after self.lm_head(x) : torch.Size([2, 20, 30522])\n",
            " Transformer fwd logits.shape before loss calc : torch.Size([2, 20, 30522])\n",
            " Transformer fwd logits.shape after reshape : torch.Size([40, 30522])\n",
            " Transformer fwd targets.shape after self.lm_head(x) : torch.Size([40])\n",
            "Estimate_loss model.eval() - k : 7\n",
            "get_batch ix.shape : torch.Size([2]), len(data) : 649\n",
            " Transformer fwd idx.shape : torch.Size([2, 20])\n",
            " Transformer fwd token_emb.shape : torch.Size([2, 20, 768])\n",
            " Transformer fwd posit_emb.shape : torch.Size([20, 768])\n",
            " Transformer fwd x.shape after token_emb + posit_emb : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after MHA x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after FFWD x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after MHA x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after FFWD x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after MHA x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after FFWD x.shape : torch.Size([2, 20, 768])\n",
            " Transformer fwd x.shape after TransformerBlock : torch.Size([2, 20, 768])\n",
            " Transformer fwd logits.shape after self.lm_head(x) : torch.Size([2, 20, 30522])\n",
            " Transformer fwd logits.shape before loss calc : torch.Size([2, 20, 30522])\n",
            " Transformer fwd logits.shape after reshape : torch.Size([40, 30522])\n",
            " Transformer fwd targets.shape after self.lm_head(x) : torch.Size([40])\n",
            "Estimate_loss model.eval() - k : 8\n",
            "get_batch ix.shape : torch.Size([2]), len(data) : 649\n",
            " Transformer fwd idx.shape : torch.Size([2, 20])\n",
            " Transformer fwd token_emb.shape : torch.Size([2, 20, 768])\n",
            " Transformer fwd posit_emb.shape : torch.Size([20, 768])\n",
            " Transformer fwd x.shape after token_emb + posit_emb : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after MHA x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after FFWD x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after MHA x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after FFWD x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after MHA x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after FFWD x.shape : torch.Size([2, 20, 768])\n",
            " Transformer fwd x.shape after TransformerBlock : torch.Size([2, 20, 768])\n",
            " Transformer fwd logits.shape after self.lm_head(x) : torch.Size([2, 20, 30522])\n",
            " Transformer fwd logits.shape before loss calc : torch.Size([2, 20, 30522])\n",
            " Transformer fwd logits.shape after reshape : torch.Size([40, 30522])\n",
            " Transformer fwd targets.shape after self.lm_head(x) : torch.Size([40])\n",
            "Estimate_loss model.eval() - k : 9\n",
            "get_batch ix.shape : torch.Size([2]), len(data) : 649\n",
            " Transformer fwd idx.shape : torch.Size([2, 20])\n",
            " Transformer fwd token_emb.shape : torch.Size([2, 20, 768])\n",
            " Transformer fwd posit_emb.shape : torch.Size([20, 768])\n",
            " Transformer fwd x.shape after token_emb + posit_emb : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after MHA x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after FFWD x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after MHA x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after FFWD x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after MHA x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after FFWD x.shape : torch.Size([2, 20, 768])\n",
            " Transformer fwd x.shape after TransformerBlock : torch.Size([2, 20, 768])\n",
            " Transformer fwd logits.shape after self.lm_head(x) : torch.Size([2, 20, 30522])\n",
            " Transformer fwd logits.shape before loss calc : torch.Size([2, 20, 30522])\n",
            " Transformer fwd logits.shape after reshape : torch.Size([40, 30522])\n",
            " Transformer fwd targets.shape after self.lm_head(x) : torch.Size([40])\n",
            "Estimate_loss model.train()\n",
            " EstimatingVal losses\n",
            "Estimate_loss model.eval() - len(data) : 73\n",
            "Estimate_loss model.eval() - k : 0\n",
            "get_batch ix.shape : torch.Size([2]), len(data) : 73\n",
            " Transformer fwd idx.shape : torch.Size([2, 20])\n",
            " Transformer fwd token_emb.shape : torch.Size([2, 20, 768])\n",
            " Transformer fwd posit_emb.shape : torch.Size([20, 768])\n",
            " Transformer fwd x.shape after token_emb + posit_emb : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after MHA x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after FFWD x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after MHA x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after FFWD x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after MHA x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after FFWD x.shape : torch.Size([2, 20, 768])\n",
            " Transformer fwd x.shape after TransformerBlock : torch.Size([2, 20, 768])\n",
            " Transformer fwd logits.shape after self.lm_head(x) : torch.Size([2, 20, 30522])\n",
            " Transformer fwd logits.shape before loss calc : torch.Size([2, 20, 30522])\n",
            " Transformer fwd logits.shape after reshape : torch.Size([40, 30522])\n",
            " Transformer fwd targets.shape after self.lm_head(x) : torch.Size([40])\n",
            "Estimate_loss model.eval() - k : 1\n",
            "get_batch ix.shape : torch.Size([2]), len(data) : 73\n",
            " Transformer fwd idx.shape : torch.Size([2, 20])\n",
            " Transformer fwd token_emb.shape : torch.Size([2, 20, 768])\n",
            " Transformer fwd posit_emb.shape : torch.Size([20, 768])\n",
            " Transformer fwd x.shape after token_emb + posit_emb : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after MHA x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after FFWD x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after MHA x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after FFWD x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after MHA x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after FFWD x.shape : torch.Size([2, 20, 768])\n",
            " Transformer fwd x.shape after TransformerBlock : torch.Size([2, 20, 768])\n",
            " Transformer fwd logits.shape after self.lm_head(x) : torch.Size([2, 20, 30522])\n",
            " Transformer fwd logits.shape before loss calc : torch.Size([2, 20, 30522])\n",
            " Transformer fwd logits.shape after reshape : torch.Size([40, 30522])\n",
            " Transformer fwd targets.shape after self.lm_head(x) : torch.Size([40])\n",
            "Estimate_loss model.eval() - k : 2\n",
            "get_batch ix.shape : torch.Size([2]), len(data) : 73\n",
            " Transformer fwd idx.shape : torch.Size([2, 20])\n",
            " Transformer fwd token_emb.shape : torch.Size([2, 20, 768])\n",
            " Transformer fwd posit_emb.shape : torch.Size([20, 768])\n",
            " Transformer fwd x.shape after token_emb + posit_emb : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after MHA x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after FFWD x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after MHA x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after FFWD x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after MHA x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after FFWD x.shape : torch.Size([2, 20, 768])\n",
            " Transformer fwd x.shape after TransformerBlock : torch.Size([2, 20, 768])\n",
            " Transformer fwd logits.shape after self.lm_head(x) : torch.Size([2, 20, 30522])\n",
            " Transformer fwd logits.shape before loss calc : torch.Size([2, 20, 30522])\n",
            " Transformer fwd logits.shape after reshape : torch.Size([40, 30522])\n",
            " Transformer fwd targets.shape after self.lm_head(x) : torch.Size([40])\n",
            "Estimate_loss model.eval() - k : 3\n",
            "get_batch ix.shape : torch.Size([2]), len(data) : 73\n",
            " Transformer fwd idx.shape : torch.Size([2, 20])\n",
            " Transformer fwd token_emb.shape : torch.Size([2, 20, 768])\n",
            " Transformer fwd posit_emb.shape : torch.Size([20, 768])\n",
            " Transformer fwd x.shape after token_emb + posit_emb : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after MHA x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after FFWD x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after MHA x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after FFWD x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after MHA x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after FFWD x.shape : torch.Size([2, 20, 768])\n",
            " Transformer fwd x.shape after TransformerBlock : torch.Size([2, 20, 768])\n",
            " Transformer fwd logits.shape after self.lm_head(x) : torch.Size([2, 20, 30522])\n",
            " Transformer fwd logits.shape before loss calc : torch.Size([2, 20, 30522])\n",
            " Transformer fwd logits.shape after reshape : torch.Size([40, 30522])\n",
            " Transformer fwd targets.shape after self.lm_head(x) : torch.Size([40])\n",
            "Estimate_loss model.eval() - k : 4\n",
            "get_batch ix.shape : torch.Size([2]), len(data) : 73\n",
            " Transformer fwd idx.shape : torch.Size([2, 20])\n",
            " Transformer fwd token_emb.shape : torch.Size([2, 20, 768])\n",
            " Transformer fwd posit_emb.shape : torch.Size([20, 768])\n",
            " Transformer fwd x.shape after token_emb + posit_emb : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after MHA x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after FFWD x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after MHA x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after FFWD x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after MHA x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after FFWD x.shape : torch.Size([2, 20, 768])\n",
            " Transformer fwd x.shape after TransformerBlock : torch.Size([2, 20, 768])\n",
            " Transformer fwd logits.shape after self.lm_head(x) : torch.Size([2, 20, 30522])\n",
            " Transformer fwd logits.shape before loss calc : torch.Size([2, 20, 30522])\n",
            " Transformer fwd logits.shape after reshape : torch.Size([40, 30522])\n",
            " Transformer fwd targets.shape after self.lm_head(x) : torch.Size([40])\n",
            "Estimate_loss model.eval() - k : 5\n",
            "get_batch ix.shape : torch.Size([2]), len(data) : 73\n",
            " Transformer fwd idx.shape : torch.Size([2, 20])\n",
            " Transformer fwd token_emb.shape : torch.Size([2, 20, 768])\n",
            " Transformer fwd posit_emb.shape : torch.Size([20, 768])\n",
            " Transformer fwd x.shape after token_emb + posit_emb : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after MHA x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after FFWD x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after MHA x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after FFWD x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after MHA x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after FFWD x.shape : torch.Size([2, 20, 768])\n",
            " Transformer fwd x.shape after TransformerBlock : torch.Size([2, 20, 768])\n",
            " Transformer fwd logits.shape after self.lm_head(x) : torch.Size([2, 20, 30522])\n",
            " Transformer fwd logits.shape before loss calc : torch.Size([2, 20, 30522])\n",
            " Transformer fwd logits.shape after reshape : torch.Size([40, 30522])\n",
            " Transformer fwd targets.shape after self.lm_head(x) : torch.Size([40])\n",
            "Estimate_loss model.eval() - k : 6\n",
            "get_batch ix.shape : torch.Size([2]), len(data) : 73\n",
            " Transformer fwd idx.shape : torch.Size([2, 20])\n",
            " Transformer fwd token_emb.shape : torch.Size([2, 20, 768])\n",
            " Transformer fwd posit_emb.shape : torch.Size([20, 768])\n",
            " Transformer fwd x.shape after token_emb + posit_emb : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after MHA x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after FFWD x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after MHA x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after FFWD x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after MHA x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after FFWD x.shape : torch.Size([2, 20, 768])\n",
            " Transformer fwd x.shape after TransformerBlock : torch.Size([2, 20, 768])\n",
            " Transformer fwd logits.shape after self.lm_head(x) : torch.Size([2, 20, 30522])\n",
            " Transformer fwd logits.shape before loss calc : torch.Size([2, 20, 30522])\n",
            " Transformer fwd logits.shape after reshape : torch.Size([40, 30522])\n",
            " Transformer fwd targets.shape after self.lm_head(x) : torch.Size([40])\n",
            "Estimate_loss model.eval() - k : 7\n",
            "get_batch ix.shape : torch.Size([2]), len(data) : 73\n",
            " Transformer fwd idx.shape : torch.Size([2, 20])\n",
            " Transformer fwd token_emb.shape : torch.Size([2, 20, 768])\n",
            " Transformer fwd posit_emb.shape : torch.Size([20, 768])\n",
            " Transformer fwd x.shape after token_emb + posit_emb : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after MHA x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after FFWD x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after MHA x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after FFWD x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after MHA x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after FFWD x.shape : torch.Size([2, 20, 768])\n",
            " Transformer fwd x.shape after TransformerBlock : torch.Size([2, 20, 768])\n",
            " Transformer fwd logits.shape after self.lm_head(x) : torch.Size([2, 20, 30522])\n",
            " Transformer fwd logits.shape before loss calc : torch.Size([2, 20, 30522])\n",
            " Transformer fwd logits.shape after reshape : torch.Size([40, 30522])\n",
            " Transformer fwd targets.shape after self.lm_head(x) : torch.Size([40])\n",
            "Estimate_loss model.eval() - k : 8\n",
            "get_batch ix.shape : torch.Size([2]), len(data) : 73\n",
            " Transformer fwd idx.shape : torch.Size([2, 20])\n",
            " Transformer fwd token_emb.shape : torch.Size([2, 20, 768])\n",
            " Transformer fwd posit_emb.shape : torch.Size([20, 768])\n",
            " Transformer fwd x.shape after token_emb + posit_emb : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after MHA x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after FFWD x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after MHA x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after FFWD x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after MHA x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after FFWD x.shape : torch.Size([2, 20, 768])\n",
            " Transformer fwd x.shape after TransformerBlock : torch.Size([2, 20, 768])\n",
            " Transformer fwd logits.shape after self.lm_head(x) : torch.Size([2, 20, 30522])\n",
            " Transformer fwd logits.shape before loss calc : torch.Size([2, 20, 30522])\n",
            " Transformer fwd logits.shape after reshape : torch.Size([40, 30522])\n",
            " Transformer fwd targets.shape after self.lm_head(x) : torch.Size([40])\n",
            "Estimate_loss model.eval() - k : 9\n",
            "get_batch ix.shape : torch.Size([2]), len(data) : 73\n",
            " Transformer fwd idx.shape : torch.Size([2, 20])\n",
            " Transformer fwd token_emb.shape : torch.Size([2, 20, 768])\n",
            " Transformer fwd posit_emb.shape : torch.Size([20, 768])\n",
            " Transformer fwd x.shape after token_emb + posit_emb : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after MHA x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after FFWD x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after MHA x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after FFWD x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after MHA x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after FFWD x.shape : torch.Size([2, 20, 768])\n",
            " Transformer fwd x.shape after TransformerBlock : torch.Size([2, 20, 768])\n",
            " Transformer fwd logits.shape after self.lm_head(x) : torch.Size([2, 20, 30522])\n",
            " Transformer fwd logits.shape before loss calc : torch.Size([2, 20, 30522])\n",
            " Transformer fwd logits.shape after reshape : torch.Size([40, 30522])\n",
            " Transformer fwd targets.shape after self.lm_head(x) : torch.Size([40])\n",
            "Estimate_loss model.train()\n",
            "step          0 | train loss 10.7745 | val loss 10.6386\n",
            " Resuming training with backprop\n",
            "get_batch ix.shape : torch.Size([2]), len(data) : 649\n",
            " Transformer fwd idx.shape : torch.Size([2, 20])\n",
            " Transformer fwd token_emb.shape : torch.Size([2, 20, 768])\n",
            " Transformer fwd posit_emb.shape : torch.Size([20, 768])\n",
            " Transformer fwd x.shape after token_emb + posit_emb : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after MHA x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after FFWD x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after MHA x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after FFWD x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after MHA x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after FFWD x.shape : torch.Size([2, 20, 768])\n",
            " Transformer fwd x.shape after TransformerBlock : torch.Size([2, 20, 768])\n",
            " Transformer fwd logits.shape after self.lm_head(x) : torch.Size([2, 20, 30522])\n",
            " Transformer fwd logits.shape before loss calc : torch.Size([2, 20, 30522])\n",
            " Transformer fwd logits.shape after reshape : torch.Size([40, 30522])\n",
            " Transformer fwd targets.shape after self.lm_head(x) : torch.Size([40])\n",
            "***********step : 1**************\n",
            " Resuming training with backprop\n",
            "get_batch ix.shape : torch.Size([2]), len(data) : 649\n",
            " Transformer fwd idx.shape : torch.Size([2, 20])\n",
            " Transformer fwd token_emb.shape : torch.Size([2, 20, 768])\n",
            " Transformer fwd posit_emb.shape : torch.Size([20, 768])\n",
            " Transformer fwd x.shape after token_emb + posit_emb : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after MHA x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after FFWD x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after MHA x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after FFWD x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after MHA x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after FFWD x.shape : torch.Size([2, 20, 768])\n",
            " Transformer fwd x.shape after TransformerBlock : torch.Size([2, 20, 768])\n",
            " Transformer fwd logits.shape after self.lm_head(x) : torch.Size([2, 20, 30522])\n",
            " Transformer fwd logits.shape before loss calc : torch.Size([2, 20, 30522])\n",
            " Transformer fwd logits.shape after reshape : torch.Size([40, 30522])\n",
            " Transformer fwd targets.shape after self.lm_head(x) : torch.Size([40])\n",
            "***********step : 2**************\n",
            " Estimating Train losses - EVAL_INTER : 2\n",
            "Estimate_loss model.eval() - len(data) : 649\n",
            "Estimate_loss model.eval() - k : 0\n",
            "get_batch ix.shape : torch.Size([2]), len(data) : 649\n",
            " Transformer fwd idx.shape : torch.Size([2, 20])\n",
            " Transformer fwd token_emb.shape : torch.Size([2, 20, 768])\n",
            " Transformer fwd posit_emb.shape : torch.Size([20, 768])\n",
            " Transformer fwd x.shape after token_emb + posit_emb : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after MHA x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after FFWD x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after MHA x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after FFWD x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after MHA x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after FFWD x.shape : torch.Size([2, 20, 768])\n",
            " Transformer fwd x.shape after TransformerBlock : torch.Size([2, 20, 768])\n",
            " Transformer fwd logits.shape after self.lm_head(x) : torch.Size([2, 20, 30522])\n",
            " Transformer fwd logits.shape before loss calc : torch.Size([2, 20, 30522])\n",
            " Transformer fwd logits.shape after reshape : torch.Size([40, 30522])\n",
            " Transformer fwd targets.shape after self.lm_head(x) : torch.Size([40])\n",
            "Estimate_loss model.eval() - k : 1\n",
            "get_batch ix.shape : torch.Size([2]), len(data) : 649\n",
            " Transformer fwd idx.shape : torch.Size([2, 20])\n",
            " Transformer fwd token_emb.shape : torch.Size([2, 20, 768])\n",
            " Transformer fwd posit_emb.shape : torch.Size([20, 768])\n",
            " Transformer fwd x.shape after token_emb + posit_emb : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after MHA x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after FFWD x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after MHA x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after FFWD x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after MHA x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after FFWD x.shape : torch.Size([2, 20, 768])\n",
            " Transformer fwd x.shape after TransformerBlock : torch.Size([2, 20, 768])\n",
            " Transformer fwd logits.shape after self.lm_head(x) : torch.Size([2, 20, 30522])\n",
            " Transformer fwd logits.shape before loss calc : torch.Size([2, 20, 30522])\n",
            " Transformer fwd logits.shape after reshape : torch.Size([40, 30522])\n",
            " Transformer fwd targets.shape after self.lm_head(x) : torch.Size([40])\n",
            "Estimate_loss model.eval() - k : 2\n",
            "get_batch ix.shape : torch.Size([2]), len(data) : 649\n",
            " Transformer fwd idx.shape : torch.Size([2, 20])\n",
            " Transformer fwd token_emb.shape : torch.Size([2, 20, 768])\n",
            " Transformer fwd posit_emb.shape : torch.Size([20, 768])\n",
            " Transformer fwd x.shape after token_emb + posit_emb : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after MHA x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after FFWD x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after MHA x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after FFWD x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after MHA x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after FFWD x.shape : torch.Size([2, 20, 768])\n",
            " Transformer fwd x.shape after TransformerBlock : torch.Size([2, 20, 768])\n",
            " Transformer fwd logits.shape after self.lm_head(x) : torch.Size([2, 20, 30522])\n",
            " Transformer fwd logits.shape before loss calc : torch.Size([2, 20, 30522])\n",
            " Transformer fwd logits.shape after reshape : torch.Size([40, 30522])\n",
            " Transformer fwd targets.shape after self.lm_head(x) : torch.Size([40])\n",
            "Estimate_loss model.eval() - k : 3\n",
            "get_batch ix.shape : torch.Size([2]), len(data) : 649\n",
            " Transformer fwd idx.shape : torch.Size([2, 20])\n",
            " Transformer fwd token_emb.shape : torch.Size([2, 20, 768])\n",
            " Transformer fwd posit_emb.shape : torch.Size([20, 768])\n",
            " Transformer fwd x.shape after token_emb + posit_emb : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after MHA x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after FFWD x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after MHA x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after FFWD x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after MHA x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after FFWD x.shape : torch.Size([2, 20, 768])\n",
            " Transformer fwd x.shape after TransformerBlock : torch.Size([2, 20, 768])\n",
            " Transformer fwd logits.shape after self.lm_head(x) : torch.Size([2, 20, 30522])\n",
            " Transformer fwd logits.shape before loss calc : torch.Size([2, 20, 30522])\n",
            " Transformer fwd logits.shape after reshape : torch.Size([40, 30522])\n",
            " Transformer fwd targets.shape after self.lm_head(x) : torch.Size([40])\n",
            "Estimate_loss model.eval() - k : 4\n",
            "get_batch ix.shape : torch.Size([2]), len(data) : 649\n",
            " Transformer fwd idx.shape : torch.Size([2, 20])\n",
            " Transformer fwd token_emb.shape : torch.Size([2, 20, 768])\n",
            " Transformer fwd posit_emb.shape : torch.Size([20, 768])\n",
            " Transformer fwd x.shape after token_emb + posit_emb : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after MHA x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after FFWD x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after MHA x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after FFWD x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after MHA x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after FFWD x.shape : torch.Size([2, 20, 768])\n",
            " Transformer fwd x.shape after TransformerBlock : torch.Size([2, 20, 768])\n",
            " Transformer fwd logits.shape after self.lm_head(x) : torch.Size([2, 20, 30522])\n",
            " Transformer fwd logits.shape before loss calc : torch.Size([2, 20, 30522])\n",
            " Transformer fwd logits.shape after reshape : torch.Size([40, 30522])\n",
            " Transformer fwd targets.shape after self.lm_head(x) : torch.Size([40])\n",
            "Estimate_loss model.eval() - k : 5\n",
            "get_batch ix.shape : torch.Size([2]), len(data) : 649\n",
            " Transformer fwd idx.shape : torch.Size([2, 20])\n",
            " Transformer fwd token_emb.shape : torch.Size([2, 20, 768])\n",
            " Transformer fwd posit_emb.shape : torch.Size([20, 768])\n",
            " Transformer fwd x.shape after token_emb + posit_emb : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after MHA x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after FFWD x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after MHA x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after FFWD x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after MHA x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after FFWD x.shape : torch.Size([2, 20, 768])\n",
            " Transformer fwd x.shape after TransformerBlock : torch.Size([2, 20, 768])\n",
            " Transformer fwd logits.shape after self.lm_head(x) : torch.Size([2, 20, 30522])\n",
            " Transformer fwd logits.shape before loss calc : torch.Size([2, 20, 30522])\n",
            " Transformer fwd logits.shape after reshape : torch.Size([40, 30522])\n",
            " Transformer fwd targets.shape after self.lm_head(x) : torch.Size([40])\n",
            "Estimate_loss model.eval() - k : 6\n",
            "get_batch ix.shape : torch.Size([2]), len(data) : 649\n",
            " Transformer fwd idx.shape : torch.Size([2, 20])\n",
            " Transformer fwd token_emb.shape : torch.Size([2, 20, 768])\n",
            " Transformer fwd posit_emb.shape : torch.Size([20, 768])\n",
            " Transformer fwd x.shape after token_emb + posit_emb : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after MHA x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after FFWD x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after MHA x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after FFWD x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after MHA x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after FFWD x.shape : torch.Size([2, 20, 768])\n",
            " Transformer fwd x.shape after TransformerBlock : torch.Size([2, 20, 768])\n",
            " Transformer fwd logits.shape after self.lm_head(x) : torch.Size([2, 20, 30522])\n",
            " Transformer fwd logits.shape before loss calc : torch.Size([2, 20, 30522])\n",
            " Transformer fwd logits.shape after reshape : torch.Size([40, 30522])\n",
            " Transformer fwd targets.shape after self.lm_head(x) : torch.Size([40])\n",
            "Estimate_loss model.eval() - k : 7\n",
            "get_batch ix.shape : torch.Size([2]), len(data) : 649\n",
            " Transformer fwd idx.shape : torch.Size([2, 20])\n",
            " Transformer fwd token_emb.shape : torch.Size([2, 20, 768])\n",
            " Transformer fwd posit_emb.shape : torch.Size([20, 768])\n",
            " Transformer fwd x.shape after token_emb + posit_emb : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after MHA x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after FFWD x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after MHA x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after FFWD x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after MHA x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after FFWD x.shape : torch.Size([2, 20, 768])\n",
            " Transformer fwd x.shape after TransformerBlock : torch.Size([2, 20, 768])\n",
            " Transformer fwd logits.shape after self.lm_head(x) : torch.Size([2, 20, 30522])\n",
            " Transformer fwd logits.shape before loss calc : torch.Size([2, 20, 30522])\n",
            " Transformer fwd logits.shape after reshape : torch.Size([40, 30522])\n",
            " Transformer fwd targets.shape after self.lm_head(x) : torch.Size([40])\n",
            "Estimate_loss model.eval() - k : 8\n",
            "get_batch ix.shape : torch.Size([2]), len(data) : 649\n",
            " Transformer fwd idx.shape : torch.Size([2, 20])\n",
            " Transformer fwd token_emb.shape : torch.Size([2, 20, 768])\n",
            " Transformer fwd posit_emb.shape : torch.Size([20, 768])\n",
            " Transformer fwd x.shape after token_emb + posit_emb : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after MHA x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after FFWD x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after MHA x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after FFWD x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after MHA x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after FFWD x.shape : torch.Size([2, 20, 768])\n",
            " Transformer fwd x.shape after TransformerBlock : torch.Size([2, 20, 768])\n",
            " Transformer fwd logits.shape after self.lm_head(x) : torch.Size([2, 20, 30522])\n",
            " Transformer fwd logits.shape before loss calc : torch.Size([2, 20, 30522])\n",
            " Transformer fwd logits.shape after reshape : torch.Size([40, 30522])\n",
            " Transformer fwd targets.shape after self.lm_head(x) : torch.Size([40])\n",
            "Estimate_loss model.eval() - k : 9\n",
            "get_batch ix.shape : torch.Size([2]), len(data) : 649\n",
            " Transformer fwd idx.shape : torch.Size([2, 20])\n",
            " Transformer fwd token_emb.shape : torch.Size([2, 20, 768])\n",
            " Transformer fwd posit_emb.shape : torch.Size([20, 768])\n",
            " Transformer fwd x.shape after token_emb + posit_emb : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after MHA x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after FFWD x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after MHA x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after FFWD x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after MHA x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after FFWD x.shape : torch.Size([2, 20, 768])\n",
            " Transformer fwd x.shape after TransformerBlock : torch.Size([2, 20, 768])\n",
            " Transformer fwd logits.shape after self.lm_head(x) : torch.Size([2, 20, 30522])\n",
            " Transformer fwd logits.shape before loss calc : torch.Size([2, 20, 30522])\n",
            " Transformer fwd logits.shape after reshape : torch.Size([40, 30522])\n",
            " Transformer fwd targets.shape after self.lm_head(x) : torch.Size([40])\n",
            "Estimate_loss model.train()\n",
            " EstimatingVal losses\n",
            "Estimate_loss model.eval() - len(data) : 73\n",
            "Estimate_loss model.eval() - k : 0\n",
            "get_batch ix.shape : torch.Size([2]), len(data) : 73\n",
            " Transformer fwd idx.shape : torch.Size([2, 20])\n",
            " Transformer fwd token_emb.shape : torch.Size([2, 20, 768])\n",
            " Transformer fwd posit_emb.shape : torch.Size([20, 768])\n",
            " Transformer fwd x.shape after token_emb + posit_emb : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after MHA x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after FFWD x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after MHA x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after FFWD x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after MHA x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after FFWD x.shape : torch.Size([2, 20, 768])\n",
            " Transformer fwd x.shape after TransformerBlock : torch.Size([2, 20, 768])\n",
            " Transformer fwd logits.shape after self.lm_head(x) : torch.Size([2, 20, 30522])\n",
            " Transformer fwd logits.shape before loss calc : torch.Size([2, 20, 30522])\n",
            " Transformer fwd logits.shape after reshape : torch.Size([40, 30522])\n",
            " Transformer fwd targets.shape after self.lm_head(x) : torch.Size([40])\n",
            "Estimate_loss model.eval() - k : 1\n",
            "get_batch ix.shape : torch.Size([2]), len(data) : 73\n",
            " Transformer fwd idx.shape : torch.Size([2, 20])\n",
            " Transformer fwd token_emb.shape : torch.Size([2, 20, 768])\n",
            " Transformer fwd posit_emb.shape : torch.Size([20, 768])\n",
            " Transformer fwd x.shape after token_emb + posit_emb : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after MHA x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after FFWD x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after MHA x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after FFWD x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after MHA x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after FFWD x.shape : torch.Size([2, 20, 768])\n",
            " Transformer fwd x.shape after TransformerBlock : torch.Size([2, 20, 768])\n",
            " Transformer fwd logits.shape after self.lm_head(x) : torch.Size([2, 20, 30522])\n",
            " Transformer fwd logits.shape before loss calc : torch.Size([2, 20, 30522])\n",
            " Transformer fwd logits.shape after reshape : torch.Size([40, 30522])\n",
            " Transformer fwd targets.shape after self.lm_head(x) : torch.Size([40])\n",
            "Estimate_loss model.eval() - k : 2\n",
            "get_batch ix.shape : torch.Size([2]), len(data) : 73\n",
            " Transformer fwd idx.shape : torch.Size([2, 20])\n",
            " Transformer fwd token_emb.shape : torch.Size([2, 20, 768])\n",
            " Transformer fwd posit_emb.shape : torch.Size([20, 768])\n",
            " Transformer fwd x.shape after token_emb + posit_emb : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after MHA x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after FFWD x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after MHA x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after FFWD x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after MHA x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after FFWD x.shape : torch.Size([2, 20, 768])\n",
            " Transformer fwd x.shape after TransformerBlock : torch.Size([2, 20, 768])\n",
            " Transformer fwd logits.shape after self.lm_head(x) : torch.Size([2, 20, 30522])\n",
            " Transformer fwd logits.shape before loss calc : torch.Size([2, 20, 30522])\n",
            " Transformer fwd logits.shape after reshape : torch.Size([40, 30522])\n",
            " Transformer fwd targets.shape after self.lm_head(x) : torch.Size([40])\n",
            "Estimate_loss model.eval() - k : 3\n",
            "get_batch ix.shape : torch.Size([2]), len(data) : 73\n",
            " Transformer fwd idx.shape : torch.Size([2, 20])\n",
            " Transformer fwd token_emb.shape : torch.Size([2, 20, 768])\n",
            " Transformer fwd posit_emb.shape : torch.Size([20, 768])\n",
            " Transformer fwd x.shape after token_emb + posit_emb : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after MHA x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after FFWD x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after MHA x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after FFWD x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after MHA x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after FFWD x.shape : torch.Size([2, 20, 768])\n",
            " Transformer fwd x.shape after TransformerBlock : torch.Size([2, 20, 768])\n",
            " Transformer fwd logits.shape after self.lm_head(x) : torch.Size([2, 20, 30522])\n",
            " Transformer fwd logits.shape before loss calc : torch.Size([2, 20, 30522])\n",
            " Transformer fwd logits.shape after reshape : torch.Size([40, 30522])\n",
            " Transformer fwd targets.shape after self.lm_head(x) : torch.Size([40])\n",
            "Estimate_loss model.eval() - k : 4\n",
            "get_batch ix.shape : torch.Size([2]), len(data) : 73\n",
            " Transformer fwd idx.shape : torch.Size([2, 20])\n",
            " Transformer fwd token_emb.shape : torch.Size([2, 20, 768])\n",
            " Transformer fwd posit_emb.shape : torch.Size([20, 768])\n",
            " Transformer fwd x.shape after token_emb + posit_emb : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after MHA x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after FFWD x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after MHA x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after FFWD x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after MHA x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after FFWD x.shape : torch.Size([2, 20, 768])\n",
            " Transformer fwd x.shape after TransformerBlock : torch.Size([2, 20, 768])\n",
            " Transformer fwd logits.shape after self.lm_head(x) : torch.Size([2, 20, 30522])\n",
            " Transformer fwd logits.shape before loss calc : torch.Size([2, 20, 30522])\n",
            " Transformer fwd logits.shape after reshape : torch.Size([40, 30522])\n",
            " Transformer fwd targets.shape after self.lm_head(x) : torch.Size([40])\n",
            "Estimate_loss model.eval() - k : 5\n",
            "get_batch ix.shape : torch.Size([2]), len(data) : 73\n",
            " Transformer fwd idx.shape : torch.Size([2, 20])\n",
            " Transformer fwd token_emb.shape : torch.Size([2, 20, 768])\n",
            " Transformer fwd posit_emb.shape : torch.Size([20, 768])\n",
            " Transformer fwd x.shape after token_emb + posit_emb : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after MHA x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after FFWD x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after MHA x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after FFWD x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after MHA x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after FFWD x.shape : torch.Size([2, 20, 768])\n",
            " Transformer fwd x.shape after TransformerBlock : torch.Size([2, 20, 768])\n",
            " Transformer fwd logits.shape after self.lm_head(x) : torch.Size([2, 20, 30522])\n",
            " Transformer fwd logits.shape before loss calc : torch.Size([2, 20, 30522])\n",
            " Transformer fwd logits.shape after reshape : torch.Size([40, 30522])\n",
            " Transformer fwd targets.shape after self.lm_head(x) : torch.Size([40])\n",
            "Estimate_loss model.eval() - k : 6\n",
            "get_batch ix.shape : torch.Size([2]), len(data) : 73\n",
            " Transformer fwd idx.shape : torch.Size([2, 20])\n",
            " Transformer fwd token_emb.shape : torch.Size([2, 20, 768])\n",
            " Transformer fwd posit_emb.shape : torch.Size([20, 768])\n",
            " Transformer fwd x.shape after token_emb + posit_emb : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after MHA x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after FFWD x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after MHA x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after FFWD x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after MHA x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after FFWD x.shape : torch.Size([2, 20, 768])\n",
            " Transformer fwd x.shape after TransformerBlock : torch.Size([2, 20, 768])\n",
            " Transformer fwd logits.shape after self.lm_head(x) : torch.Size([2, 20, 30522])\n",
            " Transformer fwd logits.shape before loss calc : torch.Size([2, 20, 30522])\n",
            " Transformer fwd logits.shape after reshape : torch.Size([40, 30522])\n",
            " Transformer fwd targets.shape after self.lm_head(x) : torch.Size([40])\n",
            "Estimate_loss model.eval() - k : 7\n",
            "get_batch ix.shape : torch.Size([2]), len(data) : 73\n",
            " Transformer fwd idx.shape : torch.Size([2, 20])\n",
            " Transformer fwd token_emb.shape : torch.Size([2, 20, 768])\n",
            " Transformer fwd posit_emb.shape : torch.Size([20, 768])\n",
            " Transformer fwd x.shape after token_emb + posit_emb : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after MHA x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after FFWD x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after MHA x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after FFWD x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after MHA x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after FFWD x.shape : torch.Size([2, 20, 768])\n",
            " Transformer fwd x.shape after TransformerBlock : torch.Size([2, 20, 768])\n",
            " Transformer fwd logits.shape after self.lm_head(x) : torch.Size([2, 20, 30522])\n",
            " Transformer fwd logits.shape before loss calc : torch.Size([2, 20, 30522])\n",
            " Transformer fwd logits.shape after reshape : torch.Size([40, 30522])\n",
            " Transformer fwd targets.shape after self.lm_head(x) : torch.Size([40])\n",
            "Estimate_loss model.eval() - k : 8\n",
            "get_batch ix.shape : torch.Size([2]), len(data) : 73\n",
            " Transformer fwd idx.shape : torch.Size([2, 20])\n",
            " Transformer fwd token_emb.shape : torch.Size([2, 20, 768])\n",
            " Transformer fwd posit_emb.shape : torch.Size([20, 768])\n",
            " Transformer fwd x.shape after token_emb + posit_emb : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after MHA x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after FFWD x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after MHA x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after FFWD x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after MHA x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after FFWD x.shape : torch.Size([2, 20, 768])\n",
            " Transformer fwd x.shape after TransformerBlock : torch.Size([2, 20, 768])\n",
            " Transformer fwd logits.shape after self.lm_head(x) : torch.Size([2, 20, 30522])\n",
            " Transformer fwd logits.shape before loss calc : torch.Size([2, 20, 30522])\n",
            " Transformer fwd logits.shape after reshape : torch.Size([40, 30522])\n",
            " Transformer fwd targets.shape after self.lm_head(x) : torch.Size([40])\n",
            "Estimate_loss model.eval() - k : 9\n",
            "get_batch ix.shape : torch.Size([2]), len(data) : 73\n",
            " Transformer fwd idx.shape : torch.Size([2, 20])\n",
            " Transformer fwd token_emb.shape : torch.Size([2, 20, 768])\n",
            " Transformer fwd posit_emb.shape : torch.Size([20, 768])\n",
            " Transformer fwd x.shape after token_emb + posit_emb : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after MHA x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after FFWD x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after MHA x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after FFWD x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after MHA x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after FFWD x.shape : torch.Size([2, 20, 768])\n",
            " Transformer fwd x.shape after TransformerBlock : torch.Size([2, 20, 768])\n",
            " Transformer fwd logits.shape after self.lm_head(x) : torch.Size([2, 20, 30522])\n",
            " Transformer fwd logits.shape before loss calc : torch.Size([2, 20, 30522])\n",
            " Transformer fwd logits.shape after reshape : torch.Size([40, 30522])\n",
            " Transformer fwd targets.shape after self.lm_head(x) : torch.Size([40])\n",
            "Estimate_loss model.train()\n",
            "step          2 | train loss 9.7652 | val loss 9.9725\n",
            " Resuming training with backprop\n",
            "get_batch ix.shape : torch.Size([2]), len(data) : 649\n",
            " Transformer fwd idx.shape : torch.Size([2, 20])\n",
            " Transformer fwd token_emb.shape : torch.Size([2, 20, 768])\n",
            " Transformer fwd posit_emb.shape : torch.Size([20, 768])\n",
            " Transformer fwd x.shape after token_emb + posit_emb : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after MHA x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after FFWD x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after MHA x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after FFWD x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after MHA x.shape : torch.Size([2, 20, 768])\n",
            "    TransformerBlock fwd after FFWD x.shape : torch.Size([2, 20, 768])\n",
            " Transformer fwd x.shape after TransformerBlock : torch.Size([2, 20, 768])\n",
            " Transformer fwd logits.shape after self.lm_head(x) : torch.Size([2, 20, 30522])\n",
            " Transformer fwd logits.shape before loss calc : torch.Size([2, 20, 30522])\n",
            " Transformer fwd logits.shape after reshape : torch.Size([40, 30522])\n",
            " Transformer fwd targets.shape after self.lm_head(x) : torch.Size([40])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "save_model_to_chekpoint(model=m, path_to_checkpoint=\"checkpoint\", epoch=step)"
      ],
      "metadata": {
        "id": "mMXhNNGNzAlD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generate some output based on the context\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=DEVICE)\n",
        "print(\n",
        "    decode(\n",
        "        enc_sec=m.generate(idx=context, max_new_tokens=100, block_size=BLOCK_SIZE)[0],\n",
        "        tokenizer=tokenizer,\n",
        "    )\n",
        ")"
      ],
      "metadata": {
        "id": "cTI81WbvzCYf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}