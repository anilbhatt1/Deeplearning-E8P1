{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jjWcv_ZOhpqK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3aaa8a7d-b318-481b-ecf2-5ae9fb070b87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Mar 26 13:51:52 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   57C    P8    11W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4znsvw6i30zW",
        "outputId": "50ddb3e5-c487-4802-b093-0319fbfb3819"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r \"/content/gdrive/My Drive/EVA8_S11_Course_Docs/BERT\" \".\""
      ],
      "metadata": {
        "id": "__NCPl1T330S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "import torch.nn.functional as F\n",
        "from collections import Counter\n",
        "from os.path import exists\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "import math\n",
        "import re\n",
        "import copy"
      ],
      "metadata": {
        "id": "wsz00AFnr2Kl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# Transformer\n",
        "# =============================================================================\n",
        "def attention(q, k, v, mask = None, dropout = None):\n",
        "    scores = q.matmul(k.transpose(-2, -1))\n",
        "    scores /= math.sqrt(q.shape[-1])\n",
        "\n",
        "    #mask\n",
        "    scores = scores if mask is None else scores.masked_fill(mask == 0, -1e3)\n",
        "\n",
        "    scores = F.softmax(scores, dim = -1)\n",
        "    scores = dropout(scores) if dropout is not None else scores\n",
        "    output = scores.matmul(v)\n",
        "    return output\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, n_heads, out_dim, dropout=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.linear = nn.Linear(out_dim, out_dim*3)\n",
        "\n",
        "        self.n_heads = n_heads\n",
        "        self.out_dim = out_dim\n",
        "        self.out_dim_per_head = out_dim // n_heads\n",
        "        self.out = nn.Linear(out_dim, out_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def split_heads(self, t):\n",
        "         return t.reshape(t.shape[0], -1, self.n_heads, self.out_dim_per_head)\n",
        "\n",
        "    def forward(self, x, y=None, mask=None):\n",
        "        #in decoder, y comes from encoder. In encoder, y=x\n",
        "        y = x if y is None else y\n",
        "\n",
        "        qkv = self.linear(x) # BS * SEQ_LEN * (3*EMBED_SIZE_L)\n",
        "        q = qkv[:, :, :self.out_dim] # BS * SEQ_LEN * EMBED_SIZE_L\n",
        "        k = qkv[:, :, self.out_dim:self.out_dim*2] # BS * SEQ_LEN * EMBED_SIZE_L\n",
        "        v = qkv[:, :, self.out_dim*2:] # BS * SEQ_LEN * EMBED_SIZE_L\n",
        "\n",
        "        #break into n_heads\n",
        "        q, k, v = [self.split_heads(t) for t in (q,k,v)]  # BS * SEQ_LEN * HEAD * EMBED_SIZE_P_HEAD\n",
        "        q, k, v = [t.transpose(1,2) for t in (q,k,v)]  # BS * HEAD * SEQ_LEN * EMBED_SIZE_P_HEAD\n",
        "\n",
        "        #n_heads => attention => merge the heads => mix information\n",
        "        scores = attention(q, k, v, mask, self.dropout) # BS * HEAD * SEQ_LEN * EMBED_SIZE_P_HEAD\n",
        "        scores = scores.transpose(1,2).contiguous().view(scores.shape[0], -1, self.out_dim) # BS * SEQ_LEN * EMBED_SIZE_L\n",
        "        out = self.out(scores)  # BS * SEQ_LEN * EMBED_SIZE\n",
        "\n",
        "        return out\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, inp_dim, inner_dim, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.linear1 = nn.Linear(inp_dim, inner_dim)\n",
        "        self.linear2 = nn.Linear(inner_dim, inp_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        #inp => inner => relu => dropout => inner => inp\n",
        "        return self.linear2(self.dropout(F.relu(self.linear1(x))))\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, n_heads, inner_transformer_size, inner_ff_size, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.mha = MultiHeadAttention(n_heads, inner_transformer_size, dropout)\n",
        "        self.ff = FeedForward(inner_transformer_size, inner_ff_size, dropout)\n",
        "        self.norm1 = nn.LayerNorm(inner_transformer_size)\n",
        "        self.norm2 = nn.LayerNorm(inner_transformer_size)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        x2 = self.norm1(x)\n",
        "        x = x + self.dropout1(self.mha(x2, mask=mask))\n",
        "        x2 = self.norm2(x)\n",
        "        x = x + self.dropout2(self.ff(x2))\n",
        "        return x\n",
        "\n",
        "# Positional Embedding\n",
        "class PositionalEmbedding(nn.Module):\n",
        "    def __init__(self, d_model, max_seq_len = 80):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        pe = torch.zeros(max_seq_len, d_model)\n",
        "        pe.requires_grad = False\n",
        "        for pos in range(max_seq_len):\n",
        "            for i in range(0, d_model, 2):\n",
        "                pe[pos, i] = math.sin(pos / (10000 ** ((2 * i)/d_model)))\n",
        "                pe[pos, i + 1] = math.cos(pos / (10000 ** ((2 * (i + 1))/d_model)))\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.pe[:,:x.size(1)] #x.size(1) = seq_len"
      ],
      "metadata": {
        "id": "gzxVl7g-hw1v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, n_code, n_heads, embed_size, inner_ff_size, n_embeddings, seq_len, dropout=.1, saved_embed = None):\n",
        "        super().__init__()\n",
        "\n",
        "        #model input\n",
        "        self.embeddings = nn.Embedding(n_embeddings, embed_size)\n",
        "        self.pe = PositionalEmbedding(embed_size, seq_len)\n",
        "\n",
        "        #backbone\n",
        "        encoders = []\n",
        "        for i in range(n_code):\n",
        "            encoders += [EncoderLayer(n_heads, embed_size, inner_ff_size, dropout)]\n",
        "        self.encoders = nn.ModuleList(encoders)\n",
        "\n",
        "        #language model\n",
        "        self.norm = nn.LayerNorm(embed_size)\n",
        "        self.linear = nn.Linear(embed_size, n_embeddings, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embeddings(x)\n",
        "        x = x + self.pe(x)\n",
        "        for encoder in self.encoders:\n",
        "            x = encoder(x)\n",
        "        x = self.norm(x)\n",
        "        x = self.linear(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "qyCObtpBh7Zk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# Dataset\n",
        "# =============================================================================\n",
        "class SentencesDataset(Dataset):\n",
        "    #Init dataset\n",
        "    def __init__(self, sentences, vocab, seq_len):\n",
        "        dataset = self\n",
        "\n",
        "        dataset.sentences = sentences\n",
        "        dataset.orig_vocab = vocab\n",
        "        dataset.vocab = vocab + ['<ignore>', '<oov>', '<mask>']\n",
        "        dataset.vocab = {e:i for i, e in enumerate(dataset.vocab)}\n",
        "        dataset.rvocab = {v:k for k,v in dataset.vocab.items()}\n",
        "        dataset.seq_len = seq_len\n",
        "\n",
        "        #special tags\n",
        "        dataset.IGNORE_IDX = dataset.vocab['<ignore>'] #replacement tag for tokens to ignore\n",
        "        dataset.OUT_OF_VOCAB_IDX = dataset.vocab['<oov>'] #replacement tag for unknown words\n",
        "        dataset.MASK_IDX = dataset.vocab['<mask>'] #replacement tag for the masked word prediction task\n",
        "\n",
        "    #fetch data\n",
        "    def __getitem__(self, index, p_random_mask=0.15):\n",
        "        dataset = self\n",
        "\n",
        "        #while we don't have enough word to fill the sentence for a batch\n",
        "        s = []\n",
        "        while len(s) < dataset.seq_len:\n",
        "            s.extend(dataset.get_sentence_idx(index % len(dataset)))\n",
        "            index += 1\n",
        "\n",
        "        #ensure that the sequence is of length seq_len\n",
        "        s = s[:dataset.seq_len]\n",
        "        [s.append(dataset.IGNORE_IDX) for i in range(dataset.seq_len - len(s))] #PAD ok\n",
        "\n",
        "        #apply random mask\n",
        "        #rand_rand = random.random()\n",
        "        # s = [(dataset.MASK_IDX, w) if rand_rand < p_random_mask else (w, dataset.IGNORE_IDX) for w in s]\n",
        "\n",
        "        # Replacing 15% of words in the sentence with random words selected from vocab\n",
        "        s_repl = copy.deepcopy(s)\n",
        "        noisy_idx = random.sample(range(0, seq_len-1), int(seq_len*0.15))  # Selecting the index of 15% words that need to be replaced\n",
        "        vocab_idx = random.sample( range(0, (len(dataset.orig_vocab)-1) ), int(seq_len*0.15))  # Selecting the index of vocab words for replacing\n",
        "        repl_idx = list(zip(noisy_idx, vocab_idx))\n",
        "        for orig, repl in repl_idx:\n",
        "            s_repl[orig] = repl\n",
        "\n",
        "        data_dict = {'input': torch.Tensor([w for w in s_repl]).long(),\n",
        "                     'target': torch.Tensor([w for w in s]).long()}\n",
        "\n",
        "        return data_dict\n",
        "\n",
        "    #return length\n",
        "    def __len__(self):\n",
        "        return len(self.sentences)\n",
        "\n",
        "    #get words id\n",
        "    def get_sentence_idx(self, index):\n",
        "        dataset = self\n",
        "        s = dataset.sentences[index]\n",
        "        s = [dataset.vocab[w] if w in dataset.vocab else dataset.OUT_OF_VOCAB_IDX for w in s]\n",
        "        return s"
      ],
      "metadata": {
        "id": "lXwOPxwTiBW3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# Methods / Class\n",
        "# =============================================================================\n",
        "def get_batch(loader, loader_iter):\n",
        "    try:\n",
        "        batch = next(loader_iter)\n",
        "    except StopIteration:\n",
        "        loader_iter = iter(loader)\n",
        "        batch = next(loader_iter)\n",
        "    return batch, loader_iter"
      ],
      "metadata": {
        "id": "py05esgkiHY1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# #Init\n",
        "# =============================================================================\n",
        "print('initializing..')\n",
        "batch_size = 1024\n",
        "seq_len = 20\n",
        "embed_size = 128\n",
        "inner_ff_size = embed_size * 4\n",
        "n_heads = 8\n",
        "n_code = 8\n",
        "n_vocab = 40000\n",
        "dropout = 0.1\n",
        "# n_workers = 12\n",
        "\n",
        "#optimizer\n",
        "optim_kwargs = {'lr':1e-4, 'weight_decay':1e-4, 'betas':(.9,.999)}"
      ],
      "metadata": {
        "id": "awTIvUtEiJzF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31870cc1-1638-419c-9b00-d5700c4947d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "initializing..\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# Input\n",
        "# =============================================================================\n",
        "#1) load text\n",
        "print('loading text...')\n",
        "pth = './BERT/quanta_train.txt'\n",
        "# pth = './BERT/small_training.txt'\n",
        "sentences = open(pth).read().lower().split('\\n')\n",
        "\n",
        "#2) tokenize sentences (can be done during training, you can also use spacy udpipe)\n",
        "print('tokenizing sentences...')\n",
        "special_chars = ',?;.:/*!+-()[]{}\"\\'&'\n",
        "sentences = [re.sub(f'[{re.escape(special_chars)}]', ' \\g<0> ', s).split(' ') for s in sentences]\n",
        "sentences = [[w for w in s if len(w)] for s in sentences]\n",
        "print(f'len(sentences) : {len(sentences)}')\n",
        "\n",
        "#3) create vocab if not already created\n",
        "print('creating/loading vocab...')\n",
        "pth = './BERT/quanta_vocab.txt'\n",
        "if not exists(pth):\n",
        "    words = [w for s in sentences for w in s]\n",
        "    vocab = Counter(words).most_common(n_vocab) #keep the N most frequent words\n",
        "    vocab = [w[0] for w in vocab]\n",
        "    open(pth, 'w+').write('\\n'.join(vocab))\n",
        "    print('created quanta_vocab.txt')\n",
        "else:\n",
        "    vocab = open(pth).read().split('\\n')\n",
        "\n",
        "print(len(vocab), vocab[4], vocab[1], vocab[-1])\n",
        "\n",
        "#4) create dataset\n",
        "print('creating dataset...')\n",
        "dataset = SentencesDataset(sentences, vocab, seq_len)\n",
        "# kwargs = {'num_workers':n_workers, 'shuffle':True,  'drop_last':True, 'pin_memory':True, 'batch_size':batch_size}\n",
        "kwargs = {'shuffle':True,  'drop_last':True, 'pin_memory':True, 'batch_size':batch_size}\n",
        "data_loader = torch.utils.data.DataLoader(dataset, **kwargs)"
      ],
      "metadata": {
        "id": "OHyOHDPIiMfA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01361c8f-04f5-43fc-e7ea-882ef0034a72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading text...\n",
            "tokenizing sentences...\n",
            "len(sentences) : 142210\n",
            "creating/loading vocab...\n",
            "978 a . where\n",
            "creating dataset...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# Model\n",
        "# =============================================================================\n",
        "#init model\n",
        "print(f'initializing model with len(dataset.vocab) as {len(dataset.vocab)}')\n",
        "model = Transformer(n_code, n_heads, embed_size, inner_ff_size, len(dataset.vocab), seq_len, dropout)\n",
        "model = model.cuda()\n",
        "#model.to(\"cuda\")"
      ],
      "metadata": {
        "id": "rMnVrzQ1iSlj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f867222-8b9a-4413-d73f-778825d2c6fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "initializing model with len(dataset.vocab) as 981\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# Optimizer\n",
        "# =============================================================================\n",
        "print('initializing optimizer and loss...')\n",
        "optimizer = optim.Adam(model.parameters(), **optim_kwargs)\n",
        "loss_model = nn.CrossEntropyLoss(ignore_index=dataset.IGNORE_IDX)"
      ],
      "metadata": {
        "id": "3X4pzcgHiUJ0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b087258f-55c3-46e1-deb5-07293c88d464"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "initializing optimizer and loss...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Getting the saved embedding from LAST training\n",
        "# saved_embedding = np.genfromtxt(fname='./BERT/values_03252023a.tsv', delimiter='\\t')\n",
        "# print(f'saved_embedding.shape : {saved_embedding.shape} type(saved_embedding) : {type(saved_embedding)}')\n",
        "# # Loading the model embedding with the pretrained weight we just got above\n",
        "# model.embeddings.weight.data.copy_(torch.from_numpy(saved_embedding))\n",
        "# print(f'model.embeddings.weight.shape - {model.embeddings.weight.shape}')\n",
        "\n",
        "# =============================================================================\n",
        "# Train\n",
        "# =============================================================================\n",
        "print('training...')\n",
        "print_each = 25\n",
        "model.train()\n",
        "batch_iter = iter(data_loader)\n",
        "n_iteration = 11_000\n",
        "for it in range(n_iteration):\n",
        "    #get batch\n",
        "    batch, batch_iter = get_batch(data_loader, batch_iter)\n",
        "\n",
        "    #infer\n",
        "    masked_input = batch['input']\n",
        "    masked_target = batch['target']\n",
        "\n",
        "    masked_input = masked_input.cuda(non_blocking=True)\n",
        "    masked_target = masked_target.cuda(non_blocking=True)\n",
        "    output = model(masked_input)\n",
        "    #compute the cross entropy loss\n",
        "    output_v = output.view(-1,output.shape[-1])\n",
        "    target_v = masked_target.view(-1,1).squeeze()\n",
        "    loss = loss_model(output_v, target_v)\n",
        "    #compute gradients\n",
        "    loss.backward()\n",
        "\n",
        "    #apply gradients\n",
        "    optimizer.step()\n",
        "\n",
        "    #print step\n",
        "    if it % print_each == 0:\n",
        "        print('it:', it,\n",
        "              ' | loss', np.round(loss.item(),2),\n",
        "              ' | Δw:', round(model.embeddings.weight.grad.abs().sum().item(),3))\n",
        "\n",
        "    #reset gradients\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "# =============================================================================\n",
        "# Saving the embeddings for future use\n",
        "# =============================================================================\n",
        "print('saving embeddings for future use...')\n",
        "print(f'model.embeddings.weight.shape - {model.embeddings.weight.shape}, len(dataset.rvocab) : {len(dataset.rvocab)}')\n",
        "N = len(dataset.rvocab)\n",
        "np.savetxt('values_03262023a.tsv', np.round(model.embeddings.weight.detach().cpu().numpy()[0:N], 2), delimiter='\\t', fmt='%1.2f')\n",
        "s = [dataset.rvocab[i] for i in range(N)]\n",
        "open('names_032623a.tsv', 'w+').write('\\n'.join(s) )\n",
        "\n",
        "print('end')"
      ],
      "metadata": {
        "id": "I4qNQlOPiVl-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef9ec58b-f0af-4886-8ea8-4f60f7f0bfcb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training...\n",
            "it: 0  | loss 0.58  | Δw: 4.933\n",
            "it: 25  | loss 0.48  | Δw: 4.499\n",
            "it: 50  | loss 0.6  | Δw: 4.487\n",
            "it: 75  | loss 0.62  | Δw: 4.775\n",
            "it: 100  | loss 0.63  | Δw: 5.007\n",
            "it: 125  | loss 0.58  | Δw: 4.505\n",
            "it: 150  | loss 0.59  | Δw: 4.793\n",
            "it: 175  | loss 0.66  | Δw: 5.274\n",
            "it: 200  | loss 0.66  | Δw: 5.203\n",
            "it: 225  | loss 0.64  | Δw: 5.106\n",
            "it: 250  | loss 0.6  | Δw: 5.213\n",
            "it: 275  | loss 0.55  | Δw: 5.102\n",
            "it: 300  | loss 0.58  | Δw: 4.706\n",
            "it: 325  | loss 0.62  | Δw: 5.192\n",
            "it: 350  | loss 0.56  | Δw: 5.086\n",
            "it: 375  | loss 0.6  | Δw: 4.895\n",
            "it: 400  | loss 0.54  | Δw: 5.375\n",
            "it: 425  | loss 0.62  | Δw: 5.805\n",
            "it: 450  | loss 0.56  | Δw: 4.989\n",
            "it: 475  | loss 0.6  | Δw: 5.773\n",
            "it: 500  | loss 0.55  | Δw: 5.493\n",
            "it: 525  | loss 0.66  | Δw: 5.542\n",
            "it: 550  | loss 0.54  | Δw: 4.541\n",
            "it: 575  | loss 0.53  | Δw: 5.394\n",
            "it: 600  | loss 0.47  | Δw: 4.987\n",
            "it: 625  | loss 0.65  | Δw: 5.541\n",
            "it: 650  | loss 0.63  | Δw: 4.389\n",
            "it: 675  | loss 0.51  | Δw: 4.846\n",
            "it: 700  | loss 0.59  | Δw: 5.711\n",
            "it: 725  | loss 0.55  | Δw: 4.883\n",
            "it: 750  | loss 0.54  | Δw: 4.663\n",
            "it: 775  | loss 0.53  | Δw: 5.178\n",
            "it: 800  | loss 0.5  | Δw: 5.331\n",
            "it: 825  | loss 0.5  | Δw: 4.725\n",
            "it: 850  | loss 0.6  | Δw: 4.671\n",
            "it: 875  | loss 0.59  | Δw: 5.225\n",
            "it: 900  | loss 0.5  | Δw: 5.902\n",
            "it: 925  | loss 0.52  | Δw: 5.103\n",
            "it: 950  | loss 0.52  | Δw: 4.886\n",
            "it: 975  | loss 0.58  | Δw: 5.758\n",
            "it: 1000  | loss 0.52  | Δw: 4.947\n",
            "it: 1025  | loss 0.53  | Δw: 4.904\n",
            "it: 1050  | loss 0.62  | Δw: 5.254\n",
            "it: 1075  | loss 0.48  | Δw: 5.221\n",
            "it: 1100  | loss 0.53  | Δw: 4.964\n",
            "it: 1125  | loss 0.56  | Δw: 5.385\n",
            "it: 1150  | loss 0.58  | Δw: 5.088\n",
            "it: 1175  | loss 0.52  | Δw: 5.108\n",
            "it: 1200  | loss 0.58  | Δw: 5.998\n",
            "it: 1225  | loss 0.5  | Δw: 5.774\n",
            "it: 1250  | loss 0.56  | Δw: 5.384\n",
            "it: 1275  | loss 0.55  | Δw: 4.701\n",
            "it: 1300  | loss 0.54  | Δw: 4.615\n",
            "it: 1325  | loss 0.53  | Δw: 5.869\n",
            "it: 1350  | loss 0.48  | Δw: 5.089\n",
            "it: 1375  | loss 0.64  | Δw: 6.274\n",
            "it: 1400  | loss 0.46  | Δw: 5.152\n",
            "it: 1425  | loss 0.54  | Δw: 4.918\n",
            "it: 1450  | loss 0.6  | Δw: 5.466\n",
            "it: 1475  | loss 0.55  | Δw: 5.167\n",
            "it: 1500  | loss 0.5  | Δw: 5.475\n",
            "it: 1525  | loss 0.57  | Δw: 5.973\n",
            "it: 1550  | loss 0.59  | Δw: 6.255\n",
            "it: 1575  | loss 0.52  | Δw: 5.056\n",
            "it: 1600  | loss 0.53  | Δw: 5.891\n",
            "it: 1625  | loss 1.07  | Δw: 8.284\n",
            "it: 1650  | loss 0.48  | Δw: 4.806\n",
            "it: 1675  | loss 0.59  | Δw: 5.559\n",
            "it: 1700  | loss 0.46  | Δw: 4.95\n",
            "it: 1725  | loss 0.69  | Δw: 6.588\n",
            "it: 1750  | loss 0.48  | Δw: 4.948\n",
            "it: 1775  | loss 0.51  | Δw: 5.668\n",
            "it: 1800  | loss 0.65  | Δw: 5.912\n",
            "it: 1825  | loss 0.49  | Δw: 5.448\n",
            "it: 1850  | loss 0.43  | Δw: 5.169\n",
            "it: 1875  | loss 0.48  | Δw: 6.32\n",
            "it: 1900  | loss 0.43  | Δw: 5.614\n",
            "it: 1925  | loss 0.49  | Δw: 5.759\n",
            "it: 1950  | loss 0.47  | Δw: 4.604\n",
            "it: 1975  | loss 0.52  | Δw: 5.869\n",
            "it: 2000  | loss 0.55  | Δw: 6.147\n",
            "it: 2025  | loss 0.51  | Δw: 5.493\n",
            "it: 2050  | loss 0.49  | Δw: 5.096\n",
            "it: 2075  | loss 0.41  | Δw: 5.379\n",
            "it: 2100  | loss 0.44  | Δw: 6.119\n",
            "it: 2125  | loss 0.65  | Δw: 6.352\n",
            "it: 2150  | loss 0.5  | Δw: 5.719\n",
            "it: 2175  | loss 0.47  | Δw: 5.973\n",
            "it: 2200  | loss 0.45  | Δw: 5.187\n",
            "it: 2225  | loss 0.48  | Δw: 5.32\n",
            "it: 2250  | loss 0.54  | Δw: 5.836\n",
            "it: 2275  | loss 0.41  | Δw: 5.318\n",
            "it: 2300  | loss 0.46  | Δw: 5.553\n",
            "it: 2325  | loss 0.47  | Δw: 6.263\n",
            "it: 2350  | loss 0.49  | Δw: 5.768\n",
            "it: 2375  | loss 0.51  | Δw: 5.204\n",
            "it: 2400  | loss 0.45  | Δw: 5.316\n",
            "it: 2425  | loss 0.48  | Δw: 5.74\n",
            "it: 2450  | loss 0.42  | Δw: 5.366\n",
            "it: 2475  | loss 0.48  | Δw: 6.386\n",
            "it: 2500  | loss 0.53  | Δw: 5.989\n",
            "it: 2525  | loss 0.42  | Δw: 5.67\n",
            "it: 2550  | loss 0.43  | Δw: 5.906\n",
            "it: 2575  | loss 0.43  | Δw: 5.082\n",
            "it: 2600  | loss 0.4  | Δw: 5.199\n",
            "it: 2625  | loss 0.44  | Δw: 5.614\n",
            "it: 2650  | loss 0.51  | Δw: 5.72\n",
            "it: 2675  | loss 0.43  | Δw: 5.792\n",
            "it: 2700  | loss 0.51  | Δw: 5.227\n",
            "it: 2725  | loss 0.44  | Δw: 5.556\n",
            "it: 2750  | loss 0.58  | Δw: 5.941\n",
            "it: 2775  | loss 0.45  | Δw: 5.667\n",
            "it: 2800  | loss 0.38  | Δw: 5.495\n",
            "it: 2825  | loss 0.49  | Δw: 5.828\n",
            "it: 2850  | loss 0.47  | Δw: 6.03\n",
            "it: 2875  | loss 0.29  | Δw: 4.698\n",
            "it: 2900  | loss 0.43  | Δw: 5.717\n",
            "it: 2925  | loss 0.51  | Δw: 5.5\n",
            "it: 2950  | loss 0.39  | Δw: 5.947\n",
            "it: 2975  | loss 0.48  | Δw: 6.281\n",
            "it: 3000  | loss 0.44  | Δw: 5.37\n",
            "it: 3025  | loss 0.5  | Δw: 6.427\n",
            "it: 3050  | loss 0.44  | Δw: 6.272\n",
            "it: 3075  | loss 0.43  | Δw: 5.245\n",
            "it: 3100  | loss 0.48  | Δw: 6.219\n",
            "it: 3125  | loss 0.44  | Δw: 5.787\n",
            "it: 3150  | loss 0.48  | Δw: 6.562\n",
            "it: 3175  | loss 0.42  | Δw: 5.427\n",
            "it: 3200  | loss 0.55  | Δw: 7.806\n",
            "it: 3225  | loss 0.41  | Δw: 5.505\n",
            "it: 3250  | loss 0.43  | Δw: 5.55\n",
            "it: 3275  | loss 0.44  | Δw: 5.581\n",
            "it: 3300  | loss 0.36  | Δw: 5.515\n",
            "it: 3325  | loss 0.33  | Δw: 5.492\n",
            "it: 3350  | loss 0.42  | Δw: 6.237\n",
            "it: 3375  | loss 0.47  | Δw: 5.657\n",
            "it: 3400  | loss 0.48  | Δw: 6.28\n",
            "it: 3425  | loss 0.46  | Δw: 6.004\n",
            "it: 3450  | loss 0.45  | Δw: 5.749\n",
            "it: 3475  | loss 0.41  | Δw: 5.965\n",
            "it: 3500  | loss 0.39  | Δw: 5.822\n",
            "it: 3525  | loss 0.78  | Δw: 8.928\n",
            "it: 3550  | loss 0.44  | Δw: 6.115\n",
            "it: 3575  | loss 0.4  | Δw: 6.076\n",
            "it: 3600  | loss 0.49  | Δw: 6.634\n",
            "it: 3625  | loss 0.4  | Δw: 6.707\n",
            "it: 3650  | loss 0.5  | Δw: 6.66\n",
            "it: 3675  | loss 0.36  | Δw: 6.005\n",
            "it: 3700  | loss 0.32  | Δw: 5.751\n",
            "it: 3725  | loss 0.5  | Δw: 6.91\n",
            "it: 3750  | loss 0.45  | Δw: 6.856\n",
            "it: 3775  | loss 0.39  | Δw: 6.101\n",
            "it: 3800  | loss 0.36  | Δw: 5.996\n",
            "it: 3825  | loss 0.35  | Δw: 6.105\n",
            "it: 3850  | loss 0.39  | Δw: 5.951\n",
            "it: 3875  | loss 0.39  | Δw: 6.598\n",
            "it: 3900  | loss 0.41  | Δw: 5.779\n",
            "it: 3925  | loss 0.45  | Δw: 6.272\n",
            "it: 3950  | loss 0.33  | Δw: 4.746\n",
            "it: 3975  | loss 0.44  | Δw: 5.924\n",
            "it: 4000  | loss 0.51  | Δw: 7.462\n",
            "it: 4025  | loss 0.49  | Δw: 6.343\n",
            "it: 4050  | loss 0.31  | Δw: 5.767\n",
            "it: 4075  | loss 0.32  | Δw: 5.036\n",
            "it: 4100  | loss 0.41  | Δw: 6.166\n",
            "it: 4125  | loss 0.41  | Δw: 6.726\n",
            "it: 4150  | loss 0.42  | Δw: 6.281\n",
            "it: 4175  | loss 0.36  | Δw: 6.019\n",
            "it: 4200  | loss 0.4  | Δw: 6.607\n",
            "it: 4225  | loss 0.37  | Δw: 6.321\n",
            "it: 4250  | loss 0.4  | Δw: 5.907\n",
            "it: 4275  | loss 0.44  | Δw: 6.343\n",
            "it: 4300  | loss 0.3  | Δw: 6.201\n",
            "it: 4325  | loss 0.49  | Δw: 6.837\n",
            "it: 4350  | loss 0.36  | Δw: 5.709\n",
            "it: 4375  | loss 0.39  | Δw: 6.179\n",
            "it: 4400  | loss 0.31  | Δw: 5.676\n",
            "it: 4425  | loss 0.34  | Δw: 6.049\n",
            "it: 4450  | loss 0.3  | Δw: 6.132\n",
            "it: 4475  | loss 0.39  | Δw: 6.029\n",
            "it: 4500  | loss 0.3  | Δw: 5.276\n",
            "it: 4525  | loss 0.42  | Δw: 6.374\n",
            "it: 4550  | loss 0.37  | Δw: 6.557\n",
            "it: 4575  | loss 0.45  | Δw: 6.733\n",
            "it: 4600  | loss 0.38  | Δw: 7.447\n",
            "it: 4625  | loss 0.37  | Δw: 6.165\n",
            "it: 4650  | loss 0.29  | Δw: 5.991\n",
            "it: 4675  | loss 0.36  | Δw: 7.839\n",
            "it: 4700  | loss 0.37  | Δw: 7.071\n",
            "it: 4725  | loss 0.32  | Δw: 5.745\n",
            "it: 4750  | loss 0.3  | Δw: 5.871\n",
            "it: 4775  | loss 0.44  | Δw: 6.681\n",
            "it: 4800  | loss 0.43  | Δw: 6.792\n",
            "it: 4825  | loss 0.39  | Δw: 6.829\n",
            "it: 4850  | loss 0.45  | Δw: 7.384\n",
            "it: 4875  | loss 0.34  | Δw: 5.909\n",
            "it: 4900  | loss 0.32  | Δw: 6.836\n",
            "it: 4925  | loss 0.27  | Δw: 5.423\n",
            "it: 4950  | loss 0.33  | Δw: 6.021\n",
            "it: 4975  | loss 0.37  | Δw: 5.76\n",
            "it: 5000  | loss 0.41  | Δw: 6.931\n",
            "it: 5025  | loss 0.37  | Δw: 6.935\n",
            "it: 5050  | loss 0.35  | Δw: 5.838\n",
            "it: 5075  | loss 0.44  | Δw: 6.899\n",
            "it: 5100  | loss 0.31  | Δw: 5.935\n",
            "it: 5125  | loss 0.39  | Δw: 6.324\n",
            "it: 5150  | loss 0.36  | Δw: 6.495\n",
            "it: 5175  | loss 0.42  | Δw: 7.844\n",
            "it: 5200  | loss 0.29  | Δw: 5.34\n",
            "it: 5225  | loss 0.27  | Δw: 5.885\n",
            "it: 5250  | loss 0.34  | Δw: 6.585\n",
            "it: 5275  | loss 0.58  | Δw: 7.287\n",
            "it: 5300  | loss 0.36  | Δw: 6.26\n",
            "it: 5325  | loss 0.38  | Δw: 6.95\n",
            "it: 5350  | loss 0.36  | Δw: 7.096\n",
            "it: 5375  | loss 0.33  | Δw: 6.383\n",
            "it: 5400  | loss 0.25  | Δw: 6.129\n",
            "it: 5425  | loss 0.32  | Δw: 6.498\n",
            "it: 5450  | loss 0.39  | Δw: 6.172\n",
            "it: 5475  | loss 0.35  | Δw: 6.578\n",
            "it: 5500  | loss 0.37  | Δw: 6.162\n",
            "it: 5525  | loss 0.37  | Δw: 6.13\n",
            "it: 5550  | loss 0.28  | Δw: 5.98\n",
            "it: 5575  | loss 0.48  | Δw: 7.526\n",
            "it: 5600  | loss 0.38  | Δw: 7.382\n",
            "it: 5625  | loss 0.35  | Δw: 6.142\n",
            "it: 5650  | loss 0.32  | Δw: 6.309\n",
            "it: 5675  | loss 0.36  | Δw: 6.96\n",
            "it: 5700  | loss 0.37  | Δw: 6.879\n",
            "it: 5725  | loss 0.34  | Δw: 6.566\n",
            "it: 5750  | loss 0.41  | Δw: 7.268\n",
            "it: 5775  | loss 0.38  | Δw: 6.814\n",
            "it: 5800  | loss 0.34  | Δw: 6.439\n",
            "it: 5825  | loss 0.33  | Δw: 6.82\n",
            "it: 5850  | loss 0.34  | Δw: 7.016\n",
            "it: 5875  | loss 0.29  | Δw: 5.917\n",
            "it: 5900  | loss 0.34  | Δw: 6.896\n",
            "it: 5925  | loss 0.4  | Δw: 7.732\n",
            "it: 5950  | loss 0.34  | Δw: 6.791\n",
            "it: 5975  | loss 0.32  | Δw: 6.795\n",
            "it: 6000  | loss 0.3  | Δw: 6.373\n",
            "it: 6025  | loss 0.27  | Δw: 6.414\n",
            "it: 6050  | loss 0.58  | Δw: 7.283\n",
            "it: 6075  | loss 0.32  | Δw: 6.55\n",
            "it: 6100  | loss 0.34  | Δw: 7.756\n",
            "it: 6125  | loss 0.25  | Δw: 6.092\n",
            "it: 6150  | loss 0.29  | Δw: 6.127\n",
            "it: 6175  | loss 0.28  | Δw: 6.695\n",
            "it: 6200  | loss 0.3  | Δw: 7.335\n",
            "it: 6225  | loss 0.27  | Δw: 6.262\n",
            "it: 6250  | loss 0.26  | Δw: 6.82\n",
            "it: 6275  | loss 0.25  | Δw: 5.995\n",
            "it: 6300  | loss 0.27  | Δw: 6.315\n",
            "it: 6325  | loss 0.34  | Δw: 6.867\n",
            "it: 6350  | loss 0.28  | Δw: 6.997\n",
            "it: 6375  | loss 0.32  | Δw: 7.043\n",
            "it: 6400  | loss 0.27  | Δw: 6.001\n",
            "it: 6425  | loss 0.3  | Δw: 7.517\n",
            "it: 6450  | loss 0.31  | Δw: 7.097\n",
            "it: 6475  | loss 0.36  | Δw: 7.094\n",
            "it: 6500  | loss 0.36  | Δw: 7.116\n",
            "it: 6525  | loss 0.34  | Δw: 7.132\n",
            "it: 6550  | loss 0.43  | Δw: 6.658\n",
            "it: 6575  | loss 0.26  | Δw: 8.335\n",
            "it: 6600  | loss 0.28  | Δw: 5.817\n",
            "it: 6625  | loss 0.28  | Δw: 7.484\n",
            "it: 6650  | loss 0.34  | Δw: 8.158\n",
            "it: 6675  | loss 0.3  | Δw: 7.634\n",
            "it: 6700  | loss 0.34  | Δw: 7.624\n",
            "it: 6725  | loss 0.24  | Δw: 6.335\n",
            "it: 6750  | loss 0.34  | Δw: 7.263\n",
            "it: 6775  | loss 0.36  | Δw: 8.619\n",
            "it: 6800  | loss 0.22  | Δw: 5.833\n",
            "it: 6825  | loss 0.22  | Δw: 5.442\n",
            "it: 6850  | loss 0.27  | Δw: 5.986\n",
            "it: 6875  | loss 0.31  | Δw: 7.717\n",
            "it: 6900  | loss 0.36  | Δw: 7.035\n",
            "it: 6925  | loss 0.47  | Δw: 8.036\n",
            "it: 6950  | loss 0.31  | Δw: 7.196\n",
            "it: 6975  | loss 0.29  | Δw: 9.975\n",
            "it: 7000  | loss 0.27  | Δw: 6.184\n",
            "it: 7025  | loss 0.28  | Δw: 6.194\n",
            "it: 7050  | loss 0.24  | Δw: 5.618\n",
            "it: 7075  | loss 0.3  | Δw: 6.866\n",
            "it: 7100  | loss 0.23  | Δw: 5.624\n",
            "it: 7125  | loss 0.31  | Δw: 7.557\n",
            "it: 7150  | loss 0.43  | Δw: 9.79\n",
            "it: 7175  | loss 0.2  | Δw: 4.901\n",
            "it: 7200  | loss 0.25  | Δw: 7.004\n",
            "it: 7225  | loss 0.25  | Δw: 7.423\n",
            "it: 7250  | loss 0.2  | Δw: 5.362\n",
            "it: 7275  | loss 0.22  | Δw: 5.684\n",
            "it: 7300  | loss 0.2  | Δw: 6.644\n",
            "it: 7325  | loss 0.33  | Δw: 7.013\n",
            "it: 7350  | loss 0.19  | Δw: 5.98\n",
            "it: 7375  | loss 0.21  | Δw: 6.439\n",
            "it: 7400  | loss 0.35  | Δw: 8.106\n",
            "it: 7425  | loss 0.2  | Δw: 5.999\n",
            "it: 7450  | loss 0.25  | Δw: 6.556\n",
            "it: 7475  | loss 0.23  | Δw: 6.141\n",
            "it: 7500  | loss 0.26  | Δw: 7.25\n",
            "it: 7525  | loss 0.27  | Δw: 8.05\n",
            "it: 7550  | loss 0.24  | Δw: 6.443\n",
            "it: 7575  | loss 0.4  | Δw: 7.133\n",
            "it: 7600  | loss 0.23  | Δw: 6.768\n",
            "it: 7625  | loss 0.22  | Δw: 7.507\n",
            "it: 7650  | loss 0.23  | Δw: 6.371\n",
            "it: 7675  | loss 0.24  | Δw: 6.727\n",
            "it: 7700  | loss 0.27  | Δw: 7.586\n",
            "it: 7725  | loss 0.28  | Δw: 8.498\n",
            "it: 7750  | loss 0.28  | Δw: 7.05\n",
            "it: 7775  | loss 0.19  | Δw: 4.983\n",
            "it: 7800  | loss 0.3  | Δw: 7.034\n",
            "it: 7825  | loss 0.34  | Δw: 7.768\n",
            "it: 7850  | loss 0.26  | Δw: 6.358\n",
            "it: 7875  | loss 0.24  | Δw: 7.176\n",
            "it: 7900  | loss 0.24  | Δw: 6.176\n",
            "it: 7925  | loss 0.24  | Δw: 5.798\n",
            "it: 7950  | loss 0.25  | Δw: 6.986\n",
            "it: 7975  | loss 0.32  | Δw: 8.697\n",
            "it: 8000  | loss 0.26  | Δw: 8.271\n",
            "it: 8025  | loss 0.2  | Δw: 6.713\n",
            "it: 8050  | loss 0.22  | Δw: 6.846\n",
            "it: 8075  | loss 0.3  | Δw: 7.545\n",
            "it: 8100  | loss 0.29  | Δw: 7.578\n",
            "it: 8125  | loss 0.21  | Δw: 6.536\n",
            "it: 8150  | loss 0.21  | Δw: 6.164\n",
            "it: 8175  | loss 0.28  | Δw: 5.983\n",
            "it: 8200  | loss 0.25  | Δw: 6.857\n",
            "it: 8225  | loss 0.27  | Δw: 8.659\n",
            "it: 8250  | loss 0.34  | Δw: 9.356\n",
            "it: 8275  | loss 0.29  | Δw: 7.078\n",
            "it: 8300  | loss 0.27  | Δw: 6.974\n",
            "it: 8325  | loss 0.31  | Δw: 8.14\n",
            "it: 8350  | loss 0.24  | Δw: 6.03\n",
            "it: 8375  | loss 0.2  | Δw: 6.103\n",
            "it: 8400  | loss 0.36  | Δw: 8.565\n",
            "it: 8425  | loss 0.23  | Δw: 5.693\n",
            "it: 8450  | loss 0.21  | Δw: 5.821\n",
            "it: 8475  | loss 0.16  | Δw: 5.543\n",
            "it: 8500  | loss 0.27  | Δw: 7.508\n",
            "it: 8525  | loss 0.28  | Δw: 6.959\n",
            "it: 8550  | loss 0.23  | Δw: 7.8\n",
            "it: 8575  | loss 0.2  | Δw: 6.259\n",
            "it: 8600  | loss 0.28  | Δw: 8.134\n",
            "it: 8625  | loss 0.12  | Δw: 4.252\n",
            "it: 8650  | loss 0.21  | Δw: 6.172\n",
            "it: 8675  | loss 0.2  | Δw: 6.294\n",
            "it: 8700  | loss 0.21  | Δw: 5.966\n",
            "it: 8725  | loss 0.25  | Δw: 7.845\n",
            "it: 8750  | loss 0.24  | Δw: 7.28\n",
            "it: 8775  | loss 0.27  | Δw: 6.862\n",
            "it: 8800  | loss 0.18  | Δw: 5.718\n",
            "it: 8825  | loss 0.25  | Δw: 6.899\n",
            "it: 8850  | loss 0.24  | Δw: 6.436\n",
            "it: 8875  | loss 0.17  | Δw: 5.394\n",
            "it: 8900  | loss 0.21  | Δw: 6.834\n",
            "it: 8925  | loss 0.27  | Δw: 8.122\n",
            "it: 8950  | loss 0.35  | Δw: 8.656\n",
            "it: 8975  | loss 0.22  | Δw: 7.924\n",
            "it: 9000  | loss 0.22  | Δw: 6.556\n",
            "it: 9025  | loss 0.19  | Δw: 6.015\n",
            "it: 9050  | loss 0.17  | Δw: 5.231\n",
            "it: 9075  | loss 0.17  | Δw: 6.539\n",
            "it: 9100  | loss 0.18  | Δw: 6.331\n",
            "it: 9125  | loss 0.21  | Δw: 6.344\n",
            "it: 9150  | loss 0.2  | Δw: 6.784\n",
            "it: 9175  | loss 0.23  | Δw: 6.575\n",
            "it: 9200  | loss 0.23  | Δw: 7.437\n",
            "it: 9225  | loss 0.17  | Δw: 6.704\n",
            "it: 9250  | loss 0.28  | Δw: 7.393\n",
            "it: 9275  | loss 0.21  | Δw: 5.163\n",
            "it: 9300  | loss 0.23  | Δw: 6.987\n",
            "it: 9325  | loss 0.22  | Δw: 7.329\n",
            "it: 9350  | loss 0.17  | Δw: 5.528\n",
            "it: 9375  | loss 0.15  | Δw: 6.019\n",
            "it: 9400  | loss 0.2  | Δw: 7.878\n",
            "it: 9425  | loss 0.19  | Δw: 6.443\n",
            "it: 9450  | loss 0.21  | Δw: 6.428\n",
            "it: 9475  | loss 0.16  | Δw: 6.134\n",
            "it: 9500  | loss 0.18  | Δw: 7.061\n",
            "it: 9525  | loss 0.25  | Δw: 7.718\n",
            "it: 9550  | loss 0.14  | Δw: 5.605\n",
            "it: 9575  | loss 0.25  | Δw: 6.914\n",
            "it: 9600  | loss 0.22  | Δw: 7.23\n",
            "it: 9625  | loss 0.12  | Δw: 4.743\n",
            "it: 9650  | loss 0.24  | Δw: 7.767\n",
            "it: 9675  | loss 0.22  | Δw: 6.729\n",
            "it: 9700  | loss 0.12  | Δw: 5.494\n",
            "it: 9725  | loss 0.18  | Δw: 6.642\n",
            "it: 9750  | loss 0.23  | Δw: 7.958\n",
            "it: 9775  | loss 0.17  | Δw: 5.746\n",
            "it: 9800  | loss 0.16  | Δw: 6.55\n",
            "it: 9825  | loss 0.2  | Δw: 6.447\n",
            "it: 9850  | loss 0.2  | Δw: 6.481\n",
            "it: 9875  | loss 0.19  | Δw: 7.891\n",
            "it: 9900  | loss 0.21  | Δw: 7.566\n",
            "it: 9925  | loss 0.19  | Δw: 6.651\n",
            "it: 9950  | loss 0.16  | Δw: 6.201\n",
            "it: 9975  | loss 0.2  | Δw: 6.031\n",
            "it: 10000  | loss 0.2  | Δw: 7.478\n",
            "it: 10025  | loss 0.21  | Δw: 6.933\n",
            "it: 10050  | loss 0.11  | Δw: 4.697\n",
            "it: 10075  | loss 0.2  | Δw: 7.041\n",
            "it: 10100  | loss 0.22  | Δw: 7.861\n",
            "it: 10125  | loss 0.26  | Δw: 7.843\n",
            "it: 10150  | loss 0.24  | Δw: 7.561\n",
            "it: 10175  | loss 0.23  | Δw: 7.413\n",
            "it: 10200  | loss 0.18  | Δw: 7.284\n",
            "it: 10225  | loss 0.19  | Δw: 7.432\n",
            "it: 10250  | loss 0.18  | Δw: 6.292\n",
            "it: 10275  | loss 0.23  | Δw: 6.875\n",
            "it: 10300  | loss 0.21  | Δw: 6.571\n",
            "it: 10325  | loss 0.21  | Δw: 7.622\n",
            "it: 10350  | loss 0.16  | Δw: 7.799\n",
            "it: 10375  | loss 0.18  | Δw: 6.28\n",
            "it: 10400  | loss 0.19  | Δw: 7.497\n",
            "it: 10425  | loss 0.18  | Δw: 5.911\n",
            "it: 10450  | loss 0.14  | Δw: 4.882\n",
            "it: 10475  | loss 0.2  | Δw: 7.595\n",
            "it: 10500  | loss 0.2  | Δw: 8.314\n",
            "it: 10525  | loss 0.26  | Δw: 8.602\n",
            "it: 10550  | loss 0.12  | Δw: 5.311\n",
            "it: 10575  | loss 0.19  | Δw: 6.703\n",
            "it: 10600  | loss 0.12  | Δw: 5.158\n",
            "it: 10625  | loss 0.23  | Δw: 6.441\n",
            "it: 10650  | loss 0.15  | Δw: 6.008\n",
            "it: 10675  | loss 0.15  | Δw: 5.931\n",
            "it: 10700  | loss 0.26  | Δw: 9.089\n",
            "it: 10725  | loss 0.15  | Δw: 5.935\n",
            "it: 10750  | loss 0.14  | Δw: 5.285\n",
            "it: 10775  | loss 0.17  | Δw: 6.909\n",
            "it: 10800  | loss 0.2  | Δw: 8.637\n",
            "it: 10825  | loss 0.19  | Δw: 7.905\n",
            "it: 10850  | loss 0.12  | Δw: 5.006\n",
            "it: 10875  | loss 0.16  | Δw: 6.798\n",
            "it: 10900  | loss 0.1  | Δw: 5.552\n",
            "it: 10925  | loss 0.16  | Δw: 6.926\n",
            "it: 10950  | loss 0.11  | Δw: 4.461\n",
            "it: 10975  | loss 0.17  | Δw: 6.905\n",
            "saving embeddings for future use...\n",
            "model.embeddings.weight.shape - torch.Size([981, 128]), len(dataset.rvocab) : 981\n",
            "end\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r \"/content/values_03262023a.tsv\" \"/content/gdrive/My Drive/EVA8_S11_Course_Docs/BERT\"\n",
        "!cp -r \"/content/names_032623a.tsv\" \"/content/gdrive/My Drive/EVA8_S11_Course_Docs/BERT\""
      ],
      "metadata": {
        "id": "XMnme6pz1-fh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Quick inferencing with above trained model weights**"
      ],
      "metadata": {
        "id": "Y0J2aVzBiIcz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Loading the Test Input\n",
        "pth = './BERT/quanta_test2.txt'\n",
        "test_sentences = open(pth).read().lower().split('\\n')\n",
        "\n",
        "print('tokenizing test_sentences...')\n",
        "special_chars = ',?;.:/*!+-()[]{}\"\\'&'\n",
        "test_sentences = [re.sub(f'[{re.escape(special_chars)}]', ' \\g<0> ', s).split(' ') for s in test_sentences]\n",
        "test_sentences = [[w for w in s if len(w)] for s in test_sentences]\n",
        "print(f'len(test_sentences) : {len(test_sentences)}')\n",
        "\n",
        "#creating test_dataset, test_data_loader\n",
        "print('creating test_dataset and test_data_loader...')\n",
        "batch_size = 10\n",
        "test_dataset = SentencesDataset(test_sentences, vocab, seq_len)\n",
        "kwargs = {'shuffle':True,  'drop_last':True, 'pin_memory':True, 'batch_size':batch_size}\n",
        "test_data_loader = torch.utils.data.DataLoader(test_dataset, **kwargs)\n",
        "\n",
        "# Creating test_batch and test_input and test_target with 5 sentences\n",
        "test_batch_iter = iter(test_data_loader)\n",
        "test_batch, test_batch_iter = get_batch(test_data_loader, test_batch_iter)\n",
        "test_input = test_batch['input']\n",
        "test_target = test_batch['target']\n",
        "test_input = test_input.cuda(non_blocking=True)\n",
        "test_target = test_target.cuda(non_blocking=True)\n",
        "\n",
        "# Prediction\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "      test_out = model(test_input)\n",
        "      test_out_f = test_out.argmax(dim=-1)\n",
        "print(f'test_out.shape, test_out_f.shape : {test_out.shape}, {test_out_f.shape}')\n",
        "\n",
        "# Using test_dataset.rvocab for decoding the output\n",
        "test_input_2 = test_input.tolist()\n",
        "test_target_2 = test_target.tolist()\n",
        "test_output  = test_out_f.tolist()\n",
        "\n",
        "s_in = []\n",
        "s_out = []\n",
        "s_target = []\n",
        "\n",
        "for i in range(batch_size):\n",
        "   s_in = []\n",
        "   print(f'*** Sentence {i} ***')\n",
        "   for elem in test_input_2[i]:\n",
        "       s_in.append(test_dataset.rvocab[elem])\n",
        "   print(f'Input : {\" \".join(s_in)}')\n",
        "   s_target = []\n",
        "   for elem in test_target_2[i]:\n",
        "       s_target.append(test_dataset.rvocab[elem])\n",
        "   print(f'Target: {\" \".join(s_target)}')\n",
        "   for idx, elem in enumerate(s_in):\n",
        "       orig = s_target[idx]\n",
        "       if elem == s_target[idx]:\n",
        "           pass\n",
        "       else:\n",
        "           print(f' ** idx {idx} having word \"{orig}\" was replaced with \"{elem}\"')\n",
        "   s_out = []\n",
        "   for elem in test_output[i]:\n",
        "       s_out.append(test_dataset.rvocab[elem])\n",
        "   print(f'Output: {\" \".join(s_out)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pR9xwxjviHiW",
        "outputId": "0f840cd3-cda2-48cf-cee9-537a374b317a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tokenizing test_sentences...\n",
            "len(test_sentences) : 21\n",
            "creating test_dataset and test_data_loader...\n",
            "test_out.shape, test_out_f.shape : torch.Size([10, 20, 981]), torch.Size([10, 20])\n",
            "*** Sentence 0 ***\n",
            "Input : on their face grow dual systems look completely different . the process is posted unlike bathwater a computer . how\n",
            "Target: on their face , dual systems look completely different . the process is not unlike watching a computer . how\n",
            " ** idx 3 having word \",\" was replaced with \"grow\"\n",
            " ** idx 13 having word \"not\" was replaced with \"posted\"\n",
            " ** idx 15 having word \"watching\" was replaced with \"bathwater\"\n",
            "Output: on their face not dual systems look completely different . the as quantum in , by a quantum . how\n",
            "*** Sentence 1 ***\n",
            "Input : a major hope of the field less to develop the ability perfect access the still mysterious spotted of quantum gravity\n",
            "Target: a major hope of the field is to develop the ability to access the still mysterious rules of quantum gravity\n",
            " ** idx 6 having word \"is\" was replaced with \"less\"\n",
            " ** idx 11 having word \"to\" was replaced with \"perfect\"\n",
            " ** idx 16 having word \"rules\" was replaced with \"spotted\"\n",
            "Output: a major hope of the field is to develop the ability to access the still mysterious rules of quantum gravity\n",
            "*** Sentence 2 ***\n",
            "Input : holography involves the terms of profound relationships known as dualities . on bridge face , “fully systems look completely different\n",
            "Target: holography involves the study of profound relationships known as dualities . on their face , dual systems look completely different\n",
            " ** idx 3 having word \"study\" was replaced with \"terms\"\n",
            " ** idx 12 having word \"their\" was replaced with \"bridge\"\n",
            " ** idx 15 having word \"dual\" was replaced with \"“fully\"\n",
            "Output: holography involves the study of profound relationships known as dualities . on face face , dual systems look completely different\n",
            "*** Sentence 3 ***\n",
            "Input : does every true quantum theory pop into 100 gravity theory when viewed holographically ? can physicists understand gravity dual our\n",
            "Target: does every conceivable quantum theory pop into a gravity theory when viewed holographically ? can physicists understand gravity in our\n",
            " ** idx 2 having word \"conceivable\" was replaced with \"true\"\n",
            " ** idx 7 having word \"a\" was replaced with \"100\"\n",
            " ** idx 18 having word \"in\" was replaced with \"dual\"\n",
            "Output: does every conceivable quantum theory pop into into gravity theory when viewed holographically ? can physicists understand gravity in our\n",
            "*** Sentence 4 ***\n",
            "Input : a enterprise questions in concrete physics is that dualities also seem to link certain well systems trained gravitational systems .\n",
            "Target: a major questions in concrete physics is that dualities also seem to link certain speculation systems to gravitational systems .\n",
            " ** idx 1 having word \"major\" was replaced with \"enterprise\"\n",
            " ** idx 14 having word \"speculation\" was replaced with \"well\"\n",
            " ** idx 16 having word \"to\" was replaced with \"trained\"\n",
            "Output: a major finding in modern physics is that dualities also seem to link certain certain systems quantum gravitational systems .\n",
            "*** Sentence 5 ***\n",
            "Input : physicists characters spent decades developing mathematical “dictionaries” that let them translate quantum elements into gravitational elements either vice guess ,\n",
            "Target: physicists have spent decades developing mathematical “dictionaries” that let them translate quantum elements into gravitational elements and vice versa ,\n",
            " ** idx 1 having word \"have\" was replaced with \"characters\"\n",
            " ** idx 16 having word \"and\" was replaced with \"either\"\n",
            " ** idx 18 having word \"versa\" was replaced with \"guess\"\n",
            "Output: physicists have spent decades developing mathematical “dictionaries” that let them translate quantum elements into gravitational elements quantum vice versa ,\n",
            "*** Sentence 6 ***\n",
            "Input : or , as if by popping universe own pair decades 3d collaborators , we showcases the collection of particles as\n",
            "Target: or , as if by popping on a pair of 3d collaborators , we showcases the collection of particles as\n",
            " ** idx 6 having word \"on\" was replaced with \"universe\"\n",
            " ** idx 7 having word \"a\" was replaced with \"own\"\n",
            " ** idx 9 having word \"of\" was replaced with \"decades\"\n",
            "Output: or , as if by popping a on pair of 3d glasses , we might the collection of particles as\n",
            "*** Sentence 7 ***\n",
            "Input : the process is not unlike fully a left . how does gravity work in the quantum framework a major questions\n",
            "Target: the process is not unlike watching a computer . how does gravity work in the quantum ? a major questions\n",
            " ** idx 5 having word \"watching\" was replaced with \"fully\"\n",
            " ** idx 7 having word \"computer\" was replaced with \"left\"\n",
            " ** idx 16 having word \"?\" was replaced with \"framework\"\n",
            "Output: the major is not of of a of . how does gravity of in the quantum of a major are\n",
            "*** Sentence 8 ***\n",
            "Input : a couple of years later , another seven made their speculation concrete . in 2019 , argues chaotically his collaborators\n",
            "Target: a couple of years later , another team made their speculation concrete . in 2019 , gharibyan and his collaborators\n",
            " ** idx 7 having word \"team\" was replaced with \"seven\"\n",
            " ** idx 16 having word \"gharibyan\" was replaced with \"argues\"\n",
            " ** idx 17 having word \"and\" was replaced with \"chaotically\"\n",
            "Output: a couple of years later , another team made their speculation concrete . in 2019 , and and his collaborators\n",
            "*** Sentence 9 ***\n",
            "Input : gao , jafferis and model did already suggested in 2016 future passing through a wormhole ( a gravitational enterprise )\n",
            "Target: gao , jafferis and wall had already suggested in 2016 that passing through a wormhole ( a gravitational enterprise )\n",
            " ** idx 4 having word \"wall\" was replaced with \"model\"\n",
            " ** idx 5 having word \"had\" was replaced with \"did\"\n",
            " ** idx 10 having word \"that\" was replaced with \"future\"\n",
            "Output: gao , jafferis and had had already suggested in 2016 that passing through a wormhole ( a gravitational enterprise )\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Inferencing at a later point (w/o training model). We will use the embeddings saved from last training**"
      ],
      "metadata": {
        "id": "MBqqsve7iY6D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Loading the Test Input & create vocab if not already created\n",
        "\n",
        "print('loading testing text...')\n",
        "pth = './BERT/small_training.txt'\n",
        "test_sentences = open(pth).read().lower().split('\\n')\n",
        "\n",
        "print('tokenizing test_sentences...')\n",
        "special_chars = ',?;.:/*!+-()[]{}\"\\'&'\n",
        "test_sentences = [re.sub(f'[{re.escape(special_chars)}]', ' \\g<0> ', s).split(' ') for s in test_sentences]\n",
        "test_sentences = [[w for w in s if len(w)] for s in test_sentences]\n",
        "print(f'len(test_sentences) : {len(test_sentences)}')\n",
        "\n",
        "print('creating/loading vocab...')\n",
        "pth = './BERT/vocab.txt'\n",
        "if not exists(pth):\n",
        "    words = [w for s in sentences for w in s]\n",
        "    vocab = Counter(words).most_common(n_vocab) #keep the N most frequent words\n",
        "    vocab = [w[0] for w in vocab]\n",
        "    open(pth, 'w+').write('\\n'.join(vocab))\n",
        "else:\n",
        "    vocab = open(pth).read().split('\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5lSvdUhGMqU1",
        "outputId": "5477d56f-c6fd-4b94-af15-c85372afc7e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading testing text...\n",
            "tokenizing test_sentences...\n",
            "len(test_sentences) : 6\n",
            "creating/loading vocab...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#creating test_dataset, test_data_loader\n",
        "\n",
        "print('creating test_dataset and test_data_loader...')\n",
        "batch_size = 2\n",
        "test_dataset = SentencesDataset(test_sentences, vocab, seq_len)\n",
        "kwargs = {'shuffle':True,  'drop_last':True, 'pin_memory':True, 'batch_size':batch_size}\n",
        "test_data_loader = torch.utils.data.DataLoader(test_dataset, **kwargs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "si8MFkZp9yqJ",
        "outputId": "0d2d59e3-07a0-42bb-8e08-24c2861b4537"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "initializing..\n",
            "creating test_dataset and test_data_loader...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# initializingmodel (Use this step only if are not training the model)\n",
        "\n",
        "print('initializing..')\n",
        "batch_size = 2\n",
        "seq_len = 20\n",
        "embed_size = 128\n",
        "inner_ff_size = embed_size * 4\n",
        "n_heads = 8\n",
        "n_code = 8\n",
        "n_vocab = 40000\n",
        "dropout = 0.1\n",
        "\n",
        "model = Transformer(n_code, n_heads, embed_size, inner_ff_size, len(test_dataset.vocab), seq_len, dropout)\n",
        "model = model.cuda()\n",
        "\n",
        "# Getting the saved embedding from LAST training\n",
        "saved_embedding = np.genfromtxt(fname='./BERT/values_03242023a.tsv', delimiter='\\t')\n",
        "print(f'saved_embedding.shape : {saved_embedding.shape} type(saved_embedding) : {type(saved_embedding)}')\n",
        "# Loading the model embedding with the pretrained weight we just got above\n",
        "model.embeddings.weight.data.copy_(torch.from_numpy(saved_embedding))\n",
        "print(f'model.embeddings.weight.shape - {model.embeddings.weight.shape}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "96m8G9j9frcp",
        "outputId": "9ab8aa81-e2c5-499a-e6d5-7387d81ee428"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "saved_embedding.shape : (23948, 128) type(saved_embedding) : <class 'numpy.ndarray'>\n",
            "model.embeddings.weight.shape - torch.Size([23948, 128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating test_batch and test_input and test_target with just 1 sentence\n",
        "test_batch_iter = iter(test_data_loader)\n",
        "test_batch, test_batch_iter = get_batch(test_data_loader, test_batch_iter)\n",
        "print(f\"len(test_batch['input']) : {len(test_batch['input'])} len(test_batch['target']) : {len(test_batch['target'])}\")\n",
        "test_input = test_batch['input'][0].unsqueeze(0)\n",
        "test_target = test_batch['target'][0].unsqueeze(0)\n",
        "test_input = test_input.cuda(non_blocking=True)\n",
        "test_target = test_target.cuda(non_blocking=True)\n",
        "print(test_input.shape, test_target.shape, test_input)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "do_q4ayyNMjR",
        "outputId": "dc3c7c68-248c-4646-a5ae-82b8755afdf8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "len(test_batch['input']) : 2 len(test_batch['target']) : 2\n",
            "torch.Size([1, 20]) torch.Size([1, 20]) tensor([[   37, 10934,   114,     5,   107,    19,  4542, 23946,    14,    13,\n",
            "            29, 23946, 23946,     0,    45,    36,   887,    75,     6,  3263]],\n",
            "       device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prediction\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "      test_out = model(test_input)\n",
        "      test_out_f = test_out.argmax(dim=-1)\n",
        "print(test_out.shape, test_out_f.shape)\n",
        "print(test_input)\n",
        "print(test_out_f)"
      ],
      "metadata": {
        "id": "QGN-A7NUOGqS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd3ea23c-54a9-4018-9165-d22c1682dc97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 20, 23948]) torch.Size([1, 20])\n",
            "tensor([[   37, 10934,   114,     5,   107,    19,  4542, 23946,    14,    13,\n",
            "            29, 23946, 23946,     0,    45,    36,   887,    75,     6,  3263]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 9884,  1912, 17347,  1885, 17609, 16511, 22379, 18563, 13211, 21702,\n",
            "         19497, 22380,  8627, 17609,  7169, 18382,  7146, 15389, 17347,  6336]],\n",
            "       device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Using test_dataset.rvocab for decoding the output\n",
        "test_input_2 = test_input.squeeze(0).tolist()\n",
        "test_output  = test_out_f.squeeze(0).tolist()\n",
        "s_in = []\n",
        "for elem in test_input_2:\n",
        "   s_in.append(test_dataset.rvocab[elem])\n",
        "print(s_in)\n",
        "s_out = []\n",
        "for elem in test_output:\n",
        "   s_out.append(test_dataset.rvocab[elem])\n",
        "print(s_out)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9hAXKe6UON9E",
        "outputId": "7fd09b7a-147d-4180-81f0-98c04dbedbfd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['what', 'dimensions', 'can', 'i', 'give', 'for', 'text', '<oov>', '?', 'in', 'this', '<oov>', '<oov>', ',', 'we', 'will', 'learn', 'how', 'to', 'create']\n",
            "['accessary', 'aim', 'joyed', 'victory', 'capacities', 'cannibally', 'spangle', 'minotaurs', 'corporate', 'seymour', 'wheer', 'allots', 'pursuing', 'capacities', 'searching', 'fightest', 'bolts', '123', 'joyed', 'castles']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "U8o9a2E1OYSS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}