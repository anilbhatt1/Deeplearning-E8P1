{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jjWcv_ZOhpqK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa3aec13-3390-4b7a-9847-8ff58dc93b05"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat Mar 25 11:36:04 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   66C    P8    10W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4znsvw6i30zW",
        "outputId": "538bb00f-2670-489c-bd20-61ba3434d0c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r \"/content/gdrive/My Drive/EVA8_S11_Course_Docs/BERT\" \".\""
      ],
      "metadata": {
        "id": "__NCPl1T330S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "import torch.nn.functional as F\n",
        "from collections import Counter\n",
        "from os.path import exists\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "import math\n",
        "import re\n",
        "import copy"
      ],
      "metadata": {
        "id": "wsz00AFnr2Kl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# Transformer\n",
        "# =============================================================================\n",
        "def attention(q, k, v, mask = None, dropout = None):\n",
        "    scores = q.matmul(k.transpose(-2, -1))\n",
        "    scores /= math.sqrt(q.shape[-1])\n",
        "\n",
        "    #mask\n",
        "    scores = scores if mask is None else scores.masked_fill(mask == 0, -1e3)\n",
        "\n",
        "    scores = F.softmax(scores, dim = -1)\n",
        "    scores = dropout(scores) if dropout is not None else scores\n",
        "    output = scores.matmul(v)\n",
        "    return output\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, n_heads, out_dim, dropout=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.linear = nn.Linear(out_dim, out_dim*3)\n",
        "\n",
        "        self.n_heads = n_heads\n",
        "        self.out_dim = out_dim\n",
        "        self.out_dim_per_head = out_dim // n_heads\n",
        "        self.out = nn.Linear(out_dim, out_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def split_heads(self, t):\n",
        "         return t.reshape(t.shape[0], -1, self.n_heads, self.out_dim_per_head)\n",
        "\n",
        "    def forward(self, x, y=None, mask=None):\n",
        "        #in decoder, y comes from encoder. In encoder, y=x\n",
        "        y = x if y is None else y\n",
        "\n",
        "        qkv = self.linear(x) # BS * SEQ_LEN * (3*EMBED_SIZE_L)\n",
        "        q = qkv[:, :, :self.out_dim] # BS * SEQ_LEN * EMBED_SIZE_L\n",
        "        k = qkv[:, :, self.out_dim:self.out_dim*2] # BS * SEQ_LEN * EMBED_SIZE_L\n",
        "        v = qkv[:, :, self.out_dim*2:] # BS * SEQ_LEN * EMBED_SIZE_L\n",
        "\n",
        "        #break into n_heads\n",
        "        q, k, v = [self.split_heads(t) for t in (q,k,v)]  # BS * SEQ_LEN * HEAD * EMBED_SIZE_P_HEAD\n",
        "        q, k, v = [t.transpose(1,2) for t in (q,k,v)]  # BS * HEAD * SEQ_LEN * EMBED_SIZE_P_HEAD\n",
        "\n",
        "        #n_heads => attention => merge the heads => mix information\n",
        "        scores = attention(q, k, v, mask, self.dropout) # BS * HEAD * SEQ_LEN * EMBED_SIZE_P_HEAD\n",
        "        scores = scores.transpose(1,2).contiguous().view(scores.shape[0], -1, self.out_dim) # BS * SEQ_LEN * EMBED_SIZE_L\n",
        "        out = self.out(scores)  # BS * SEQ_LEN * EMBED_SIZE\n",
        "\n",
        "        return out\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, inp_dim, inner_dim, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.linear1 = nn.Linear(inp_dim, inner_dim)\n",
        "        self.linear2 = nn.Linear(inner_dim, inp_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        #inp => inner => relu => dropout => inner => inp\n",
        "        return self.linear2(self.dropout(F.relu(self.linear1(x))))\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, n_heads, inner_transformer_size, inner_ff_size, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.mha = MultiHeadAttention(n_heads, inner_transformer_size, dropout)\n",
        "        self.ff = FeedForward(inner_transformer_size, inner_ff_size, dropout)\n",
        "        self.norm1 = nn.LayerNorm(inner_transformer_size)\n",
        "        self.norm2 = nn.LayerNorm(inner_transformer_size)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        x2 = self.norm1(x)\n",
        "        x = x + self.dropout1(self.mha(x2, mask=mask))\n",
        "        x2 = self.norm2(x)\n",
        "        x = x + self.dropout2(self.ff(x2))\n",
        "        return x\n",
        "\n",
        "# Positional Embedding\n",
        "class PositionalEmbedding(nn.Module):\n",
        "    def __init__(self, d_model, max_seq_len = 80):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        pe = torch.zeros(max_seq_len, d_model)\n",
        "        pe.requires_grad = False\n",
        "        for pos in range(max_seq_len):\n",
        "            for i in range(0, d_model, 2):\n",
        "                pe[pos, i] = math.sin(pos / (10000 ** ((2 * i)/d_model)))\n",
        "                pe[pos, i + 1] = math.cos(pos / (10000 ** ((2 * (i + 1))/d_model)))\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.pe[:,:x.size(1)] #x.size(1) = seq_len"
      ],
      "metadata": {
        "id": "gzxVl7g-hw1v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, n_code, n_heads, embed_size, inner_ff_size, n_embeddings, seq_len, dropout=.1, saved_embed = None):\n",
        "        super().__init__()\n",
        "\n",
        "        #model input\n",
        "        self.embeddings = nn.Embedding(n_embeddings, embed_size)\n",
        "        self.pe = PositionalEmbedding(embed_size, seq_len)\n",
        "\n",
        "        #backbone\n",
        "        encoders = []\n",
        "        for i in range(n_code):\n",
        "            encoders += [EncoderLayer(n_heads, embed_size, inner_ff_size, dropout)]\n",
        "        self.encoders = nn.ModuleList(encoders)\n",
        "\n",
        "        #language model\n",
        "        self.norm = nn.LayerNorm(embed_size)\n",
        "        self.linear = nn.Linear(embed_size, n_embeddings, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embeddings(x)\n",
        "        x = x + self.pe(x)\n",
        "        for encoder in self.encoders:\n",
        "            x = encoder(x)\n",
        "        x = self.norm(x)\n",
        "        x = self.linear(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "qyCObtpBh7Zk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# Dataset\n",
        "# =============================================================================\n",
        "class SentencesDataset(Dataset):\n",
        "    #Init dataset\n",
        "    def __init__(self, sentences, vocab, seq_len):\n",
        "        dataset = self\n",
        "\n",
        "        dataset.sentences = sentences\n",
        "        dataset.orig_vocab = vocab\n",
        "        dataset.vocab = vocab + ['<ignore>', '<oov>', '<mask>']\n",
        "        dataset.vocab = {e:i for i, e in enumerate(dataset.vocab)}\n",
        "        dataset.rvocab = {v:k for k,v in dataset.vocab.items()}\n",
        "        dataset.seq_len = seq_len\n",
        "\n",
        "        #special tags\n",
        "        dataset.IGNORE_IDX = dataset.vocab['<ignore>'] #replacement tag for tokens to ignore\n",
        "        dataset.OUT_OF_VOCAB_IDX = dataset.vocab['<oov>'] #replacement tag for unknown words\n",
        "        dataset.MASK_IDX = dataset.vocab['<mask>'] #replacement tag for the masked word prediction task\n",
        "\n",
        "    #fetch data\n",
        "    def __getitem__(self, index, p_random_mask=0.15):\n",
        "        dataset = self\n",
        "\n",
        "        #while we don't have enough word to fill the sentence for a batch\n",
        "        s = []\n",
        "        while len(s) < dataset.seq_len:\n",
        "            s.extend(dataset.get_sentence_idx(index % len(dataset)))\n",
        "            index += 1\n",
        "\n",
        "        #ensure that the sequence is of length seq_len\n",
        "        s = s[:dataset.seq_len]\n",
        "        [s.append(dataset.IGNORE_IDX) for i in range(dataset.seq_len - len(s))] #PAD ok\n",
        "\n",
        "        #apply random mask\n",
        "        #rand_rand = random.random()\n",
        "        # s = [(dataset.MASK_IDX, w) if rand_rand < p_random_mask else (w, dataset.IGNORE_IDX) for w in s]\n",
        "\n",
        "        # Replacing 15% of words in the sentence with random words selected from vocab\n",
        "        s_repl = copy.deepcopy(s)\n",
        "        noisy_idx = random.sample(range(0, seq_len-1), int(seq_len*0.15))  # Selecting the index of 15% words that need to be replaced\n",
        "        vocab_idx = random.sample( range(0, (len(dataset.orig_vocab)-1) ), int(seq_len*0.15))  # Selecting the index of vocab words for replacing\n",
        "        repl_idx = list(zip(noisy_idx, vocab_idx))\n",
        "        for orig, repl in repl_idx:\n",
        "            s_repl[orig] = repl\n",
        "\n",
        "        data_dict = {'input': torch.Tensor([w for w in s_repl]).long(),\n",
        "                     'target': torch.Tensor([w for w in s]).long()}\n",
        "\n",
        "        return data_dict\n",
        "\n",
        "    #return length\n",
        "    def __len__(self):\n",
        "        return len(self.sentences)\n",
        "\n",
        "    #get words id\n",
        "    def get_sentence_idx(self, index):\n",
        "        dataset = self\n",
        "        s = dataset.sentences[index]\n",
        "        s = [dataset.vocab[w] if w in dataset.vocab else dataset.OUT_OF_VOCAB_IDX for w in s]\n",
        "        return s"
      ],
      "metadata": {
        "id": "lXwOPxwTiBW3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# Methods / Class\n",
        "# =============================================================================\n",
        "def get_batch(loader, loader_iter):\n",
        "    try:\n",
        "        batch = next(loader_iter)\n",
        "    except StopIteration:\n",
        "        loader_iter = iter(loader)\n",
        "        batch = next(loader_iter)\n",
        "    return batch, loader_iter"
      ],
      "metadata": {
        "id": "py05esgkiHY1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# #Init\n",
        "# =============================================================================\n",
        "print('initializing..')\n",
        "batch_size = 1024\n",
        "seq_len = 20\n",
        "embed_size = 128\n",
        "inner_ff_size = embed_size * 4\n",
        "n_heads = 8\n",
        "n_code = 8\n",
        "n_vocab = 40000\n",
        "dropout = 0.1\n",
        "# n_workers = 12\n",
        "\n",
        "#optimizer\n",
        "optim_kwargs = {'lr':1e-4, 'weight_decay':1e-4, 'betas':(.9,.999)}"
      ],
      "metadata": {
        "id": "awTIvUtEiJzF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c5d7906-b199-4ae5-afaf-3da52b05eb3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "initializing..\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# Input\n",
        "# =============================================================================\n",
        "#1) load text\n",
        "print('loading text...')\n",
        "pth = './BERT/training.txt'\n",
        "# pth = './BERT/small_training.txt'\n",
        "sentences = open(pth).read().lower().split('\\n')\n",
        "\n",
        "#2) tokenize sentences (can be done during training, you can also use spacy udpipe)\n",
        "print('tokenizing sentences...')\n",
        "special_chars = ',?;.:/*!+-()[]{}\"\\'&'\n",
        "sentences = [re.sub(f'[{re.escape(special_chars)}]', ' \\g<0> ', s).split(' ') for s in sentences]\n",
        "sentences = [[w for w in s if len(w)] for s in sentences]\n",
        "\n",
        "#3) create vocab if not already created\n",
        "print('creating/loading vocab...')\n",
        "pth = './BERT/vocab.txt'\n",
        "if not exists(pth):\n",
        "    words = [w for s in sentences for w in s]\n",
        "    vocab = Counter(words).most_common(n_vocab) #keep the N most frequent words\n",
        "    vocab = [w[0] for w in vocab]\n",
        "    open(pth, 'w+').write('\\n'.join(vocab))\n",
        "else:\n",
        "    vocab = open(pth).read().split('\\n')\n",
        "\n",
        "print(vocab[4], vocab[1], vocab[-1])\n",
        "\n",
        "#4) create dataset\n",
        "print('creating dataset...')\n",
        "dataset = SentencesDataset(sentences, vocab, seq_len)\n",
        "# kwargs = {'num_workers':n_workers, 'shuffle':True,  'drop_last':True, 'pin_memory':True, 'batch_size':batch_size}\n",
        "kwargs = {'shuffle':True,  'drop_last':True, 'pin_memory':True, 'batch_size':batch_size}\n",
        "data_loader = torch.utils.data.DataLoader(dataset, **kwargs)"
      ],
      "metadata": {
        "id": "OHyOHDPIiMfA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "249a85f3-e38a-49f2-e65c-e8f166738d98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading text...\n",
            "tokenizing sentences...\n",
            "creating/loading vocab...\n",
            "and . glowed\n",
            "creating dataset...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# Model\n",
        "# =============================================================================\n",
        "#init model\n",
        "print('initializing model...')\n",
        "model = Transformer(n_code, n_heads, embed_size, inner_ff_size, len(dataset.vocab), seq_len, dropout)\n",
        "model = model.cuda()\n",
        "model.to(\"cuda\")"
      ],
      "metadata": {
        "id": "rMnVrzQ1iSlj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4023d21-3154-4087-c7f8-eb298315e5b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "initializing model...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Transformer(\n",
              "  (embeddings): Embedding(23948, 128)\n",
              "  (pe): PositionalEmbedding()\n",
              "  (encoders): ModuleList(\n",
              "    (0): EncoderLayer(\n",
              "      (mha): MultiHeadAttention(\n",
              "        (linear): Linear(in_features=128, out_features=384, bias=True)\n",
              "        (out): Linear(in_features=128, out_features=128, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
              "        (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "      (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout1): Dropout(p=0.1, inplace=False)\n",
              "      (dropout2): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (1): EncoderLayer(\n",
              "      (mha): MultiHeadAttention(\n",
              "        (linear): Linear(in_features=128, out_features=384, bias=True)\n",
              "        (out): Linear(in_features=128, out_features=128, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
              "        (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "      (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout1): Dropout(p=0.1, inplace=False)\n",
              "      (dropout2): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (2): EncoderLayer(\n",
              "      (mha): MultiHeadAttention(\n",
              "        (linear): Linear(in_features=128, out_features=384, bias=True)\n",
              "        (out): Linear(in_features=128, out_features=128, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
              "        (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "      (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout1): Dropout(p=0.1, inplace=False)\n",
              "      (dropout2): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (3): EncoderLayer(\n",
              "      (mha): MultiHeadAttention(\n",
              "        (linear): Linear(in_features=128, out_features=384, bias=True)\n",
              "        (out): Linear(in_features=128, out_features=128, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
              "        (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "      (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout1): Dropout(p=0.1, inplace=False)\n",
              "      (dropout2): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (4): EncoderLayer(\n",
              "      (mha): MultiHeadAttention(\n",
              "        (linear): Linear(in_features=128, out_features=384, bias=True)\n",
              "        (out): Linear(in_features=128, out_features=128, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
              "        (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "      (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout1): Dropout(p=0.1, inplace=False)\n",
              "      (dropout2): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (5): EncoderLayer(\n",
              "      (mha): MultiHeadAttention(\n",
              "        (linear): Linear(in_features=128, out_features=384, bias=True)\n",
              "        (out): Linear(in_features=128, out_features=128, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
              "        (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "      (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout1): Dropout(p=0.1, inplace=False)\n",
              "      (dropout2): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (6): EncoderLayer(\n",
              "      (mha): MultiHeadAttention(\n",
              "        (linear): Linear(in_features=128, out_features=384, bias=True)\n",
              "        (out): Linear(in_features=128, out_features=128, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
              "        (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "      (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout1): Dropout(p=0.1, inplace=False)\n",
              "      (dropout2): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (7): EncoderLayer(\n",
              "      (mha): MultiHeadAttention(\n",
              "        (linear): Linear(in_features=128, out_features=384, bias=True)\n",
              "        (out): Linear(in_features=128, out_features=128, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
              "        (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "      (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout1): Dropout(p=0.1, inplace=False)\n",
              "      (dropout2): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "  )\n",
              "  (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "  (linear): Linear(in_features=128, out_features=23948, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# Optimizer\n",
        "# =============================================================================\n",
        "print('initializing optimizer and loss...')\n",
        "optimizer = optim.Adam(model.parameters(), **optim_kwargs)\n",
        "loss_model = nn.CrossEntropyLoss(ignore_index=dataset.IGNORE_IDX)"
      ],
      "metadata": {
        "id": "3X4pzcgHiUJ0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0cd6b1c0-06b4-4e06-f7b1-9b78afc7313e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "initializing optimizer and loss...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# Train\n",
        "# =============================================================================\n",
        "print('training...')\n",
        "print_each = 20\n",
        "model.train()\n",
        "batch_iter = iter(data_loader)\n",
        "n_iteration = 6000\n",
        "for it in range(n_iteration):\n",
        "    #get batch\n",
        "    batch, batch_iter = get_batch(data_loader, batch_iter)\n",
        "\n",
        "    #infer\n",
        "    masked_input = batch['input']\n",
        "    masked_target = batch['target']\n",
        "\n",
        "    masked_input = masked_input.cuda(non_blocking=True)\n",
        "    masked_target = masked_target.cuda(non_blocking=True)\n",
        "    output = model(masked_input)\n",
        "    #compute the cross entropy loss\n",
        "    output_v = output.view(-1,output.shape[-1])\n",
        "    target_v = masked_target.view(-1,1).squeeze()\n",
        "    loss = loss_model(output_v, target_v)\n",
        "    #compute gradients\n",
        "    loss.backward()\n",
        "\n",
        "    #apply gradients\n",
        "    optimizer.step()\n",
        "\n",
        "    #print step\n",
        "    if it % print_each == 0:\n",
        "        print('it:', it,\n",
        "              ' | loss', np.round(loss.item(),2),\n",
        "              ' | Δw:', round(model.embeddings.weight.grad.abs().sum().item(),3))\n",
        "\n",
        "    #reset gradients\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "# =============================================================================\n",
        "# Saving the embeddings for future use\n",
        "# =============================================================================\n",
        "print('saving embeddings for future use...')\n",
        "print(f'model.embeddings.weight.shape - {model.embeddings.weight.shape}, len(dataset.rvocab) : {len(dataset.rvocab)}')\n",
        "N = len(dataset.rvocab)\n",
        "np.savetxt('values_03252023a.tsv', np.round(model.embeddings.weight.detach().cpu().numpy()[0:N], 2), delimiter='\\t', fmt='%1.2f')\n",
        "s = [dataset.rvocab[i] for i in range(N)]\n",
        "open('names_032523a.tsv', 'w+').write('\\n'.join(s) )\n",
        "\n",
        "print('end')"
      ],
      "metadata": {
        "id": "I4qNQlOPiVl-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2019ab3b-d211-4a98-abe2-84c65d801502"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training...\n",
            "it: 0  | loss 10.23  | Δw: 4.233\n",
            "it: 20  | loss 8.87  | Δw: 3.463\n",
            "it: 40  | loss 8.06  | Δw: 3.475\n",
            "it: 60  | loss 7.49  | Δw: 3.265\n",
            "it: 80  | loss 6.99  | Δw: 3.069\n",
            "it: 100  | loss 6.59  | Δw: 2.951\n",
            "it: 120  | loss 6.19  | Δw: 2.798\n",
            "it: 140  | loss 5.88  | Δw: 2.715\n",
            "it: 160  | loss 5.55  | Δw: 2.617\n",
            "it: 180  | loss 5.25  | Δw: 2.547\n",
            "it: 200  | loss 4.98  | Δw: 2.505\n",
            "it: 220  | loss 4.77  | Δw: 2.474\n",
            "it: 240  | loss 4.55  | Δw: 2.465\n",
            "it: 260  | loss 4.36  | Δw: 2.459\n",
            "it: 280  | loss 4.16  | Δw: 2.478\n",
            "it: 300  | loss 4.02  | Δw: 2.474\n",
            "it: 320  | loss 3.86  | Δw: 2.489\n",
            "it: 340  | loss 3.68  | Δw: 2.466\n",
            "it: 360  | loss 3.58  | Δw: 2.508\n",
            "it: 380  | loss 3.48  | Δw: 2.514\n",
            "it: 400  | loss 3.38  | Δw: 2.531\n",
            "it: 420  | loss 3.31  | Δw: 2.52\n",
            "it: 440  | loss 3.21  | Δw: 2.525\n",
            "it: 460  | loss 3.14  | Δw: 2.537\n",
            "it: 480  | loss 3.07  | Δw: 2.541\n",
            "it: 500  | loss 2.98  | Δw: 2.535\n",
            "it: 520  | loss 2.92  | Δw: 2.517\n",
            "it: 540  | loss 2.88  | Δw: 2.552\n",
            "it: 560  | loss 2.79  | Δw: 2.53\n",
            "it: 580  | loss 2.75  | Δw: 2.506\n",
            "it: 600  | loss 2.72  | Δw: 2.527\n",
            "it: 620  | loss 2.65  | Δw: 2.532\n",
            "it: 640  | loss 2.64  | Δw: 2.536\n",
            "it: 660  | loss 2.57  | Δw: 2.527\n",
            "it: 680  | loss 2.52  | Δw: 2.535\n",
            "it: 700  | loss 2.5  | Δw: 2.554\n",
            "it: 720  | loss 2.43  | Δw: 2.515\n",
            "it: 740  | loss 2.4  | Δw: 2.502\n",
            "it: 760  | loss 2.35  | Δw: 2.501\n",
            "it: 780  | loss 2.31  | Δw: 2.501\n",
            "it: 800  | loss 2.33  | Δw: 2.52\n",
            "it: 820  | loss 2.3  | Δw: 2.516\n",
            "it: 840  | loss 2.31  | Δw: 2.57\n",
            "it: 860  | loss 2.25  | Δw: 2.553\n",
            "it: 880  | loss 2.22  | Δw: 2.578\n",
            "it: 900  | loss 2.21  | Δw: 2.564\n",
            "it: 920  | loss 2.14  | Δw: 2.56\n",
            "it: 940  | loss 2.13  | Δw: 2.581\n",
            "it: 960  | loss 2.11  | Δw: 2.599\n",
            "it: 980  | loss 2.13  | Δw: 2.617\n",
            "it: 1000  | loss 2.11  | Δw: 2.642\n",
            "it: 1020  | loss 2.05  | Δw: 2.634\n",
            "it: 1040  | loss 2.03  | Δw: 2.619\n",
            "it: 1060  | loss 1.99  | Δw: 2.652\n",
            "it: 1080  | loss 2.01  | Δw: 2.672\n",
            "it: 1100  | loss 1.98  | Δw: 2.707\n",
            "it: 1120  | loss 1.97  | Δw: 2.676\n",
            "it: 1140  | loss 1.96  | Δw: 2.694\n",
            "it: 1160  | loss 1.97  | Δw: 2.705\n",
            "it: 1180  | loss 1.95  | Δw: 2.734\n",
            "it: 1200  | loss 1.96  | Δw: 2.77\n",
            "it: 1220  | loss 1.91  | Δw: 2.768\n",
            "it: 1240  | loss 1.86  | Δw: 2.778\n",
            "it: 1260  | loss 1.88  | Δw: 2.844\n",
            "it: 1280  | loss 1.87  | Δw: 2.764\n",
            "it: 1300  | loss 1.86  | Δw: 2.848\n",
            "it: 1320  | loss 1.89  | Δw: 2.819\n",
            "it: 1340  | loss 1.84  | Δw: 2.816\n",
            "it: 1360  | loss 1.83  | Δw: 2.793\n",
            "it: 1380  | loss 1.82  | Δw: 2.805\n",
            "it: 1400  | loss 1.81  | Δw: 2.828\n",
            "it: 1420  | loss 1.79  | Δw: 2.804\n",
            "it: 1440  | loss 1.8  | Δw: 2.87\n",
            "it: 1460  | loss 1.77  | Δw: 2.827\n",
            "it: 1480  | loss 1.77  | Δw: 2.828\n",
            "it: 1500  | loss 1.76  | Δw: 2.823\n",
            "it: 1520  | loss 1.77  | Δw: 2.863\n",
            "it: 1540  | loss 1.74  | Δw: 2.836\n",
            "it: 1560  | loss 1.75  | Δw: 2.857\n",
            "it: 1580  | loss 1.72  | Δw: 2.843\n",
            "it: 1600  | loss 1.74  | Δw: 2.907\n",
            "it: 1620  | loss 1.74  | Δw: 2.922\n",
            "it: 1640  | loss 1.71  | Δw: 2.839\n",
            "it: 1660  | loss 1.7  | Δw: 2.836\n",
            "it: 1680  | loss 1.68  | Δw: 2.859\n",
            "it: 1700  | loss 1.7  | Δw: 2.871\n",
            "it: 1720  | loss 1.69  | Δw: 2.877\n",
            "it: 1740  | loss 1.71  | Δw: 2.892\n",
            "it: 1760  | loss 1.7  | Δw: 2.932\n",
            "it: 1780  | loss 1.66  | Δw: 2.908\n",
            "it: 1800  | loss 1.65  | Δw: 2.894\n",
            "it: 1820  | loss 1.66  | Δw: 2.864\n",
            "it: 1840  | loss 1.64  | Δw: 2.878\n",
            "it: 1860  | loss 1.65  | Δw: 2.913\n",
            "it: 1880  | loss 1.63  | Δw: 2.898\n",
            "it: 1900  | loss 1.67  | Δw: 2.927\n",
            "it: 1920  | loss 1.63  | Δw: 2.947\n",
            "it: 1940  | loss 1.66  | Δw: 2.942\n",
            "it: 1960  | loss 1.64  | Δw: 2.899\n",
            "it: 1980  | loss 1.63  | Δw: 2.957\n",
            "it: 2000  | loss 1.62  | Δw: 2.973\n",
            "it: 2020  | loss 1.63  | Δw: 3.024\n",
            "it: 2040  | loss 1.58  | Δw: 2.911\n",
            "it: 2060  | loss 1.64  | Δw: 3.031\n",
            "it: 2080  | loss 1.61  | Δw: 2.923\n",
            "it: 2100  | loss 1.58  | Δw: 2.882\n",
            "it: 2120  | loss 1.56  | Δw: 2.894\n",
            "it: 2140  | loss 1.58  | Δw: 2.894\n",
            "it: 2160  | loss 1.61  | Δw: 2.937\n",
            "it: 2180  | loss 1.57  | Δw: 2.904\n",
            "it: 2200  | loss 1.56  | Δw: 2.946\n",
            "it: 2220  | loss 1.56  | Δw: 2.967\n",
            "it: 2240  | loss 1.58  | Δw: 2.968\n",
            "it: 2260  | loss 1.55  | Δw: 2.935\n",
            "it: 2280  | loss 1.57  | Δw: 2.912\n",
            "it: 2300  | loss 1.57  | Δw: 2.908\n",
            "it: 2320  | loss 1.54  | Δw: 2.971\n",
            "it: 2340  | loss 1.57  | Δw: 2.941\n",
            "it: 2360  | loss 1.51  | Δw: 2.896\n",
            "it: 2380  | loss 1.56  | Δw: 2.969\n",
            "it: 2400  | loss 1.58  | Δw: 2.919\n",
            "it: 2420  | loss 1.53  | Δw: 2.912\n",
            "it: 2440  | loss 1.55  | Δw: 2.972\n",
            "it: 2460  | loss 1.54  | Δw: 3.01\n",
            "it: 2480  | loss 1.58  | Δw: 3.012\n",
            "it: 2500  | loss 1.54  | Δw: 2.956\n",
            "it: 2520  | loss 1.54  | Δw: 2.987\n",
            "it: 2540  | loss 1.53  | Δw: 2.97\n",
            "it: 2560  | loss 1.55  | Δw: 3.038\n",
            "it: 2580  | loss 1.5  | Δw: 2.952\n",
            "it: 2600  | loss 1.52  | Δw: 2.956\n",
            "it: 2620  | loss 1.54  | Δw: 3.013\n",
            "it: 2640  | loss 1.54  | Δw: 3.008\n",
            "it: 2660  | loss 1.48  | Δw: 2.917\n",
            "it: 2680  | loss 1.55  | Δw: 2.952\n",
            "it: 2700  | loss 1.55  | Δw: 2.991\n",
            "it: 2720  | loss 1.53  | Δw: 3.052\n",
            "it: 2740  | loss 1.5  | Δw: 2.995\n",
            "it: 2760  | loss 1.5  | Δw: 2.987\n",
            "it: 2780  | loss 1.48  | Δw: 2.935\n",
            "it: 2800  | loss 1.5  | Δw: 2.994\n",
            "it: 2820  | loss 1.51  | Δw: 3.086\n",
            "it: 2840  | loss 1.5  | Δw: 2.972\n",
            "it: 2860  | loss 1.48  | Δw: 2.947\n",
            "it: 2880  | loss 1.49  | Δw: 2.979\n",
            "it: 2900  | loss 1.47  | Δw: 2.988\n",
            "it: 2920  | loss 1.49  | Δw: 3.01\n",
            "it: 2940  | loss 1.46  | Δw: 2.953\n",
            "it: 2960  | loss 1.48  | Δw: 2.977\n",
            "it: 2980  | loss 1.48  | Δw: 2.938\n",
            "it: 3000  | loss 1.49  | Δw: 3.012\n",
            "it: 3020  | loss 1.5  | Δw: 3.141\n",
            "it: 3040  | loss 1.5  | Δw: 3.008\n",
            "it: 3060  | loss 1.47  | Δw: 2.924\n",
            "it: 3080  | loss 1.47  | Δw: 2.976\n",
            "it: 3100  | loss 1.49  | Δw: 3.032\n",
            "it: 3120  | loss 1.49  | Δw: 2.978\n",
            "it: 3140  | loss 1.46  | Δw: 2.981\n",
            "it: 3160  | loss 1.49  | Δw: 3.029\n",
            "it: 3180  | loss 1.44  | Δw: 3.013\n",
            "it: 3200  | loss 1.47  | Δw: 3.014\n",
            "it: 3220  | loss 1.48  | Δw: 2.971\n",
            "it: 3240  | loss 1.48  | Δw: 2.983\n",
            "it: 3260  | loss 1.45  | Δw: 2.985\n",
            "it: 3280  | loss 1.43  | Δw: 2.992\n",
            "it: 3300  | loss 1.47  | Δw: 3.047\n",
            "it: 3320  | loss 1.43  | Δw: 2.983\n",
            "it: 3340  | loss 1.43  | Δw: 2.999\n",
            "it: 3360  | loss 1.44  | Δw: 2.993\n",
            "it: 3380  | loss 1.46  | Δw: 3.041\n",
            "it: 3400  | loss 1.42  | Δw: 3.036\n",
            "it: 3420  | loss 1.45  | Δw: 3.135\n",
            "it: 3440  | loss 1.43  | Δw: 3.09\n",
            "it: 3460  | loss 1.43  | Δw: 3.034\n",
            "it: 3480  | loss 1.41  | Δw: 3.034\n",
            "it: 3500  | loss 1.44  | Δw: 3.031\n",
            "it: 3520  | loss 1.44  | Δw: 3.003\n",
            "it: 3540  | loss 1.42  | Δw: 3.01\n",
            "it: 3560  | loss 1.46  | Δw: 3.12\n",
            "it: 3580  | loss 1.41  | Δw: 3.044\n",
            "it: 3600  | loss 1.42  | Δw: 3.056\n",
            "it: 3620  | loss 1.41  | Δw: 3.117\n",
            "it: 3640  | loss 1.43  | Δw: 3.077\n",
            "it: 3660  | loss 1.39  | Δw: 3.036\n",
            "it: 3680  | loss 1.39  | Δw: 3.079\n",
            "it: 3700  | loss 1.43  | Δw: 3.102\n",
            "it: 3720  | loss 1.4  | Δw: 3.038\n",
            "it: 3740  | loss 1.42  | Δw: 3.035\n",
            "it: 3760  | loss 1.43  | Δw: 3.088\n",
            "it: 3780  | loss 1.37  | Δw: 3.189\n",
            "it: 3800  | loss 1.41  | Δw: 3.115\n",
            "it: 3820  | loss 1.42  | Δw: 3.112\n",
            "it: 3840  | loss 1.41  | Δw: 3.136\n",
            "it: 3860  | loss 1.42  | Δw: 3.132\n",
            "it: 3880  | loss 1.39  | Δw: 3.122\n",
            "it: 3900  | loss 1.4  | Δw: 3.137\n",
            "it: 3920  | loss 1.41  | Δw: 3.089\n",
            "it: 3940  | loss 1.39  | Δw: 3.115\n",
            "it: 3960  | loss 1.4  | Δw: 3.081\n",
            "it: 3980  | loss 1.4  | Δw: 3.09\n",
            "it: 4000  | loss 1.37  | Δw: 3.106\n",
            "it: 4020  | loss 1.43  | Δw: 3.192\n",
            "it: 4040  | loss 1.39  | Δw: 3.051\n",
            "it: 4060  | loss 1.39  | Δw: 3.09\n",
            "it: 4080  | loss 1.4  | Δw: 3.162\n",
            "it: 4100  | loss 1.39  | Δw: 3.217\n",
            "it: 4120  | loss 1.39  | Δw: 3.081\n",
            "it: 4140  | loss 1.38  | Δw: 3.127\n",
            "it: 4160  | loss 1.37  | Δw: 3.132\n",
            "it: 4180  | loss 1.38  | Δw: 3.215\n",
            "it: 4200  | loss 1.39  | Δw: 3.107\n",
            "it: 4220  | loss 1.4  | Δw: 3.116\n",
            "it: 4240  | loss 1.38  | Δw: 3.169\n",
            "it: 4260  | loss 1.36  | Δw: 3.101\n",
            "it: 4280  | loss 1.4  | Δw: 3.182\n",
            "it: 4300  | loss 1.37  | Δw: 3.13\n",
            "it: 4320  | loss 1.38  | Δw: 3.121\n",
            "it: 4340  | loss 1.38  | Δw: 3.112\n",
            "it: 4360  | loss 1.37  | Δw: 3.185\n",
            "it: 4380  | loss 1.38  | Δw: 3.212\n",
            "it: 4400  | loss 1.37  | Δw: 3.175\n",
            "it: 4420  | loss 1.38  | Δw: 3.197\n",
            "it: 4440  | loss 1.37  | Δw: 3.159\n",
            "it: 4460  | loss 1.34  | Δw: 3.13\n",
            "it: 4480  | loss 1.36  | Δw: 3.142\n",
            "it: 4500  | loss 1.36  | Δw: 3.131\n",
            "it: 4520  | loss 1.39  | Δw: 3.156\n",
            "it: 4540  | loss 1.4  | Δw: 3.132\n",
            "it: 4560  | loss 1.38  | Δw: 3.256\n",
            "it: 4580  | loss 1.33  | Δw: 3.21\n",
            "it: 4600  | loss 1.38  | Δw: 3.246\n",
            "it: 4620  | loss 1.35  | Δw: 3.19\n",
            "it: 4640  | loss 1.38  | Δw: 3.286\n",
            "it: 4660  | loss 1.36  | Δw: 3.193\n",
            "it: 4680  | loss 1.35  | Δw: 3.278\n",
            "it: 4700  | loss 1.36  | Δw: 3.228\n",
            "it: 4720  | loss 1.33  | Δw: 3.168\n",
            "it: 4740  | loss 1.4  | Δw: 3.229\n",
            "it: 4760  | loss 1.35  | Δw: 3.174\n",
            "it: 4780  | loss 1.37  | Δw: 3.22\n",
            "it: 4800  | loss 1.35  | Δw: 3.233\n",
            "it: 4820  | loss 1.36  | Δw: 3.278\n",
            "it: 4840  | loss 1.37  | Δw: 3.263\n",
            "it: 4860  | loss 1.35  | Δw: 3.264\n",
            "it: 4880  | loss 1.37  | Δw: 3.253\n",
            "it: 4900  | loss 1.34  | Δw: 3.229\n",
            "it: 4920  | loss 1.35  | Δw: 3.273\n",
            "it: 4940  | loss 1.35  | Δw: 3.24\n",
            "it: 4960  | loss 1.34  | Δw: 3.231\n",
            "it: 4980  | loss 1.35  | Δw: 3.264\n",
            "it: 5000  | loss 1.34  | Δw: 3.307\n",
            "it: 5020  | loss 1.34  | Δw: 3.299\n",
            "it: 5040  | loss 1.34  | Δw: 3.276\n",
            "it: 5060  | loss 1.37  | Δw: 3.215\n",
            "it: 5080  | loss 1.3  | Δw: 3.298\n",
            "it: 5100  | loss 1.32  | Δw: 3.171\n",
            "it: 5120  | loss 1.33  | Δw: 3.259\n",
            "it: 5140  | loss 1.32  | Δw: 3.365\n",
            "it: 5160  | loss 1.33  | Δw: 3.271\n",
            "it: 5180  | loss 1.33  | Δw: 3.29\n",
            "it: 5200  | loss 1.32  | Δw: 3.203\n",
            "it: 5220  | loss 1.35  | Δw: 3.254\n",
            "it: 5240  | loss 1.32  | Δw: 3.28\n",
            "it: 5260  | loss 1.32  | Δw: 3.277\n",
            "it: 5280  | loss 1.32  | Δw: 3.349\n",
            "it: 5300  | loss 1.33  | Δw: 3.305\n",
            "it: 5320  | loss 1.33  | Δw: 3.322\n",
            "it: 5340  | loss 1.32  | Δw: 3.293\n",
            "it: 5360  | loss 1.33  | Δw: 3.298\n",
            "it: 5380  | loss 1.3  | Δw: 3.328\n",
            "it: 5400  | loss 1.34  | Δw: 3.387\n",
            "it: 5420  | loss 1.33  | Δw: 3.284\n",
            "it: 5440  | loss 1.31  | Δw: 3.329\n",
            "it: 5460  | loss 1.28  | Δw: 3.281\n",
            "it: 5480  | loss 1.31  | Δw: 3.357\n",
            "it: 5500  | loss 1.34  | Δw: 3.367\n",
            "it: 5520  | loss 1.33  | Δw: 3.352\n",
            "it: 5540  | loss 1.31  | Δw: 3.339\n",
            "it: 5560  | loss 1.31  | Δw: 3.399\n",
            "it: 5580  | loss 1.31  | Δw: 3.38\n",
            "it: 5600  | loss 1.32  | Δw: 3.42\n",
            "it: 5620  | loss 1.32  | Δw: 3.347\n",
            "it: 5640  | loss 1.31  | Δw: 3.409\n",
            "it: 5660  | loss 1.31  | Δw: 3.367\n",
            "it: 5680  | loss 1.32  | Δw: 3.397\n",
            "it: 5700  | loss 1.3  | Δw: 3.409\n",
            "it: 5720  | loss 1.3  | Δw: 3.428\n",
            "it: 5740  | loss 1.3  | Δw: 3.371\n",
            "it: 5760  | loss 1.32  | Δw: 3.44\n",
            "it: 5780  | loss 1.29  | Δw: 3.477\n",
            "it: 5800  | loss 1.31  | Δw: 3.399\n",
            "it: 5820  | loss 1.29  | Δw: 3.552\n",
            "it: 5840  | loss 1.3  | Δw: 3.438\n",
            "it: 5860  | loss 1.32  | Δw: 3.382\n",
            "it: 5880  | loss 1.31  | Δw: 3.499\n",
            "it: 5900  | loss 1.31  | Δw: 3.407\n",
            "it: 5920  | loss 1.3  | Δw: 3.507\n",
            "it: 5940  | loss 1.3  | Δw: 3.349\n",
            "it: 5960  | loss 1.3  | Δw: 3.492\n",
            "it: 5980  | loss 1.3  | Δw: 3.481\n",
            "saving embeddings for future use...\n",
            "model.embeddings.weight.shape - torch.Size([23948, 128]), len(dataset.rvocab) : 23948\n",
            "end\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r \"/content/values_03252023a.tsv\" \"/content/gdrive/My Drive/EVA8_S11_Course_Docs/BERT\"\n",
        "!cp -r \"/content/names_032523a.tsv\" \"/content/gdrive/My Drive/EVA8_S11_Course_Docs/BERT\""
      ],
      "metadata": {
        "id": "XMnme6pz1-fh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Quick inferencing with above trained model weights**"
      ],
      "metadata": {
        "id": "Y0J2aVzBiIcz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Loading the Test Input\n",
        "pth = './BERT/small_training.txt'\n",
        "test_sentences = open(pth).read().lower().split('\\n')\n",
        "\n",
        "print('tokenizing test_sentences...')\n",
        "special_chars = ',?;.:/*!+-()[]{}\"\\'&'\n",
        "test_sentences = [re.sub(f'[{re.escape(special_chars)}]', ' \\g<0> ', s).split(' ') for s in test_sentences]\n",
        "test_sentences = [[w for w in s if len(w)] for s in test_sentences]\n",
        "print(f'len(test_sentences) : {len(test_sentences)}')\n",
        "\n",
        "#creating test_dataset, test_data_loader\n",
        "print('creating test_dataset and test_data_loader...')\n",
        "batch_size = 5\n",
        "test_dataset = SentencesDataset(test_sentences, vocab, seq_len)\n",
        "kwargs = {'shuffle':True,  'drop_last':True, 'pin_memory':True, 'batch_size':batch_size}\n",
        "test_data_loader = torch.utils.data.DataLoader(test_dataset, **kwargs)\n",
        "\n",
        "# Creating test_batch and test_input and test_target with 5 sentences\n",
        "test_batch_iter = iter(test_data_loader)\n",
        "test_batch, test_batch_iter = get_batch(test_data_loader, test_batch_iter)\n",
        "print(f\"len(test_batch['input']) : {len(test_batch['input'])} len(test_batch['target']) : {len(test_batch['target'])}\")\n",
        "test_input = test_batch['input']\n",
        "test_target = test_batch['target']\n",
        "test_input = test_input.cuda(non_blocking=True)\n",
        "test_target = test_target.cuda(non_blocking=True)\n",
        "print(test_input.shape, test_target.shape)\n",
        "\n",
        "# Prediction\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "      test_out = model(test_input)\n",
        "      test_out_f = test_out.argmax(dim=-1)\n",
        "print(test_out.shape, test_out_f.shape)\n",
        "print(test_input)\n",
        "print(test_out_f)\n",
        "\n",
        "# Using test_dataset.rvocab for decoding the output\n",
        "test_input_2 = test_input.squeeze(0).tolist()\n",
        "test_output  = test_out_f.squeeze(0).tolist()\n",
        "s_in = []\n",
        "s_out = []\n",
        "for i in range(batch_size):\n",
        "   s_in = []\n",
        "   print(f'*** Sentence {i} ***')\n",
        "   for elem in test_input_2[i]:\n",
        "       s_in.append(test_dataset.rvocab[elem])\n",
        "   print(f'Input : {s_in}')\n",
        "   s_out = []\n",
        "   for elem in test_output[i]:\n",
        "       s_out.append(test_dataset.rvocab[elem])\n",
        "   print(f'Output: {s_out}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pR9xwxjviHiW",
        "outputId": "97161c40-31f4-43ac-89e7-f798be894503"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tokenizing test_sentences...\n",
            "len(test_sentences) : 5\n",
            "creating test_dataset and test_data_loader...\n",
            "len(test_batch['input']) : 5 len(test_batch['target']) : 5\n",
            "torch.Size([5, 20]) torch.Size([5, 20])\n",
            "torch.Size([5, 20, 23948]) torch.Size([5, 20])\n",
            "tensor([[    5,    93,  7161,  4320,  1898, 23946,    45,   217, 13220,     1,\n",
            "            37, 10934,   114,     5,   107,    19,  4542, 23946,  1970,    13],\n",
            "        [   13,    29, 23946, 23946,     0,    45,    36, 19173,    75,     6,\n",
            "          3263, 23302, 23946, 13465, 13220,    13, 23946,     4,    45,    36],\n",
            "        [ 3650,  7505,    45,    36,  2034,  5753, 23946,     1,     5,    68,\n",
            "            89,     6,  4785,    11, 23946,  4731, 13220,   190,  4542, 23946],\n",
            "        [   37, 10934,   114, 15391,   107,    19,  4542, 23946, 14388,    13,\n",
            "            29, 23946, 23946,     0,    45,    36,   887,  5353,     6,  3263],\n",
            "        [    5,    68,    89,     6,  3234,    11,   715,  4731, 13220,   190,\n",
            "          4542, 23946, 12381,  6114,     5,    93,    12,    19,  1898, 23946]],\n",
            "       device='cuda:0')\n",
            "tensor([[   5,   93,    1, 4320, 1898,    0,   45,  217,    0,    1,   37,    1,\n",
            "          114,    5,  107,   19, 4542,    0, 1970,   13],\n",
            "        [  13,   29,   83,   83,    0,   45,   36,    0,   75,    6, 3263,    0,\n",
            "           83,    4,    0,   13,   83,    4,   45,   36],\n",
            "        [3650,    1,   45,   36, 2034, 5753,    4,    1,    5,   68,   89,    6,\n",
            "         4785,   11,    4, 4731,    0,  190, 4542,    4],\n",
            "        [  37,    1,  114,    0,  107,   19, 4542,   83,    0,   13,   29,    4,\n",
            "           83,    0,   45,   36,  887, 5353,    6, 3263],\n",
            "        [   5,   68,   89,    6, 3234,   11,  715, 4731,    0,  190, 4542,    4,\n",
            "            0, 6114,    5,   93,   12,   19, 1898,    4]], device='cuda:0')\n",
            "*** Sentence 0 ***\n",
            "Input : ['i', 'know', 'outface', 'bank', 'image', '<oov>', 'we', 'use', 'summary', '.', 'what', 'dimensions', 'can', 'i', 'give', 'for', 'text', '<oov>', 'runs', 'in']\n",
            "Output: ['i', 'know', '.', 'bank', 'image', ',', 'we', 'use', ',', '.', 'what', '.', 'can', 'i', 'give', 'for', 'text', ',', 'runs', 'in']\n",
            "*** Sentence 1 ***\n",
            "Input : ['in', 'this', '<oov>', '<oov>', ',', 'we', 'will', 'christenings', 'how', 'to', 'create', 'straps', '<oov>', 'notre', 'summary', 'in', '<oov>', 'and', 'we', 'will']\n",
            "Output: ['in', 'this', 'hath', 'hath', ',', 'we', 'will', ',', 'how', 'to', 'create', ',', 'hath', 'and', ',', 'in', 'hath', 'and', 'we', 'will']\n",
            "*** Sentence 2 ***\n",
            "Input : ['moreover', 'gud', 'we', 'will', 'cover', 'spectacle', '<oov>', '.', 'i', 'would', 'like', 'to', 'sinful', 'my', '<oov>', 'model', 'summary', '(', 'text', '<oov>']\n",
            "Output: ['moreover', '.', 'we', 'will', 'cover', 'spectacle', 'and', '.', 'i', 'would', 'like', 'to', 'sinful', 'my', 'and', 'model', ',', '(', 'text', 'and']\n",
            "*** Sentence 3 ***\n",
            "Input : ['what', 'dimensions', 'can', 'foist', 'give', 'for', 'text', '<oov>', 'likings', 'in', 'this', '<oov>', '<oov>', ',', 'we', 'will', 'learn', 'enow', 'to', 'create']\n",
            "Output: ['what', '.', 'can', ',', 'give', 'for', 'text', 'hath', ',', 'in', 'this', 'and', 'hath', ',', 'we', 'will', 'learn', 'enow', 'to', 'create']\n",
            "*** Sentence 4 ***\n",
            "Input : ['i', 'would', 'like', 'to', 'print', 'my', 'stands', 'model', 'summary', '(', 'text', '<oov>', 'males', 'fouler', 'i', 'know', 'that', 'for', 'image', '<oov>']\n",
            "Output: ['i', 'would', 'like', 'to', 'print', 'my', 'stands', 'model', ',', '(', 'text', 'and', ',', 'fouler', 'i', 'know', 'that', 'for', 'image', 'and']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Inferencing at a later point (w/o training model). We will use the embeddings saved from last training**"
      ],
      "metadata": {
        "id": "MBqqsve7iY6D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Loading the Test Input & create vocab if not already created\n",
        "\n",
        "print('loading testing text...')\n",
        "pth = './BERT/small_training.txt'\n",
        "test_sentences = open(pth).read().lower().split('\\n')\n",
        "\n",
        "print('tokenizing test_sentences...')\n",
        "special_chars = ',?;.:/*!+-()[]{}\"\\'&'\n",
        "test_sentences = [re.sub(f'[{re.escape(special_chars)}]', ' \\g<0> ', s).split(' ') for s in test_sentences]\n",
        "test_sentences = [[w for w in s if len(w)] for s in test_sentences]\n",
        "print(f'len(test_sentences) : {len(test_sentences)}')\n",
        "\n",
        "print('creating/loading vocab...')\n",
        "pth = './BERT/vocab.txt'\n",
        "if not exists(pth):\n",
        "    words = [w for s in sentences for w in s]\n",
        "    vocab = Counter(words).most_common(n_vocab) #keep the N most frequent words\n",
        "    vocab = [w[0] for w in vocab]\n",
        "    open(pth, 'w+').write('\\n'.join(vocab))\n",
        "else:\n",
        "    vocab = open(pth).read().split('\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5lSvdUhGMqU1",
        "outputId": "5477d56f-c6fd-4b94-af15-c85372afc7e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading testing text...\n",
            "tokenizing test_sentences...\n",
            "len(test_sentences) : 6\n",
            "creating/loading vocab...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#creating test_dataset, test_data_loader\n",
        "\n",
        "print('creating test_dataset and test_data_loader...')\n",
        "batch_size = 2\n",
        "test_dataset = SentencesDataset(test_sentences, vocab, seq_len)\n",
        "kwargs = {'shuffle':True,  'drop_last':True, 'pin_memory':True, 'batch_size':batch_size}\n",
        "test_data_loader = torch.utils.data.DataLoader(test_dataset, **kwargs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "si8MFkZp9yqJ",
        "outputId": "0d2d59e3-07a0-42bb-8e08-24c2861b4537"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "initializing..\n",
            "creating test_dataset and test_data_loader...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# initializingmodel (Use this step only if are not training the model)\n",
        "\n",
        "print('initializing..')\n",
        "batch_size = 2\n",
        "seq_len = 20\n",
        "embed_size = 128\n",
        "inner_ff_size = embed_size * 4\n",
        "n_heads = 8\n",
        "n_code = 8\n",
        "n_vocab = 40000\n",
        "dropout = 0.1\n",
        "\n",
        "model = Transformer(n_code, n_heads, embed_size, inner_ff_size, len(test_dataset.vocab), seq_len, dropout)\n",
        "model = model.cuda()\n",
        "\n",
        "# Getting the saved embedding from LAST training\n",
        "saved_embedding = np.genfromtxt(fname='./BERT/values_03242023a.tsv', delimiter='\\t')\n",
        "print(f'saved_embedding.shape : {saved_embedding.shape} type(saved_embedding) : {type(saved_embedding)}')\n",
        "# Loading the model embedding with the pretrained weight we just got above\n",
        "model.embeddings.weight.data.copy_(torch.from_numpy(saved_embedding))\n",
        "print(f'model.embeddings.weight.shape - {model.embeddings.weight.shape}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "96m8G9j9frcp",
        "outputId": "9ab8aa81-e2c5-499a-e6d5-7387d81ee428"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "saved_embedding.shape : (23948, 128) type(saved_embedding) : <class 'numpy.ndarray'>\n",
            "model.embeddings.weight.shape - torch.Size([23948, 128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating test_batch and test_input and test_target with just 1 sentence\n",
        "test_batch_iter = iter(test_data_loader)\n",
        "test_batch, test_batch_iter = get_batch(test_data_loader, test_batch_iter)\n",
        "print(f\"len(test_batch['input']) : {len(test_batch['input'])} len(test_batch['target']) : {len(test_batch['target'])}\")\n",
        "test_input = test_batch['input'][0].unsqueeze(0)\n",
        "test_target = test_batch['target'][0].unsqueeze(0)\n",
        "test_input = test_input.cuda(non_blocking=True)\n",
        "test_target = test_target.cuda(non_blocking=True)\n",
        "print(test_input.shape, test_target.shape, test_input)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "do_q4ayyNMjR",
        "outputId": "dc3c7c68-248c-4646-a5ae-82b8755afdf8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "len(test_batch['input']) : 2 len(test_batch['target']) : 2\n",
            "torch.Size([1, 20]) torch.Size([1, 20]) tensor([[   37, 10934,   114,     5,   107,    19,  4542, 23946,    14,    13,\n",
            "            29, 23946, 23946,     0,    45,    36,   887,    75,     6,  3263]],\n",
            "       device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prediction\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "      test_out = model(test_input)\n",
        "      test_out_f = test_out.argmax(dim=-1)\n",
        "print(test_out.shape, test_out_f.shape)\n",
        "print(test_input)\n",
        "print(test_out_f)"
      ],
      "metadata": {
        "id": "QGN-A7NUOGqS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd3ea23c-54a9-4018-9165-d22c1682dc97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 20, 23948]) torch.Size([1, 20])\n",
            "tensor([[   37, 10934,   114,     5,   107,    19,  4542, 23946,    14,    13,\n",
            "            29, 23946, 23946,     0,    45,    36,   887,    75,     6,  3263]],\n",
            "       device='cuda:0')\n",
            "tensor([[ 9884,  1912, 17347,  1885, 17609, 16511, 22379, 18563, 13211, 21702,\n",
            "         19497, 22380,  8627, 17609,  7169, 18382,  7146, 15389, 17347,  6336]],\n",
            "       device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Using test_dataset.rvocab for decoding the output\n",
        "test_input_2 = test_input.squeeze(0).tolist()\n",
        "test_output  = test_out_f.squeeze(0).tolist()\n",
        "s_in = []\n",
        "for elem in test_input_2:\n",
        "   s_in.append(test_dataset.rvocab[elem])\n",
        "print(s_in)\n",
        "s_out = []\n",
        "for elem in test_output:\n",
        "   s_out.append(test_dataset.rvocab[elem])\n",
        "print(s_out)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9hAXKe6UON9E",
        "outputId": "7fd09b7a-147d-4180-81f0-98c04dbedbfd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['what', 'dimensions', 'can', 'i', 'give', 'for', 'text', '<oov>', '?', 'in', 'this', '<oov>', '<oov>', ',', 'we', 'will', 'learn', 'how', 'to', 'create']\n",
            "['accessary', 'aim', 'joyed', 'victory', 'capacities', 'cannibally', 'spangle', 'minotaurs', 'corporate', 'seymour', 'wheer', 'allots', 'pursuing', 'capacities', 'searching', 'fightest', 'bolts', '123', 'joyed', 'castles']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "U8o9a2E1OYSS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}