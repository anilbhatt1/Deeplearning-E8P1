{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jjWcv_ZOhpqK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bff508b6-8da0-4187-d337-860f2c4e2bdf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Mar 23 07:09:14 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   58C    P0    29W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4znsvw6i30zW",
        "outputId": "24fdc9fd-f8c6-4424-a48b-52cd2ff91fb6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r \"/content/gdrive/My Drive/EVA8_S11_Course_Docs/BERT\" \".\""
      ],
      "metadata": {
        "id": "__NCPl1T330S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "import torch.nn.functional as F\n",
        "from collections import Counter\n",
        "from os.path import exists\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "import math\n",
        "import re"
      ],
      "metadata": {
        "id": "wsz00AFnr2Kl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# Transformer\n",
        "# =============================================================================\n",
        "def attention(q, k, v, mask = None, dropout = None):\n",
        "    scores = q.matmul(k.transpose(-2, -1))\n",
        "    scores /= math.sqrt(q.shape[-1])\n",
        "\n",
        "    #mask\n",
        "    scores = scores if mask is None else scores.masked_fill(mask == 0, -1e3)\n",
        "\n",
        "    scores = F.softmax(scores, dim = -1)\n",
        "    scores = dropout(scores) if dropout is not None else scores\n",
        "    output = scores.matmul(v)\n",
        "    print(f' Attention output : {output.shape} v : {v.shape} scores : {scores.shape}')\n",
        "    return output"
      ],
      "metadata": {
        "id": "gzxVl7g-hw1v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, n_heads, out_dim, dropout=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "#        self.q_linear = nn.Linear(out_dim, out_dim)\n",
        "#        self.k_linear = nn.Linear(out_dim, out_dim)\n",
        "#        self.v_linear = nn.Linear(out_dim, out_dim)\n",
        "        self.linear = nn.Linear(out_dim, out_dim*3)\n",
        "\n",
        "        self.n_heads = n_heads\n",
        "        self.out_dim = out_dim\n",
        "        self.out_dim_per_head = out_dim // n_heads\n",
        "        self.out = nn.Linear(out_dim, out_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def split_heads(self, t):\n",
        "        print(f'MHA : split_heads t.shape {t.shape} self.n_heads {self.n_heads} self.out_dim_per_head {self.out_dim_per_head}')\n",
        "        return t.reshape(t.shape[0], -1, self.n_heads, self.out_dim_per_head)\n",
        "\n",
        "    def forward(self, x, y=None, mask=None):\n",
        "        #in decoder, y comes from encoder. In encoder, y=x\n",
        "        y = x if y is None else y\n",
        "\n",
        "        qkv = self.linear(x) # BS * SEQ_LEN * (3*EMBED_SIZE_L)\n",
        "        print(f'MHA : x.shape {x.shape} , qkv.shape {qkv.shape}')\n",
        "        q = qkv[:, :, :self.out_dim] # BS * SEQ_LEN * EMBED_SIZE_L\n",
        "        print(f'MHA : q.shape {q.shape} , self.out_dim {self.out_dim}')\n",
        "        k = qkv[:, :, self.out_dim:self.out_dim*2] # BS * SEQ_LEN * EMBED_SIZE_L\n",
        "        print(f'MHA : k.shape {k.shape} , self.out_dim*2 {self.out_dim*2}')\n",
        "        v = qkv[:, :, self.out_dim*2:] # BS * SEQ_LEN * EMBED_SIZE_L\n",
        "        print(f'MHA : v.shape {v.shape} ')\n",
        "\n",
        "        #break into n_heads\n",
        "        q, k, v = [self.split_heads(t) for t in (q,k,v)]  # BS * SEQ_LEN * HEAD * EMBED_SIZE_P_HEAD\n",
        "        print(f'MHA : After split_heads q.shape {q.shape} k.shape {k.shape} v.shape {v.shape}')\n",
        "        q, k, v = [t.transpose(1,2) for t in (q,k,v)]  # BS * HEAD * SEQ_LEN * EMBED_SIZE_P_HEAD\n",
        "        print(f'MHA : After transpose(1,2) q.shape {q.shape} k.shape {k.shape} v.shape {v.shape}')\n",
        "\n",
        "        #n_heads => attention => merge the heads => mix information\n",
        "        scores = attention(q, k, v, mask, self.dropout) # BS * HEAD * SEQ_LEN * EMBED_SIZE_P_HEAD\n",
        "        print(f'MHA : scores after attention {scores.shape}')\n",
        "        scores = scores.transpose(1,2).contiguous().view(scores.shape[0], -1, self.out_dim) # BS * SEQ_LEN * EMBED_SIZE_L\n",
        "        print(f'MHA : scores after view & transpose {scores.shape}')\n",
        "        out = self.out(scores)  # BS * SEQ_LEN * EMBED_SIZE\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "iHP1PrUphzxq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, inp_dim, inner_dim, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.linear1 = nn.Linear(inp_dim, inner_dim)\n",
        "        self.linear2 = nn.Linear(inner_dim, inp_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        print(f'FeedForward inp_dim inner_dim : {inp_dim} {inner_dim}')\n",
        "\n",
        "    def forward(self, x):\n",
        "        #inp => inner => relu => dropout => inner => inp\n",
        "        print(f'FeedForward before changes x.shape {x.shape}')\n",
        "        return self.linear2(self.dropout(F.relu(self.linear1(x))))"
      ],
      "metadata": {
        "id": "vyrQCVqEh221"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, n_heads, inner_transformer_size, inner_ff_size, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.mha = MultiHeadAttention(n_heads, inner_transformer_size, dropout)\n",
        "        self.ff = FeedForward(inner_transformer_size, inner_ff_size, dropout)\n",
        "        self.norm1 = nn.LayerNorm(inner_transformer_size)\n",
        "        self.norm2 = nn.LayerNorm(inner_transformer_size)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        x2 = self.norm1(x)\n",
        "        print(f'Encoder Layer x2.shape after norm1 {x2.shape} x.shape {x.shape}')\n",
        "        x = x + self.dropout1(self.mha(x2, mask=mask))\n",
        "        print(f'Encoder Layer x.shape after mha {x.shape} mask {mask}')\n",
        "        x2 = self.norm2(x)\n",
        "        print(f'Encoder Layer x2.shape after norm2 {x2.shape} x.shape {x.shape}')\n",
        "        x = x + self.dropout2(self.ff(x2))\n",
        "        print(f'Encoder Layer x.shape after ff {x.shape} ')\n",
        "        return x"
      ],
      "metadata": {
        "id": "RRogQtMGh5eN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, n_code, n_heads, embed_size, inner_ff_size, n_embeddings, seq_len, dropout=.1):\n",
        "        super().__init__()\n",
        "\n",
        "        #model input\n",
        "        self.embeddings = nn.Embedding(n_embeddings, embed_size)\n",
        "        self.pe = PositionalEmbedding(embed_size, seq_len)\n",
        "\n",
        "        #backbone\n",
        "        encoders = []\n",
        "        for i in range(n_code):\n",
        "            encoders += [EncoderLayer(n_heads, embed_size, inner_ff_size, dropout)]\n",
        "        self.encoders = nn.ModuleList(encoders)\n",
        "\n",
        "        #language model\n",
        "        self.norm = nn.LayerNorm(embed_size)\n",
        "        self.linear = nn.Linear(embed_size, n_embeddings, bias=False)\n",
        "        print(f'Transformer-init n_code {n_code} n_heads {n_heads} embed_size {embed_size} inner_ff_size {inner_ff_size} \\\n",
        "                n_embeddings {n_embeddings} seq_len {seq_len}')\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        print(f'Transformer x.shape {x.shape}')\n",
        "        x = self.embeddings(x)\n",
        "        print(f'Transformer after self.embeddings x.shape {x.shape}')\n",
        "        x = x + self.pe(x)\n",
        "        print(f'Transformer after pe addition x.shape {x.shape} pe.shape {self.pe(x).shape}')\n",
        "        for encoder in self.encoders:\n",
        "            x = encoder(x)\n",
        "        x = self.norm(x)\n",
        "        x = self.linear(x)\n",
        "        print(f'Transformer after self.linear x.shape {x.shape}')\n",
        "        return x"
      ],
      "metadata": {
        "id": "qyCObtpBh7Zk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Positional Embedding\n",
        "class PositionalEmbedding(nn.Module):\n",
        "    def __init__(self, d_model, max_seq_len = 80):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        pe = torch.zeros(max_seq_len, d_model)\n",
        "        pe.requires_grad = False\n",
        "        for pos in range(max_seq_len):\n",
        "            for i in range(0, d_model, 2):\n",
        "                pe[pos, i] = math.sin(pos / (10000 ** ((2 * i)/d_model)))\n",
        "                pe[pos, i + 1] = math.cos(pos / (10000 ** ((2 * (i + 1))/d_model)))\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        print(f'PositionalEmbedding self.pe.shape {self.pe.shape} x.shape {x.shape}')\n",
        "        return self.pe[:,:x.size(1)] #x.size(1) = seq_len"
      ],
      "metadata": {
        "id": "YyEm4aeeh_Ux"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# Dataset\n",
        "# =============================================================================\n",
        "class SentencesDataset(Dataset):\n",
        "    #Init dataset\n",
        "    def __init__(self, sentences, vocab, seq_len):\n",
        "        dataset = self\n",
        "\n",
        "        dataset.sentences = sentences\n",
        "        dataset.vocab = vocab + ['<ignore>', '<oov>', '<mask>']\n",
        "        dataset.vocab = {e:i for i, e in enumerate(dataset.vocab)}\n",
        "        dataset.rvocab = {v:k for k,v in dataset.vocab.items()}\n",
        "        dataset.seq_len = seq_len\n",
        "\n",
        "        #special tags\n",
        "        dataset.IGNORE_IDX = dataset.vocab['<ignore>'] #replacement tag for tokens to ignore\n",
        "        dataset.OUT_OF_VOCAB_IDX = dataset.vocab['<oov>'] #replacement tag for unknown words\n",
        "        dataset.MASK_IDX = dataset.vocab['<mask>'] #replacement tag for the masked word prediction task\n",
        "        print(f'len(sentences) len(vocab) seq_len : {len(sentences)} {len(vocab)} {seq_len}')\n",
        "        print(f'dataset.IGNORE_IDX {dataset.IGNORE_IDX} dataset.OUT_OF_VOCAB_IDX {dataset.OUT_OF_VOCAB_IDX} ')\n",
        "        print(f'dataset.MASK_IDX {dataset.MASK_IDX}')\n",
        "\n",
        "\n",
        "    #fetch data\n",
        "    def __getitem__(self, index, p_random_mask=0.15):\n",
        "        print(f'index : {index}')\n",
        "        dataset = self\n",
        "\n",
        "        #while we don't have enough word to fill the sentence for a batch\n",
        "        s = []\n",
        "        while len(s) < dataset.seq_len:\n",
        "            print(f' getitem len(s) < dataset.seq_len: {len(s)} dataset.seq_len {dataset.seq_len}')\n",
        "            s.extend(dataset.get_sentence_idx(index % len(dataset)))\n",
        "            print(f' getitem index % len(dataset) : {index % len(dataset)} - {len(dataset)}')\n",
        "            print(f' getitem dataset.get_sentence_idx(index % len(dataset)) : {dataset.get_sentence_idx(index % len(dataset))}')\n",
        "            print(f' getitem s value after extension : {s}')\n",
        "            index += 1\n",
        "        print(f' getitem after len(s) < dataset.seq_len : {len(s)} - {s}')\n",
        "\n",
        "        #ensure that the sequence is of length seq_len\n",
        "        s = s[:dataset.seq_len]\n",
        "        print(f' getitem s after checking seq_len : {len(s)} - {s}')\n",
        "        [s.append(dataset.IGNORE_IDX) for i in range(dataset.seq_len - len(s))] #PAD ok\n",
        "        print(f' getitem s after IGNORE_IDX : {len(s)} - {s}')\n",
        "\n",
        "        #apply random mask\n",
        "        rand_rand = random.random()\n",
        "        s = [(dataset.MASK_IDX, w) if rand_rand < p_random_mask else (w, dataset.IGNORE_IDX) for w in s]\n",
        "        print(f' getitem rand_rand {rand_rand} p_random_mask {p_random_mask}')\n",
        "        print(f' getitem s after apply random mask : {len(s)} - {s}')\n",
        "\n",
        "        data_dict = {'input': torch.Tensor([w[0] for w in s]).long(),\n",
        "                     'target': torch.Tensor([w[1] for w in s]).long()}\n",
        "\n",
        "        print(f' getitem data_dict : {data_dict}')\n",
        "\n",
        "        return {'input': torch.Tensor([w[0] for w in s]).long(),\n",
        "                'target': torch.Tensor([w[1] for w in s]).long()}\n",
        "\n",
        "    #return length\n",
        "    def __len__(self):\n",
        "        return len(self.sentences)\n",
        "\n",
        "    #get words id\n",
        "    def get_sentence_idx(self, index):\n",
        "        dataset = self\n",
        "        s = dataset.sentences[index]\n",
        "        print(f'get_sentence_idx s gathering index - {index} : {len(s)} - {s}')\n",
        "        s = [dataset.vocab[w] if w in dataset.vocab else dataset.OUT_OF_VOCAB_IDX for w in s]\n",
        "        print(f'get_sentence_idx s after vocab lookup: {len(s)} - {s}')\n",
        "        return s"
      ],
      "metadata": {
        "id": "lXwOPxwTiBW3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# Methods / Class\n",
        "# =============================================================================\n",
        "def get_batch(loader, loader_iter):\n",
        "    try:\n",
        "        batch = next(loader_iter)\n",
        "    except StopIteration:\n",
        "        loader_iter = iter(loader)\n",
        "        batch = next(loader_iter)\n",
        "    return batch, loader_iter"
      ],
      "metadata": {
        "id": "py05esgkiHY1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# #Init\n",
        "# =============================================================================\n",
        "print('initializing..')\n",
        "batch_size = 2\n",
        "seq_len = 20\n",
        "embed_size = 128\n",
        "inner_ff_size = embed_size * 4\n",
        "n_heads = 8\n",
        "n_code = 8\n",
        "n_vocab = 40000\n",
        "dropout = 0.1\n",
        "# n_workers = 12\n",
        "\n",
        "#optimizer\n",
        "optim_kwargs = {'lr':1e-4, 'weight_decay':1e-4, 'betas':(.9,.999)}"
      ],
      "metadata": {
        "id": "awTIvUtEiJzF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af34fc65-1bf6-4201-a899-a36a88bf28f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "initializing..\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# Input\n",
        "# =============================================================================\n",
        "#1) load text\n",
        "print('loading text...')\n",
        "# pth = './BERT/training.txt'\n",
        "pth = './BERT/small_training.txt'\n",
        "sentences = open(pth).read().lower().split('\\n')\n",
        "print(f'sentences length - {len(sentences)}, {sentences[:2]}')\n",
        "\n",
        "#2) tokenize sentences (can be done during training, you can also use spacy udpipe)\n",
        "print('tokenizing sentences...')\n",
        "special_chars = ',?;.:/*!+-()[]{}\"\\'&'\n",
        "sentences = [re.sub(f'[{re.escape(special_chars)}]', ' \\g<0> ', s).split(' ') for s in sentences]\n",
        "print(f'sentences length - {len(sentences)}, {sentences[:2]}')\n",
        "sentences = [[w for w in s if len(w)] for s in sentences]\n",
        "print(f'sentences length - {len(sentences)}, {len(sentences[0])} {len(sentences[-1])} {sentences[:2]}')"
      ],
      "metadata": {
        "id": "OHyOHDPIiMfA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7e2f537-3213-4d7f-cd54-acb6149983b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading text...\n",
            "sentences length - 6, ['i would like to print my bert model summary (text classification). ', 'i know that for image classification we use summary.']\n",
            "tokenizing sentences...\n",
            "sentences length - 6, [['i', 'would', 'like', 'to', 'print', 'my', 'bert', 'model', 'summary', '', '(', 'text', 'classification', ')', '', '.', '', ''], ['i', 'know', 'that', 'for', 'image', 'classification', 'we', 'use', 'summary', '.', '']]\n",
            "sentences length - 6, 14 0 [['i', 'would', 'like', 'to', 'print', 'my', 'bert', 'model', 'summary', '(', 'text', 'classification', ')', '.'], ['i', 'know', 'that', 'for', 'image', 'classification', 'we', 'use', 'summary', '.']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for s in sentences:\n",
        "  print(len(s), s)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A3v_y2Nyzp6e",
        "outputId": "3bc7333f-add1-402d-9f0e-fd4a12dcd94a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "14 ['i', 'would', 'like', 'to', 'print', 'my', 'bert', 'model', 'summary', '(', 'text', 'classification', ')', '.']\n",
            "10 ['i', 'know', 'that', 'for', 'image', 'classification', 'we', 'use', 'summary', '.']\n",
            "9 ['what', 'dimensions', 'can', 'i', 'give', 'for', 'text', 'bert', '?']\n",
            "31 ['in', 'this', 'python', 'tutorial', ',', 'we', 'will', 'learn', 'how', 'to', 'create', 'a', 'pytorch', 'model', 'summary', 'in', 'python', 'and', 'we', 'will', 'also', 'cover', 'different', 'examples', 'related', 'to', 'the', 'pytorch', 'model', 'summary', '.']\n",
            "8 ['moreover', ',', 'we', 'will', 'cover', 'these', 'topics', '.']\n",
            "0 []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#3) create vocab if not already created\n",
        "print('creating/loading vocab...')\n",
        "pth = './BERT/vocab.txt'\n",
        "if not exists(pth):\n",
        "    words = [w for s in sentences for w in s]\n",
        "    vocab = Counter(words).most_common(n_vocab) #keep the N most frequent words\n",
        "    vocab = [w[0] for w in vocab]\n",
        "    open(pth, 'w+').write('\\n'.join(vocab))\n",
        "else:\n",
        "    vocab = open(pth).read().split('\\n')\n",
        "print(f'vocab : {len(vocab)} {vocab[:5]}')"
      ],
      "metadata": {
        "id": "U_eG2ZHjiPEL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95457aaf-8cfd-459f-d087-62a51872630b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "creating/loading vocab...\n",
            "vocab : 23945 [',', '.', \"'\", 'the', 'and']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#4) create dataset\n",
        "print('creating dataset...')\n",
        "dataset = SentencesDataset(sentences, vocab, seq_len)\n",
        "# kwargs = {'num_workers':n_workers, 'shuffle':True,  'drop_last':True, 'pin_memory':True, 'batch_size':batch_size}\n",
        "kwargs = {'shuffle':True,  'drop_last':True, 'pin_memory':True, 'batch_size':batch_size}\n",
        "data_loader = torch.utils.data.DataLoader(dataset, **kwargs)\n",
        "print(f'kwargs : {kwargs}')"
      ],
      "metadata": {
        "id": "19sQnM6_iQ5b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa912077-69be-4c67-84f4-c9299d0e2fba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "creating dataset...\n",
            "len(sentences) len(vocab) seq_len : 6 23945 20\n",
            "dataset.IGNORE_IDX 23945 dataset.OUT_OF_VOCAB_IDX 23946 \n",
            "dataset.MASK_IDX 23947\n",
            "kwargs : {'shuffle': True, 'drop_last': True, 'pin_memory': True, 'batch_size': 2}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# Model\n",
        "# =============================================================================\n",
        "#init model\n",
        "print('initializing model...')\n",
        "model = Transformer(n_code, n_heads, embed_size, inner_ff_size, len(dataset.vocab), seq_len, dropout)\n",
        "model = model.cuda()"
      ],
      "metadata": {
        "id": "rMnVrzQ1iSlj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a98484a-5b6e-4e3f-8724-5c3e95062316"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "initializing model...\n",
            "FeedForward inp_dim inner_dim : 128 512\n",
            "FeedForward inp_dim inner_dim : 128 512\n",
            "FeedForward inp_dim inner_dim : 128 512\n",
            "FeedForward inp_dim inner_dim : 128 512\n",
            "FeedForward inp_dim inner_dim : 128 512\n",
            "FeedForward inp_dim inner_dim : 128 512\n",
            "FeedForward inp_dim inner_dim : 128 512\n",
            "FeedForward inp_dim inner_dim : 128 512\n",
            "Transformer-init n_code 8 n_heads 8 embed_size 128 inner_ff_size 512                 n_embeddings 23948 seq_len 20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "print(\"device:\", DEVICE)\n",
        "model.to(DEVICE)"
      ],
      "metadata": {
        "id": "kJnxYcsJ80yH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64943ccb-ac5c-409b-b803-30d15c0cfecc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "device: cuda\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Transformer(\n",
              "  (embeddings): Embedding(23948, 128)\n",
              "  (pe): PositionalEmbedding()\n",
              "  (encoders): ModuleList(\n",
              "    (0): EncoderLayer(\n",
              "      (mha): MultiHeadAttention(\n",
              "        (linear): Linear(in_features=128, out_features=384, bias=True)\n",
              "        (out): Linear(in_features=128, out_features=128, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
              "        (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "      (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout1): Dropout(p=0.1, inplace=False)\n",
              "      (dropout2): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (1): EncoderLayer(\n",
              "      (mha): MultiHeadAttention(\n",
              "        (linear): Linear(in_features=128, out_features=384, bias=True)\n",
              "        (out): Linear(in_features=128, out_features=128, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
              "        (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "      (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout1): Dropout(p=0.1, inplace=False)\n",
              "      (dropout2): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (2): EncoderLayer(\n",
              "      (mha): MultiHeadAttention(\n",
              "        (linear): Linear(in_features=128, out_features=384, bias=True)\n",
              "        (out): Linear(in_features=128, out_features=128, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
              "        (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "      (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout1): Dropout(p=0.1, inplace=False)\n",
              "      (dropout2): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (3): EncoderLayer(\n",
              "      (mha): MultiHeadAttention(\n",
              "        (linear): Linear(in_features=128, out_features=384, bias=True)\n",
              "        (out): Linear(in_features=128, out_features=128, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
              "        (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "      (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout1): Dropout(p=0.1, inplace=False)\n",
              "      (dropout2): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (4): EncoderLayer(\n",
              "      (mha): MultiHeadAttention(\n",
              "        (linear): Linear(in_features=128, out_features=384, bias=True)\n",
              "        (out): Linear(in_features=128, out_features=128, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
              "        (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "      (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout1): Dropout(p=0.1, inplace=False)\n",
              "      (dropout2): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (5): EncoderLayer(\n",
              "      (mha): MultiHeadAttention(\n",
              "        (linear): Linear(in_features=128, out_features=384, bias=True)\n",
              "        (out): Linear(in_features=128, out_features=128, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
              "        (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "      (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout1): Dropout(p=0.1, inplace=False)\n",
              "      (dropout2): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (6): EncoderLayer(\n",
              "      (mha): MultiHeadAttention(\n",
              "        (linear): Linear(in_features=128, out_features=384, bias=True)\n",
              "        (out): Linear(in_features=128, out_features=128, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
              "        (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "      (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout1): Dropout(p=0.1, inplace=False)\n",
              "      (dropout2): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (7): EncoderLayer(\n",
              "      (mha): MultiHeadAttention(\n",
              "        (linear): Linear(in_features=128, out_features=384, bias=True)\n",
              "        (out): Linear(in_features=128, out_features=128, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
              "        (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "      (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout1): Dropout(p=0.1, inplace=False)\n",
              "      (dropout2): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "  )\n",
              "  (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "  (linear): Linear(in_features=128, out_features=23948, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# Optimizer\n",
        "# =============================================================================\n",
        "print('initializing optimizer and loss...')\n",
        "optimizer = optim.Adam(model.parameters(), **optim_kwargs)\n",
        "loss_model = nn.CrossEntropyLoss(ignore_index=dataset.IGNORE_IDX)"
      ],
      "metadata": {
        "id": "3X4pzcgHiUJ0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20906767-7504-4163-dda5-2ee8946ee47e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "initializing optimizer and loss...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# Train\n",
        "# =============================================================================\n",
        "print('training...')\n",
        "print_each = 10\n",
        "model.train()\n",
        "batch_iter = iter(data_loader)\n",
        "n_iteration = 2\n",
        "for it in range(n_iteration):\n",
        "    print(f'iteration - it : {it}')\n",
        "    #get batch\n",
        "    batch, batch_iter = get_batch(data_loader, batch_iter)\n",
        "\n",
        "    #infer\n",
        "    masked_input = batch['input']\n",
        "    masked_target = batch['target']\n",
        "\n",
        "    print(f' masked_input : {len(masked_input)} - {masked_input}')\n",
        "    print(f' masked_target : {len(masked_target)} - {masked_target}')\n",
        "\n",
        "    masked_input = masked_input.cuda(non_blocking=True)\n",
        "    masked_target = masked_target.cuda(non_blocking=True)\n",
        "    output = model(masked_input)\n",
        "    print(f' output : {output.shape}')\n",
        "    #compute the cross entropy loss\n",
        "    output_v = output.view(-1,output.shape[-1])\n",
        "    print(f' output_v : {output_v.shape}')\n",
        "    target_v = masked_target.view(-1,1).squeeze()\n",
        "    print(f' target_v : {target_v.shape}')\n",
        "    loss = loss_model(output_v, target_v)\n",
        "    print(f' loss : {type(loss)} {loss.size()}')\n",
        "    if it == 1:\n",
        "      print(f'loss : {loss}')\n",
        "    #compute gradients\n",
        "    loss.backward()\n",
        "\n",
        "    #apply gradients\n",
        "    optimizer.step()\n",
        "\n",
        "    #print step\n",
        "    if it % print_each == 0:\n",
        "        print('it:', it,\n",
        "              ' | loss', np.round(loss.item(),2),\n",
        "              ' | Î”w:', round(model.embeddings.weight.grad.abs().sum().item(),3))\n",
        "\n",
        "    #reset gradients\n",
        "    optimizer.zero_grad()\n",
        ""
      ],
      "metadata": {
        "id": "I4qNQlOPiVl-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "516afed1-a353-476e-aac6-7ee86e5a0e97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training...\n",
            "iteration - it : 0\n",
            "index : 3\n",
            " getitem len(s) < dataset.seq_len: 0 dataset.seq_len 20\n",
            "get_sentence_idx s gathering index - 3 : 31 - ['in', 'this', 'python', 'tutorial', ',', 'we', 'will', 'learn', 'how', 'to', 'create', 'a', 'pytorch', 'model', 'summary', 'in', 'python', 'and', 'we', 'will', 'also', 'cover', 'different', 'examples', 'related', 'to', 'the', 'pytorch', 'model', 'summary', '.']\n",
            "get_sentence_idx s after vocab lookup: 31 - [13, 29, 23946, 23946, 0, 45, 36, 887, 75, 6, 3263, 9, 23946, 4731, 13220, 13, 23946, 4, 45, 36, 2061, 2034, 4783, 6696, 23946, 6, 3, 23946, 4731, 13220, 1]\n",
            " getitem index % len(dataset) : 3 - 6\n",
            "get_sentence_idx s gathering index - 3 : 31 - ['in', 'this', 'python', 'tutorial', ',', 'we', 'will', 'learn', 'how', 'to', 'create', 'a', 'pytorch', 'model', 'summary', 'in', 'python', 'and', 'we', 'will', 'also', 'cover', 'different', 'examples', 'related', 'to', 'the', 'pytorch', 'model', 'summary', '.']\n",
            "get_sentence_idx s after vocab lookup: 31 - [13, 29, 23946, 23946, 0, 45, 36, 887, 75, 6, 3263, 9, 23946, 4731, 13220, 13, 23946, 4, 45, 36, 2061, 2034, 4783, 6696, 23946, 6, 3, 23946, 4731, 13220, 1]\n",
            " getitem dataset.get_sentence_idx(index % len(dataset)) : [13, 29, 23946, 23946, 0, 45, 36, 887, 75, 6, 3263, 9, 23946, 4731, 13220, 13, 23946, 4, 45, 36, 2061, 2034, 4783, 6696, 23946, 6, 3, 23946, 4731, 13220, 1]\n",
            " getitem s value after extension : [13, 29, 23946, 23946, 0, 45, 36, 887, 75, 6, 3263, 9, 23946, 4731, 13220, 13, 23946, 4, 45, 36, 2061, 2034, 4783, 6696, 23946, 6, 3, 23946, 4731, 13220, 1]\n",
            " getitem after len(s) < dataset.seq_len : 31 - [13, 29, 23946, 23946, 0, 45, 36, 887, 75, 6, 3263, 9, 23946, 4731, 13220, 13, 23946, 4, 45, 36, 2061, 2034, 4783, 6696, 23946, 6, 3, 23946, 4731, 13220, 1]\n",
            " getitem s after checking seq_len : 20 - [13, 29, 23946, 23946, 0, 45, 36, 887, 75, 6, 3263, 9, 23946, 4731, 13220, 13, 23946, 4, 45, 36]\n",
            " getitem s after IGNORE_IDX : 20 - [13, 29, 23946, 23946, 0, 45, 36, 887, 75, 6, 3263, 9, 23946, 4731, 13220, 13, 23946, 4, 45, 36]\n",
            " getitem rand_rand 0.9456751128079983 p_random_mask 0.15\n",
            " getitem s after apply random mask : 20 - [(13, 23945), (29, 23945), (23946, 23945), (23946, 23945), (0, 23945), (45, 23945), (36, 23945), (887, 23945), (75, 23945), (6, 23945), (3263, 23945), (9, 23945), (23946, 23945), (4731, 23945), (13220, 23945), (13, 23945), (23946, 23945), (4, 23945), (45, 23945), (36, 23945)]\n",
            " getitem data_dict : {'input': tensor([   13,    29, 23946, 23946,     0,    45,    36,   887,    75,     6,\n",
            "         3263,     9, 23946,  4731, 13220,    13, 23946,     4,    45,    36]), 'target': tensor([23945, 23945, 23945, 23945, 23945, 23945, 23945, 23945, 23945, 23945,\n",
            "        23945, 23945, 23945, 23945, 23945, 23945, 23945, 23945, 23945, 23945])}\n",
            "index : 0\n",
            " getitem len(s) < dataset.seq_len: 0 dataset.seq_len 20\n",
            "get_sentence_idx s gathering index - 0 : 14 - ['i', 'would', 'like', 'to', 'print', 'my', 'bert', 'model', 'summary', '(', 'text', 'classification', ')', '.']\n",
            "get_sentence_idx s after vocab lookup: 14 - [5, 68, 89, 6, 3234, 11, 23946, 4731, 13220, 190, 4542, 23946, 189, 1]\n",
            " getitem index % len(dataset) : 0 - 6\n",
            "get_sentence_idx s gathering index - 0 : 14 - ['i', 'would', 'like', 'to', 'print', 'my', 'bert', 'model', 'summary', '(', 'text', 'classification', ')', '.']\n",
            "get_sentence_idx s after vocab lookup: 14 - [5, 68, 89, 6, 3234, 11, 23946, 4731, 13220, 190, 4542, 23946, 189, 1]\n",
            " getitem dataset.get_sentence_idx(index % len(dataset)) : [5, 68, 89, 6, 3234, 11, 23946, 4731, 13220, 190, 4542, 23946, 189, 1]\n",
            " getitem s value after extension : [5, 68, 89, 6, 3234, 11, 23946, 4731, 13220, 190, 4542, 23946, 189, 1]\n",
            " getitem len(s) < dataset.seq_len: 14 dataset.seq_len 20\n",
            "get_sentence_idx s gathering index - 1 : 10 - ['i', 'know', 'that', 'for', 'image', 'classification', 'we', 'use', 'summary', '.']\n",
            "get_sentence_idx s after vocab lookup: 10 - [5, 93, 12, 19, 1898, 23946, 45, 217, 13220, 1]\n",
            " getitem index % len(dataset) : 1 - 6\n",
            "get_sentence_idx s gathering index - 1 : 10 - ['i', 'know', 'that', 'for', 'image', 'classification', 'we', 'use', 'summary', '.']\n",
            "get_sentence_idx s after vocab lookup: 10 - [5, 93, 12, 19, 1898, 23946, 45, 217, 13220, 1]\n",
            " getitem dataset.get_sentence_idx(index % len(dataset)) : [5, 93, 12, 19, 1898, 23946, 45, 217, 13220, 1]\n",
            " getitem s value after extension : [5, 68, 89, 6, 3234, 11, 23946, 4731, 13220, 190, 4542, 23946, 189, 1, 5, 93, 12, 19, 1898, 23946, 45, 217, 13220, 1]\n",
            " getitem after len(s) < dataset.seq_len : 24 - [5, 68, 89, 6, 3234, 11, 23946, 4731, 13220, 190, 4542, 23946, 189, 1, 5, 93, 12, 19, 1898, 23946, 45, 217, 13220, 1]\n",
            " getitem s after checking seq_len : 20 - [5, 68, 89, 6, 3234, 11, 23946, 4731, 13220, 190, 4542, 23946, 189, 1, 5, 93, 12, 19, 1898, 23946]\n",
            " getitem s after IGNORE_IDX : 20 - [5, 68, 89, 6, 3234, 11, 23946, 4731, 13220, 190, 4542, 23946, 189, 1, 5, 93, 12, 19, 1898, 23946]\n",
            " getitem rand_rand 0.7231757417569664 p_random_mask 0.15\n",
            " getitem s after apply random mask : 20 - [(5, 23945), (68, 23945), (89, 23945), (6, 23945), (3234, 23945), (11, 23945), (23946, 23945), (4731, 23945), (13220, 23945), (190, 23945), (4542, 23945), (23946, 23945), (189, 23945), (1, 23945), (5, 23945), (93, 23945), (12, 23945), (19, 23945), (1898, 23945), (23946, 23945)]\n",
            " getitem data_dict : {'input': tensor([    5,    68,    89,     6,  3234,    11, 23946,  4731, 13220,   190,\n",
            "         4542, 23946,   189,     1,     5,    93,    12,    19,  1898, 23946]), 'target': tensor([23945, 23945, 23945, 23945, 23945, 23945, 23945, 23945, 23945, 23945,\n",
            "        23945, 23945, 23945, 23945, 23945, 23945, 23945, 23945, 23945, 23945])}\n",
            " masked_input : 2 - tensor([[   13,    29, 23946, 23946,     0,    45,    36,   887,    75,     6,\n",
            "          3263,     9, 23946,  4731, 13220,    13, 23946,     4,    45,    36],\n",
            "        [    5,    68,    89,     6,  3234,    11, 23946,  4731, 13220,   190,\n",
            "          4542, 23946,   189,     1,     5,    93,    12,    19,  1898, 23946]])\n",
            " masked_target : 2 - tensor([[23945, 23945, 23945, 23945, 23945, 23945, 23945, 23945, 23945, 23945,\n",
            "         23945, 23945, 23945, 23945, 23945, 23945, 23945, 23945, 23945, 23945],\n",
            "        [23945, 23945, 23945, 23945, 23945, 23945, 23945, 23945, 23945, 23945,\n",
            "         23945, 23945, 23945, 23945, 23945, 23945, 23945, 23945, 23945, 23945]])\n",
            "Transformer x.shape torch.Size([2, 20])\n",
            "Transformer after self.embeddings x.shape torch.Size([2, 20, 128])\n",
            "PositionalEmbedding self.pe.shape torch.Size([1, 20, 128]) x.shape torch.Size([2, 20, 128])\n",
            "PositionalEmbedding self.pe.shape torch.Size([1, 20, 128]) x.shape torch.Size([2, 20, 128])\n",
            "Transformer after pe addition x.shape torch.Size([2, 20, 128]) pe.shape torch.Size([1, 20, 128])\n",
            "Encoder Layer x2.shape after norm1 torch.Size([2, 20, 128]) x.shape torch.Size([2, 20, 128])\n",
            "MHA : x.shape torch.Size([2, 20, 128]) , qkv.shape torch.Size([2, 20, 384])\n",
            "MHA : q.shape torch.Size([2, 20, 128]) , self.out_dim 128\n",
            "MHA : k.shape torch.Size([2, 20, 128]) , self.out_dim*2 256\n",
            "MHA : v.shape torch.Size([2, 20, 128]) \n",
            "MHA : split_heads t.shape torch.Size([2, 20, 128]) self.n_heads 8 self.out_dim_per_head 16\n",
            "MHA : split_heads t.shape torch.Size([2, 20, 128]) self.n_heads 8 self.out_dim_per_head 16\n",
            "MHA : split_heads t.shape torch.Size([2, 20, 128]) self.n_heads 8 self.out_dim_per_head 16\n",
            "MHA : After split_heads q.shape torch.Size([2, 20, 8, 16]) k.shape torch.Size([2, 20, 8, 16]) v.shape torch.Size([2, 20, 8, 16])\n",
            "MHA : After transpose(1,2) q.shape torch.Size([2, 8, 20, 16]) k.shape torch.Size([2, 8, 20, 16]) v.shape torch.Size([2, 8, 20, 16])\n",
            " Attention output : torch.Size([2, 8, 20, 16]) v : torch.Size([2, 8, 20, 16]) scores : torch.Size([2, 8, 20, 20])\n",
            "MHA : scores after attention torch.Size([2, 8, 20, 16])\n",
            "MHA : scores after view & transpose torch.Size([2, 20, 128])\n",
            "Encoder Layer x.shape after mha torch.Size([2, 20, 128]) mask None\n",
            "Encoder Layer x2.shape after norm2 torch.Size([2, 20, 128]) x.shape torch.Size([2, 20, 128])\n",
            "FeedForward before changes x.shape torch.Size([2, 20, 128])\n",
            "Encoder Layer x.shape after ff torch.Size([2, 20, 128]) \n",
            "Encoder Layer x2.shape after norm1 torch.Size([2, 20, 128]) x.shape torch.Size([2, 20, 128])\n",
            "MHA : x.shape torch.Size([2, 20, 128]) , qkv.shape torch.Size([2, 20, 384])\n",
            "MHA : q.shape torch.Size([2, 20, 128]) , self.out_dim 128\n",
            "MHA : k.shape torch.Size([2, 20, 128]) , self.out_dim*2 256\n",
            "MHA : v.shape torch.Size([2, 20, 128]) \n",
            "MHA : split_heads t.shape torch.Size([2, 20, 128]) self.n_heads 8 self.out_dim_per_head 16\n",
            "MHA : split_heads t.shape torch.Size([2, 20, 128]) self.n_heads 8 self.out_dim_per_head 16\n",
            "MHA : split_heads t.shape torch.Size([2, 20, 128]) self.n_heads 8 self.out_dim_per_head 16\n",
            "MHA : After split_heads q.shape torch.Size([2, 20, 8, 16]) k.shape torch.Size([2, 20, 8, 16]) v.shape torch.Size([2, 20, 8, 16])\n",
            "MHA : After transpose(1,2) q.shape torch.Size([2, 8, 20, 16]) k.shape torch.Size([2, 8, 20, 16]) v.shape torch.Size([2, 8, 20, 16])\n",
            " Attention output : torch.Size([2, 8, 20, 16]) v : torch.Size([2, 8, 20, 16]) scores : torch.Size([2, 8, 20, 20])\n",
            "MHA : scores after attention torch.Size([2, 8, 20, 16])\n",
            "MHA : scores after view & transpose torch.Size([2, 20, 128])\n",
            "Encoder Layer x.shape after mha torch.Size([2, 20, 128]) mask None\n",
            "Encoder Layer x2.shape after norm2 torch.Size([2, 20, 128]) x.shape torch.Size([2, 20, 128])\n",
            "FeedForward before changes x.shape torch.Size([2, 20, 128])\n",
            "Encoder Layer x.shape after ff torch.Size([2, 20, 128]) \n",
            "Encoder Layer x2.shape after norm1 torch.Size([2, 20, 128]) x.shape torch.Size([2, 20, 128])\n",
            "MHA : x.shape torch.Size([2, 20, 128]) , qkv.shape torch.Size([2, 20, 384])\n",
            "MHA : q.shape torch.Size([2, 20, 128]) , self.out_dim 128\n",
            "MHA : k.shape torch.Size([2, 20, 128]) , self.out_dim*2 256\n",
            "MHA : v.shape torch.Size([2, 20, 128]) \n",
            "MHA : split_heads t.shape torch.Size([2, 20, 128]) self.n_heads 8 self.out_dim_per_head 16\n",
            "MHA : split_heads t.shape torch.Size([2, 20, 128]) self.n_heads 8 self.out_dim_per_head 16\n",
            "MHA : split_heads t.shape torch.Size([2, 20, 128]) self.n_heads 8 self.out_dim_per_head 16\n",
            "MHA : After split_heads q.shape torch.Size([2, 20, 8, 16]) k.shape torch.Size([2, 20, 8, 16]) v.shape torch.Size([2, 20, 8, 16])\n",
            "MHA : After transpose(1,2) q.shape torch.Size([2, 8, 20, 16]) k.shape torch.Size([2, 8, 20, 16]) v.shape torch.Size([2, 8, 20, 16])\n",
            " Attention output : torch.Size([2, 8, 20, 16]) v : torch.Size([2, 8, 20, 16]) scores : torch.Size([2, 8, 20, 20])\n",
            "MHA : scores after attention torch.Size([2, 8, 20, 16])\n",
            "MHA : scores after view & transpose torch.Size([2, 20, 128])\n",
            "Encoder Layer x.shape after mha torch.Size([2, 20, 128]) mask None\n",
            "Encoder Layer x2.shape after norm2 torch.Size([2, 20, 128]) x.shape torch.Size([2, 20, 128])\n",
            "FeedForward before changes x.shape torch.Size([2, 20, 128])\n",
            "Encoder Layer x.shape after ff torch.Size([2, 20, 128]) \n",
            "Encoder Layer x2.shape after norm1 torch.Size([2, 20, 128]) x.shape torch.Size([2, 20, 128])\n",
            "MHA : x.shape torch.Size([2, 20, 128]) , qkv.shape torch.Size([2, 20, 384])\n",
            "MHA : q.shape torch.Size([2, 20, 128]) , self.out_dim 128\n",
            "MHA : k.shape torch.Size([2, 20, 128]) , self.out_dim*2 256\n",
            "MHA : v.shape torch.Size([2, 20, 128]) \n",
            "MHA : split_heads t.shape torch.Size([2, 20, 128]) self.n_heads 8 self.out_dim_per_head 16\n",
            "MHA : split_heads t.shape torch.Size([2, 20, 128]) self.n_heads 8 self.out_dim_per_head 16\n",
            "MHA : split_heads t.shape torch.Size([2, 20, 128]) self.n_heads 8 self.out_dim_per_head 16\n",
            "MHA : After split_heads q.shape torch.Size([2, 20, 8, 16]) k.shape torch.Size([2, 20, 8, 16]) v.shape torch.Size([2, 20, 8, 16])\n",
            "MHA : After transpose(1,2) q.shape torch.Size([2, 8, 20, 16]) k.shape torch.Size([2, 8, 20, 16]) v.shape torch.Size([2, 8, 20, 16])\n",
            " Attention output : torch.Size([2, 8, 20, 16]) v : torch.Size([2, 8, 20, 16]) scores : torch.Size([2, 8, 20, 20])\n",
            "MHA : scores after attention torch.Size([2, 8, 20, 16])\n",
            "MHA : scores after view & transpose torch.Size([2, 20, 128])\n",
            "Encoder Layer x.shape after mha torch.Size([2, 20, 128]) mask None\n",
            "Encoder Layer x2.shape after norm2 torch.Size([2, 20, 128]) x.shape torch.Size([2, 20, 128])\n",
            "FeedForward before changes x.shape torch.Size([2, 20, 128])\n",
            "Encoder Layer x.shape after ff torch.Size([2, 20, 128]) \n",
            "Encoder Layer x2.shape after norm1 torch.Size([2, 20, 128]) x.shape torch.Size([2, 20, 128])\n",
            "MHA : x.shape torch.Size([2, 20, 128]) , qkv.shape torch.Size([2, 20, 384])\n",
            "MHA : q.shape torch.Size([2, 20, 128]) , self.out_dim 128\n",
            "MHA : k.shape torch.Size([2, 20, 128]) , self.out_dim*2 256\n",
            "MHA : v.shape torch.Size([2, 20, 128]) \n",
            "MHA : split_heads t.shape torch.Size([2, 20, 128]) self.n_heads 8 self.out_dim_per_head 16\n",
            "MHA : split_heads t.shape torch.Size([2, 20, 128]) self.n_heads 8 self.out_dim_per_head 16\n",
            "MHA : split_heads t.shape torch.Size([2, 20, 128]) self.n_heads 8 self.out_dim_per_head 16\n",
            "MHA : After split_heads q.shape torch.Size([2, 20, 8, 16]) k.shape torch.Size([2, 20, 8, 16]) v.shape torch.Size([2, 20, 8, 16])\n",
            "MHA : After transpose(1,2) q.shape torch.Size([2, 8, 20, 16]) k.shape torch.Size([2, 8, 20, 16]) v.shape torch.Size([2, 8, 20, 16])\n",
            " Attention output : torch.Size([2, 8, 20, 16]) v : torch.Size([2, 8, 20, 16]) scores : torch.Size([2, 8, 20, 20])\n",
            "MHA : scores after attention torch.Size([2, 8, 20, 16])\n",
            "MHA : scores after view & transpose torch.Size([2, 20, 128])\n",
            "Encoder Layer x.shape after mha torch.Size([2, 20, 128]) mask None\n",
            "Encoder Layer x2.shape after norm2 torch.Size([2, 20, 128]) x.shape torch.Size([2, 20, 128])\n",
            "FeedForward before changes x.shape torch.Size([2, 20, 128])\n",
            "Encoder Layer x.shape after ff torch.Size([2, 20, 128]) \n",
            "Encoder Layer x2.shape after norm1 torch.Size([2, 20, 128]) x.shape torch.Size([2, 20, 128])\n",
            "MHA : x.shape torch.Size([2, 20, 128]) , qkv.shape torch.Size([2, 20, 384])\n",
            "MHA : q.shape torch.Size([2, 20, 128]) , self.out_dim 128\n",
            "MHA : k.shape torch.Size([2, 20, 128]) , self.out_dim*2 256\n",
            "MHA : v.shape torch.Size([2, 20, 128]) \n",
            "MHA : split_heads t.shape torch.Size([2, 20, 128]) self.n_heads 8 self.out_dim_per_head 16\n",
            "MHA : split_heads t.shape torch.Size([2, 20, 128]) self.n_heads 8 self.out_dim_per_head 16\n",
            "MHA : split_heads t.shape torch.Size([2, 20, 128]) self.n_heads 8 self.out_dim_per_head 16\n",
            "MHA : After split_heads q.shape torch.Size([2, 20, 8, 16]) k.shape torch.Size([2, 20, 8, 16]) v.shape torch.Size([2, 20, 8, 16])\n",
            "MHA : After transpose(1,2) q.shape torch.Size([2, 8, 20, 16]) k.shape torch.Size([2, 8, 20, 16]) v.shape torch.Size([2, 8, 20, 16])\n",
            " Attention output : torch.Size([2, 8, 20, 16]) v : torch.Size([2, 8, 20, 16]) scores : torch.Size([2, 8, 20, 20])\n",
            "MHA : scores after attention torch.Size([2, 8, 20, 16])\n",
            "MHA : scores after view & transpose torch.Size([2, 20, 128])\n",
            "Encoder Layer x.shape after mha torch.Size([2, 20, 128]) mask None\n",
            "Encoder Layer x2.shape after norm2 torch.Size([2, 20, 128]) x.shape torch.Size([2, 20, 128])\n",
            "FeedForward before changes x.shape torch.Size([2, 20, 128])\n",
            "Encoder Layer x.shape after ff torch.Size([2, 20, 128]) \n",
            "Encoder Layer x2.shape after norm1 torch.Size([2, 20, 128]) x.shape torch.Size([2, 20, 128])\n",
            "MHA : x.shape torch.Size([2, 20, 128]) , qkv.shape torch.Size([2, 20, 384])\n",
            "MHA : q.shape torch.Size([2, 20, 128]) , self.out_dim 128\n",
            "MHA : k.shape torch.Size([2, 20, 128]) , self.out_dim*2 256\n",
            "MHA : v.shape torch.Size([2, 20, 128]) \n",
            "MHA : split_heads t.shape torch.Size([2, 20, 128]) self.n_heads 8 self.out_dim_per_head 16\n",
            "MHA : split_heads t.shape torch.Size([2, 20, 128]) self.n_heads 8 self.out_dim_per_head 16\n",
            "MHA : split_heads t.shape torch.Size([2, 20, 128]) self.n_heads 8 self.out_dim_per_head 16\n",
            "MHA : After split_heads q.shape torch.Size([2, 20, 8, 16]) k.shape torch.Size([2, 20, 8, 16]) v.shape torch.Size([2, 20, 8, 16])\n",
            "MHA : After transpose(1,2) q.shape torch.Size([2, 8, 20, 16]) k.shape torch.Size([2, 8, 20, 16]) v.shape torch.Size([2, 8, 20, 16])\n",
            " Attention output : torch.Size([2, 8, 20, 16]) v : torch.Size([2, 8, 20, 16]) scores : torch.Size([2, 8, 20, 20])\n",
            "MHA : scores after attention torch.Size([2, 8, 20, 16])\n",
            "MHA : scores after view & transpose torch.Size([2, 20, 128])\n",
            "Encoder Layer x.shape after mha torch.Size([2, 20, 128]) mask None\n",
            "Encoder Layer x2.shape after norm2 torch.Size([2, 20, 128]) x.shape torch.Size([2, 20, 128])\n",
            "FeedForward before changes x.shape torch.Size([2, 20, 128])\n",
            "Encoder Layer x.shape after ff torch.Size([2, 20, 128]) \n",
            "Encoder Layer x2.shape after norm1 torch.Size([2, 20, 128]) x.shape torch.Size([2, 20, 128])\n",
            "MHA : x.shape torch.Size([2, 20, 128]) , qkv.shape torch.Size([2, 20, 384])\n",
            "MHA : q.shape torch.Size([2, 20, 128]) , self.out_dim 128\n",
            "MHA : k.shape torch.Size([2, 20, 128]) , self.out_dim*2 256\n",
            "MHA : v.shape torch.Size([2, 20, 128]) \n",
            "MHA : split_heads t.shape torch.Size([2, 20, 128]) self.n_heads 8 self.out_dim_per_head 16\n",
            "MHA : split_heads t.shape torch.Size([2, 20, 128]) self.n_heads 8 self.out_dim_per_head 16\n",
            "MHA : split_heads t.shape torch.Size([2, 20, 128]) self.n_heads 8 self.out_dim_per_head 16\n",
            "MHA : After split_heads q.shape torch.Size([2, 20, 8, 16]) k.shape torch.Size([2, 20, 8, 16]) v.shape torch.Size([2, 20, 8, 16])\n",
            "MHA : After transpose(1,2) q.shape torch.Size([2, 8, 20, 16]) k.shape torch.Size([2, 8, 20, 16]) v.shape torch.Size([2, 8, 20, 16])\n",
            " Attention output : torch.Size([2, 8, 20, 16]) v : torch.Size([2, 8, 20, 16]) scores : torch.Size([2, 8, 20, 20])\n",
            "MHA : scores after attention torch.Size([2, 8, 20, 16])\n",
            "MHA : scores after view & transpose torch.Size([2, 20, 128])\n",
            "Encoder Layer x.shape after mha torch.Size([2, 20, 128]) mask None\n",
            "Encoder Layer x2.shape after norm2 torch.Size([2, 20, 128]) x.shape torch.Size([2, 20, 128])\n",
            "FeedForward before changes x.shape torch.Size([2, 20, 128])\n",
            "Encoder Layer x.shape after ff torch.Size([2, 20, 128]) \n",
            "Transformer after self.linear x.shape torch.Size([2, 20, 23948])\n",
            " output : torch.Size([2, 20, 23948])\n",
            " output_v : torch.Size([40, 23948])\n",
            " target_v : torch.Size([40])\n",
            " loss : <class 'torch.Tensor'> torch.Size([])\n",
            "it: 0  | loss nan  | Î”w: 0.0\n",
            "iteration - it : 1\n",
            "index : 1\n",
            " getitem len(s) < dataset.seq_len: 0 dataset.seq_len 20\n",
            "get_sentence_idx s gathering index - 1 : 10 - ['i', 'know', 'that', 'for', 'image', 'classification', 'we', 'use', 'summary', '.']\n",
            "get_sentence_idx s after vocab lookup: 10 - [5, 93, 12, 19, 1898, 23946, 45, 217, 13220, 1]\n",
            " getitem index % len(dataset) : 1 - 6\n",
            "get_sentence_idx s gathering index - 1 : 10 - ['i', 'know', 'that', 'for', 'image', 'classification', 'we', 'use', 'summary', '.']\n",
            "get_sentence_idx s after vocab lookup: 10 - [5, 93, 12, 19, 1898, 23946, 45, 217, 13220, 1]\n",
            " getitem dataset.get_sentence_idx(index % len(dataset)) : [5, 93, 12, 19, 1898, 23946, 45, 217, 13220, 1]\n",
            " getitem s value after extension : [5, 93, 12, 19, 1898, 23946, 45, 217, 13220, 1]\n",
            " getitem len(s) < dataset.seq_len: 10 dataset.seq_len 20\n",
            "get_sentence_idx s gathering index - 2 : 9 - ['what', 'dimensions', 'can', 'i', 'give', 'for', 'text', 'bert', '?']\n",
            "get_sentence_idx s after vocab lookup: 9 - [37, 10934, 114, 5, 107, 19, 4542, 23946, 14]\n",
            " getitem index % len(dataset) : 2 - 6\n",
            "get_sentence_idx s gathering index - 2 : 9 - ['what', 'dimensions', 'can', 'i', 'give', 'for', 'text', 'bert', '?']\n",
            "get_sentence_idx s after vocab lookup: 9 - [37, 10934, 114, 5, 107, 19, 4542, 23946, 14]\n",
            " getitem dataset.get_sentence_idx(index % len(dataset)) : [37, 10934, 114, 5, 107, 19, 4542, 23946, 14]\n",
            " getitem s value after extension : [5, 93, 12, 19, 1898, 23946, 45, 217, 13220, 1, 37, 10934, 114, 5, 107, 19, 4542, 23946, 14]\n",
            " getitem len(s) < dataset.seq_len: 19 dataset.seq_len 20\n",
            "get_sentence_idx s gathering index - 3 : 31 - ['in', 'this', 'python', 'tutorial', ',', 'we', 'will', 'learn', 'how', 'to', 'create', 'a', 'pytorch', 'model', 'summary', 'in', 'python', 'and', 'we', 'will', 'also', 'cover', 'different', 'examples', 'related', 'to', 'the', 'pytorch', 'model', 'summary', '.']\n",
            "get_sentence_idx s after vocab lookup: 31 - [13, 29, 23946, 23946, 0, 45, 36, 887, 75, 6, 3263, 9, 23946, 4731, 13220, 13, 23946, 4, 45, 36, 2061, 2034, 4783, 6696, 23946, 6, 3, 23946, 4731, 13220, 1]\n",
            " getitem index % len(dataset) : 3 - 6\n",
            "get_sentence_idx s gathering index - 3 : 31 - ['in', 'this', 'python', 'tutorial', ',', 'we', 'will', 'learn', 'how', 'to', 'create', 'a', 'pytorch', 'model', 'summary', 'in', 'python', 'and', 'we', 'will', 'also', 'cover', 'different', 'examples', 'related', 'to', 'the', 'pytorch', 'model', 'summary', '.']\n",
            "get_sentence_idx s after vocab lookup: 31 - [13, 29, 23946, 23946, 0, 45, 36, 887, 75, 6, 3263, 9, 23946, 4731, 13220, 13, 23946, 4, 45, 36, 2061, 2034, 4783, 6696, 23946, 6, 3, 23946, 4731, 13220, 1]\n",
            " getitem dataset.get_sentence_idx(index % len(dataset)) : [13, 29, 23946, 23946, 0, 45, 36, 887, 75, 6, 3263, 9, 23946, 4731, 13220, 13, 23946, 4, 45, 36, 2061, 2034, 4783, 6696, 23946, 6, 3, 23946, 4731, 13220, 1]\n",
            " getitem s value after extension : [5, 93, 12, 19, 1898, 23946, 45, 217, 13220, 1, 37, 10934, 114, 5, 107, 19, 4542, 23946, 14, 13, 29, 23946, 23946, 0, 45, 36, 887, 75, 6, 3263, 9, 23946, 4731, 13220, 13, 23946, 4, 45, 36, 2061, 2034, 4783, 6696, 23946, 6, 3, 23946, 4731, 13220, 1]\n",
            " getitem after len(s) < dataset.seq_len : 50 - [5, 93, 12, 19, 1898, 23946, 45, 217, 13220, 1, 37, 10934, 114, 5, 107, 19, 4542, 23946, 14, 13, 29, 23946, 23946, 0, 45, 36, 887, 75, 6, 3263, 9, 23946, 4731, 13220, 13, 23946, 4, 45, 36, 2061, 2034, 4783, 6696, 23946, 6, 3, 23946, 4731, 13220, 1]\n",
            " getitem s after checking seq_len : 20 - [5, 93, 12, 19, 1898, 23946, 45, 217, 13220, 1, 37, 10934, 114, 5, 107, 19, 4542, 23946, 14, 13]\n",
            " getitem s after IGNORE_IDX : 20 - [5, 93, 12, 19, 1898, 23946, 45, 217, 13220, 1, 37, 10934, 114, 5, 107, 19, 4542, 23946, 14, 13]\n",
            " getitem rand_rand 0.0005166739223376737 p_random_mask 0.15\n",
            " getitem s after apply random mask : 20 - [(23947, 5), (23947, 93), (23947, 12), (23947, 19), (23947, 1898), (23947, 23946), (23947, 45), (23947, 217), (23947, 13220), (23947, 1), (23947, 37), (23947, 10934), (23947, 114), (23947, 5), (23947, 107), (23947, 19), (23947, 4542), (23947, 23946), (23947, 14), (23947, 13)]\n",
            " getitem data_dict : {'input': tensor([23947, 23947, 23947, 23947, 23947, 23947, 23947, 23947, 23947, 23947,\n",
            "        23947, 23947, 23947, 23947, 23947, 23947, 23947, 23947, 23947, 23947]), 'target': tensor([    5,    93,    12,    19,  1898, 23946,    45,   217, 13220,     1,\n",
            "           37, 10934,   114,     5,   107,    19,  4542, 23946,    14,    13])}\n",
            "index : 4\n",
            " getitem len(s) < dataset.seq_len: 0 dataset.seq_len 20\n",
            "get_sentence_idx s gathering index - 4 : 8 - ['moreover', ',', 'we', 'will', 'cover', 'these', 'topics', '.']\n",
            "get_sentence_idx s after vocab lookup: 8 - [3650, 0, 45, 36, 2034, 111, 23946, 1]\n",
            " getitem index % len(dataset) : 4 - 6\n",
            "get_sentence_idx s gathering index - 4 : 8 - ['moreover', ',', 'we', 'will', 'cover', 'these', 'topics', '.']\n",
            "get_sentence_idx s after vocab lookup: 8 - [3650, 0, 45, 36, 2034, 111, 23946, 1]\n",
            " getitem dataset.get_sentence_idx(index % len(dataset)) : [3650, 0, 45, 36, 2034, 111, 23946, 1]\n",
            " getitem s value after extension : [3650, 0, 45, 36, 2034, 111, 23946, 1]\n",
            " getitem len(s) < dataset.seq_len: 8 dataset.seq_len 20\n",
            "get_sentence_idx s gathering index - 5 : 0 - []\n",
            "get_sentence_idx s after vocab lookup: 0 - []\n",
            " getitem index % len(dataset) : 5 - 6\n",
            "get_sentence_idx s gathering index - 5 : 0 - []\n",
            "get_sentence_idx s after vocab lookup: 0 - []\n",
            " getitem dataset.get_sentence_idx(index % len(dataset)) : []\n",
            " getitem s value after extension : [3650, 0, 45, 36, 2034, 111, 23946, 1]\n",
            " getitem len(s) < dataset.seq_len: 8 dataset.seq_len 20\n",
            "get_sentence_idx s gathering index - 0 : 14 - ['i', 'would', 'like', 'to', 'print', 'my', 'bert', 'model', 'summary', '(', 'text', 'classification', ')', '.']\n",
            "get_sentence_idx s after vocab lookup: 14 - [5, 68, 89, 6, 3234, 11, 23946, 4731, 13220, 190, 4542, 23946, 189, 1]\n",
            " getitem index % len(dataset) : 0 - 6\n",
            "get_sentence_idx s gathering index - 0 : 14 - ['i', 'would', 'like', 'to', 'print', 'my', 'bert', 'model', 'summary', '(', 'text', 'classification', ')', '.']\n",
            "get_sentence_idx s after vocab lookup: 14 - [5, 68, 89, 6, 3234, 11, 23946, 4731, 13220, 190, 4542, 23946, 189, 1]\n",
            " getitem dataset.get_sentence_idx(index % len(dataset)) : [5, 68, 89, 6, 3234, 11, 23946, 4731, 13220, 190, 4542, 23946, 189, 1]\n",
            " getitem s value after extension : [3650, 0, 45, 36, 2034, 111, 23946, 1, 5, 68, 89, 6, 3234, 11, 23946, 4731, 13220, 190, 4542, 23946, 189, 1]\n",
            " getitem after len(s) < dataset.seq_len : 22 - [3650, 0, 45, 36, 2034, 111, 23946, 1, 5, 68, 89, 6, 3234, 11, 23946, 4731, 13220, 190, 4542, 23946, 189, 1]\n",
            " getitem s after checking seq_len : 20 - [3650, 0, 45, 36, 2034, 111, 23946, 1, 5, 68, 89, 6, 3234, 11, 23946, 4731, 13220, 190, 4542, 23946]\n",
            " getitem s after IGNORE_IDX : 20 - [3650, 0, 45, 36, 2034, 111, 23946, 1, 5, 68, 89, 6, 3234, 11, 23946, 4731, 13220, 190, 4542, 23946]\n",
            " getitem rand_rand 0.0121768051368214 p_random_mask 0.15\n",
            " getitem s after apply random mask : 20 - [(23947, 3650), (23947, 0), (23947, 45), (23947, 36), (23947, 2034), (23947, 111), (23947, 23946), (23947, 1), (23947, 5), (23947, 68), (23947, 89), (23947, 6), (23947, 3234), (23947, 11), (23947, 23946), (23947, 4731), (23947, 13220), (23947, 190), (23947, 4542), (23947, 23946)]\n",
            " getitem data_dict : {'input': tensor([23947, 23947, 23947, 23947, 23947, 23947, 23947, 23947, 23947, 23947,\n",
            "        23947, 23947, 23947, 23947, 23947, 23947, 23947, 23947, 23947, 23947]), 'target': tensor([ 3650,     0,    45,    36,  2034,   111, 23946,     1,     5,    68,\n",
            "           89,     6,  3234,    11, 23946,  4731, 13220,   190,  4542, 23946])}\n",
            " masked_input : 2 - tensor([[23947, 23947, 23947, 23947, 23947, 23947, 23947, 23947, 23947, 23947,\n",
            "         23947, 23947, 23947, 23947, 23947, 23947, 23947, 23947, 23947, 23947],\n",
            "        [23947, 23947, 23947, 23947, 23947, 23947, 23947, 23947, 23947, 23947,\n",
            "         23947, 23947, 23947, 23947, 23947, 23947, 23947, 23947, 23947, 23947]])\n",
            " masked_target : 2 - tensor([[    5,    93,    12,    19,  1898, 23946,    45,   217, 13220,     1,\n",
            "            37, 10934,   114,     5,   107,    19,  4542, 23946,    14,    13],\n",
            "        [ 3650,     0,    45,    36,  2034,   111, 23946,     1,     5,    68,\n",
            "            89,     6,  3234,    11, 23946,  4731, 13220,   190,  4542, 23946]])\n",
            "Transformer x.shape torch.Size([2, 20])\n",
            "Transformer after self.embeddings x.shape torch.Size([2, 20, 128])\n",
            "PositionalEmbedding self.pe.shape torch.Size([1, 20, 128]) x.shape torch.Size([2, 20, 128])\n",
            "PositionalEmbedding self.pe.shape torch.Size([1, 20, 128]) x.shape torch.Size([2, 20, 128])\n",
            "Transformer after pe addition x.shape torch.Size([2, 20, 128]) pe.shape torch.Size([1, 20, 128])\n",
            "Encoder Layer x2.shape after norm1 torch.Size([2, 20, 128]) x.shape torch.Size([2, 20, 128])\n",
            "MHA : x.shape torch.Size([2, 20, 128]) , qkv.shape torch.Size([2, 20, 384])\n",
            "MHA : q.shape torch.Size([2, 20, 128]) , self.out_dim 128\n",
            "MHA : k.shape torch.Size([2, 20, 128]) , self.out_dim*2 256\n",
            "MHA : v.shape torch.Size([2, 20, 128]) \n",
            "MHA : split_heads t.shape torch.Size([2, 20, 128]) self.n_heads 8 self.out_dim_per_head 16\n",
            "MHA : split_heads t.shape torch.Size([2, 20, 128]) self.n_heads 8 self.out_dim_per_head 16\n",
            "MHA : split_heads t.shape torch.Size([2, 20, 128]) self.n_heads 8 self.out_dim_per_head 16\n",
            "MHA : After split_heads q.shape torch.Size([2, 20, 8, 16]) k.shape torch.Size([2, 20, 8, 16]) v.shape torch.Size([2, 20, 8, 16])\n",
            "MHA : After transpose(1,2) q.shape torch.Size([2, 8, 20, 16]) k.shape torch.Size([2, 8, 20, 16]) v.shape torch.Size([2, 8, 20, 16])\n",
            " Attention output : torch.Size([2, 8, 20, 16]) v : torch.Size([2, 8, 20, 16]) scores : torch.Size([2, 8, 20, 20])\n",
            "MHA : scores after attention torch.Size([2, 8, 20, 16])\n",
            "MHA : scores after view & transpose torch.Size([2, 20, 128])\n",
            "Encoder Layer x.shape after mha torch.Size([2, 20, 128]) mask None\n",
            "Encoder Layer x2.shape after norm2 torch.Size([2, 20, 128]) x.shape torch.Size([2, 20, 128])\n",
            "FeedForward before changes x.shape torch.Size([2, 20, 128])\n",
            "Encoder Layer x.shape after ff torch.Size([2, 20, 128]) \n",
            "Encoder Layer x2.shape after norm1 torch.Size([2, 20, 128]) x.shape torch.Size([2, 20, 128])\n",
            "MHA : x.shape torch.Size([2, 20, 128]) , qkv.shape torch.Size([2, 20, 384])\n",
            "MHA : q.shape torch.Size([2, 20, 128]) , self.out_dim 128\n",
            "MHA : k.shape torch.Size([2, 20, 128]) , self.out_dim*2 256\n",
            "MHA : v.shape torch.Size([2, 20, 128]) \n",
            "MHA : split_heads t.shape torch.Size([2, 20, 128]) self.n_heads 8 self.out_dim_per_head 16\n",
            "MHA : split_heads t.shape torch.Size([2, 20, 128]) self.n_heads 8 self.out_dim_per_head 16\n",
            "MHA : split_heads t.shape torch.Size([2, 20, 128]) self.n_heads 8 self.out_dim_per_head 16\n",
            "MHA : After split_heads q.shape torch.Size([2, 20, 8, 16]) k.shape torch.Size([2, 20, 8, 16]) v.shape torch.Size([2, 20, 8, 16])\n",
            "MHA : After transpose(1,2) q.shape torch.Size([2, 8, 20, 16]) k.shape torch.Size([2, 8, 20, 16]) v.shape torch.Size([2, 8, 20, 16])\n",
            " Attention output : torch.Size([2, 8, 20, 16]) v : torch.Size([2, 8, 20, 16]) scores : torch.Size([2, 8, 20, 20])\n",
            "MHA : scores after attention torch.Size([2, 8, 20, 16])\n",
            "MHA : scores after view & transpose torch.Size([2, 20, 128])\n",
            "Encoder Layer x.shape after mha torch.Size([2, 20, 128]) mask None\n",
            "Encoder Layer x2.shape after norm2 torch.Size([2, 20, 128]) x.shape torch.Size([2, 20, 128])\n",
            "FeedForward before changes x.shape torch.Size([2, 20, 128])\n",
            "Encoder Layer x.shape after ff torch.Size([2, 20, 128]) \n",
            "Encoder Layer x2.shape after norm1 torch.Size([2, 20, 128]) x.shape torch.Size([2, 20, 128])\n",
            "MHA : x.shape torch.Size([2, 20, 128]) , qkv.shape torch.Size([2, 20, 384])\n",
            "MHA : q.shape torch.Size([2, 20, 128]) , self.out_dim 128\n",
            "MHA : k.shape torch.Size([2, 20, 128]) , self.out_dim*2 256\n",
            "MHA : v.shape torch.Size([2, 20, 128]) \n",
            "MHA : split_heads t.shape torch.Size([2, 20, 128]) self.n_heads 8 self.out_dim_per_head 16\n",
            "MHA : split_heads t.shape torch.Size([2, 20, 128]) self.n_heads 8 self.out_dim_per_head 16\n",
            "MHA : split_heads t.shape torch.Size([2, 20, 128]) self.n_heads 8 self.out_dim_per_head 16\n",
            "MHA : After split_heads q.shape torch.Size([2, 20, 8, 16]) k.shape torch.Size([2, 20, 8, 16]) v.shape torch.Size([2, 20, 8, 16])\n",
            "MHA : After transpose(1,2) q.shape torch.Size([2, 8, 20, 16]) k.shape torch.Size([2, 8, 20, 16]) v.shape torch.Size([2, 8, 20, 16])\n",
            " Attention output : torch.Size([2, 8, 20, 16]) v : torch.Size([2, 8, 20, 16]) scores : torch.Size([2, 8, 20, 20])\n",
            "MHA : scores after attention torch.Size([2, 8, 20, 16])\n",
            "MHA : scores after view & transpose torch.Size([2, 20, 128])\n",
            "Encoder Layer x.shape after mha torch.Size([2, 20, 128]) mask None\n",
            "Encoder Layer x2.shape after norm2 torch.Size([2, 20, 128]) x.shape torch.Size([2, 20, 128])\n",
            "FeedForward before changes x.shape torch.Size([2, 20, 128])\n",
            "Encoder Layer x.shape after ff torch.Size([2, 20, 128]) \n",
            "Encoder Layer x2.shape after norm1 torch.Size([2, 20, 128]) x.shape torch.Size([2, 20, 128])\n",
            "MHA : x.shape torch.Size([2, 20, 128]) , qkv.shape torch.Size([2, 20, 384])\n",
            "MHA : q.shape torch.Size([2, 20, 128]) , self.out_dim 128\n",
            "MHA : k.shape torch.Size([2, 20, 128]) , self.out_dim*2 256\n",
            "MHA : v.shape torch.Size([2, 20, 128]) \n",
            "MHA : split_heads t.shape torch.Size([2, 20, 128]) self.n_heads 8 self.out_dim_per_head 16\n",
            "MHA : split_heads t.shape torch.Size([2, 20, 128]) self.n_heads 8 self.out_dim_per_head 16\n",
            "MHA : split_heads t.shape torch.Size([2, 20, 128]) self.n_heads 8 self.out_dim_per_head 16\n",
            "MHA : After split_heads q.shape torch.Size([2, 20, 8, 16]) k.shape torch.Size([2, 20, 8, 16]) v.shape torch.Size([2, 20, 8, 16])\n",
            "MHA : After transpose(1,2) q.shape torch.Size([2, 8, 20, 16]) k.shape torch.Size([2, 8, 20, 16]) v.shape torch.Size([2, 8, 20, 16])\n",
            " Attention output : torch.Size([2, 8, 20, 16]) v : torch.Size([2, 8, 20, 16]) scores : torch.Size([2, 8, 20, 20])\n",
            "MHA : scores after attention torch.Size([2, 8, 20, 16])\n",
            "MHA : scores after view & transpose torch.Size([2, 20, 128])\n",
            "Encoder Layer x.shape after mha torch.Size([2, 20, 128]) mask None\n",
            "Encoder Layer x2.shape after norm2 torch.Size([2, 20, 128]) x.shape torch.Size([2, 20, 128])\n",
            "FeedForward before changes x.shape torch.Size([2, 20, 128])\n",
            "Encoder Layer x.shape after ff torch.Size([2, 20, 128]) \n",
            "Encoder Layer x2.shape after norm1 torch.Size([2, 20, 128]) x.shape torch.Size([2, 20, 128])\n",
            "MHA : x.shape torch.Size([2, 20, 128]) , qkv.shape torch.Size([2, 20, 384])\n",
            "MHA : q.shape torch.Size([2, 20, 128]) , self.out_dim 128\n",
            "MHA : k.shape torch.Size([2, 20, 128]) , self.out_dim*2 256\n",
            "MHA : v.shape torch.Size([2, 20, 128]) \n",
            "MHA : split_heads t.shape torch.Size([2, 20, 128]) self.n_heads 8 self.out_dim_per_head 16\n",
            "MHA : split_heads t.shape torch.Size([2, 20, 128]) self.n_heads 8 self.out_dim_per_head 16\n",
            "MHA : split_heads t.shape torch.Size([2, 20, 128]) self.n_heads 8 self.out_dim_per_head 16\n",
            "MHA : After split_heads q.shape torch.Size([2, 20, 8, 16]) k.shape torch.Size([2, 20, 8, 16]) v.shape torch.Size([2, 20, 8, 16])\n",
            "MHA : After transpose(1,2) q.shape torch.Size([2, 8, 20, 16]) k.shape torch.Size([2, 8, 20, 16]) v.shape torch.Size([2, 8, 20, 16])\n",
            " Attention output : torch.Size([2, 8, 20, 16]) v : torch.Size([2, 8, 20, 16]) scores : torch.Size([2, 8, 20, 20])\n",
            "MHA : scores after attention torch.Size([2, 8, 20, 16])\n",
            "MHA : scores after view & transpose torch.Size([2, 20, 128])\n",
            "Encoder Layer x.shape after mha torch.Size([2, 20, 128]) mask None\n",
            "Encoder Layer x2.shape after norm2 torch.Size([2, 20, 128]) x.shape torch.Size([2, 20, 128])\n",
            "FeedForward before changes x.shape torch.Size([2, 20, 128])\n",
            "Encoder Layer x.shape after ff torch.Size([2, 20, 128]) \n",
            "Encoder Layer x2.shape after norm1 torch.Size([2, 20, 128]) x.shape torch.Size([2, 20, 128])\n",
            "MHA : x.shape torch.Size([2, 20, 128]) , qkv.shape torch.Size([2, 20, 384])\n",
            "MHA : q.shape torch.Size([2, 20, 128]) , self.out_dim 128\n",
            "MHA : k.shape torch.Size([2, 20, 128]) , self.out_dim*2 256\n",
            "MHA : v.shape torch.Size([2, 20, 128]) \n",
            "MHA : split_heads t.shape torch.Size([2, 20, 128]) self.n_heads 8 self.out_dim_per_head 16\n",
            "MHA : split_heads t.shape torch.Size([2, 20, 128]) self.n_heads 8 self.out_dim_per_head 16\n",
            "MHA : split_heads t.shape torch.Size([2, 20, 128]) self.n_heads 8 self.out_dim_per_head 16\n",
            "MHA : After split_heads q.shape torch.Size([2, 20, 8, 16]) k.shape torch.Size([2, 20, 8, 16]) v.shape torch.Size([2, 20, 8, 16])\n",
            "MHA : After transpose(1,2) q.shape torch.Size([2, 8, 20, 16]) k.shape torch.Size([2, 8, 20, 16]) v.shape torch.Size([2, 8, 20, 16])\n",
            " Attention output : torch.Size([2, 8, 20, 16]) v : torch.Size([2, 8, 20, 16]) scores : torch.Size([2, 8, 20, 20])\n",
            "MHA : scores after attention torch.Size([2, 8, 20, 16])\n",
            "MHA : scores after view & transpose torch.Size([2, 20, 128])\n",
            "Encoder Layer x.shape after mha torch.Size([2, 20, 128]) mask None\n",
            "Encoder Layer x2.shape after norm2 torch.Size([2, 20, 128]) x.shape torch.Size([2, 20, 128])\n",
            "FeedForward before changes x.shape torch.Size([2, 20, 128])\n",
            "Encoder Layer x.shape after ff torch.Size([2, 20, 128]) \n",
            "Encoder Layer x2.shape after norm1 torch.Size([2, 20, 128]) x.shape torch.Size([2, 20, 128])\n",
            "MHA : x.shape torch.Size([2, 20, 128]) , qkv.shape torch.Size([2, 20, 384])\n",
            "MHA : q.shape torch.Size([2, 20, 128]) , self.out_dim 128\n",
            "MHA : k.shape torch.Size([2, 20, 128]) , self.out_dim*2 256\n",
            "MHA : v.shape torch.Size([2, 20, 128]) \n",
            "MHA : split_heads t.shape torch.Size([2, 20, 128]) self.n_heads 8 self.out_dim_per_head 16\n",
            "MHA : split_heads t.shape torch.Size([2, 20, 128]) self.n_heads 8 self.out_dim_per_head 16\n",
            "MHA : split_heads t.shape torch.Size([2, 20, 128]) self.n_heads 8 self.out_dim_per_head 16\n",
            "MHA : After split_heads q.shape torch.Size([2, 20, 8, 16]) k.shape torch.Size([2, 20, 8, 16]) v.shape torch.Size([2, 20, 8, 16])\n",
            "MHA : After transpose(1,2) q.shape torch.Size([2, 8, 20, 16]) k.shape torch.Size([2, 8, 20, 16]) v.shape torch.Size([2, 8, 20, 16])\n",
            " Attention output : torch.Size([2, 8, 20, 16]) v : torch.Size([2, 8, 20, 16]) scores : torch.Size([2, 8, 20, 20])\n",
            "MHA : scores after attention torch.Size([2, 8, 20, 16])\n",
            "MHA : scores after view & transpose torch.Size([2, 20, 128])\n",
            "Encoder Layer x.shape after mha torch.Size([2, 20, 128]) mask None\n",
            "Encoder Layer x2.shape after norm2 torch.Size([2, 20, 128]) x.shape torch.Size([2, 20, 128])\n",
            "FeedForward before changes x.shape torch.Size([2, 20, 128])\n",
            "Encoder Layer x.shape after ff torch.Size([2, 20, 128]) \n",
            "Encoder Layer x2.shape after norm1 torch.Size([2, 20, 128]) x.shape torch.Size([2, 20, 128])\n",
            "MHA : x.shape torch.Size([2, 20, 128]) , qkv.shape torch.Size([2, 20, 384])\n",
            "MHA : q.shape torch.Size([2, 20, 128]) , self.out_dim 128\n",
            "MHA : k.shape torch.Size([2, 20, 128]) , self.out_dim*2 256\n",
            "MHA : v.shape torch.Size([2, 20, 128]) \n",
            "MHA : split_heads t.shape torch.Size([2, 20, 128]) self.n_heads 8 self.out_dim_per_head 16\n",
            "MHA : split_heads t.shape torch.Size([2, 20, 128]) self.n_heads 8 self.out_dim_per_head 16\n",
            "MHA : split_heads t.shape torch.Size([2, 20, 128]) self.n_heads 8 self.out_dim_per_head 16\n",
            "MHA : After split_heads q.shape torch.Size([2, 20, 8, 16]) k.shape torch.Size([2, 20, 8, 16]) v.shape torch.Size([2, 20, 8, 16])\n",
            "MHA : After transpose(1,2) q.shape torch.Size([2, 8, 20, 16]) k.shape torch.Size([2, 8, 20, 16]) v.shape torch.Size([2, 8, 20, 16])\n",
            " Attention output : torch.Size([2, 8, 20, 16]) v : torch.Size([2, 8, 20, 16]) scores : torch.Size([2, 8, 20, 20])\n",
            "MHA : scores after attention torch.Size([2, 8, 20, 16])\n",
            "MHA : scores after view & transpose torch.Size([2, 20, 128])\n",
            "Encoder Layer x.shape after mha torch.Size([2, 20, 128]) mask None\n",
            "Encoder Layer x2.shape after norm2 torch.Size([2, 20, 128]) x.shape torch.Size([2, 20, 128])\n",
            "FeedForward before changes x.shape torch.Size([2, 20, 128])\n",
            "Encoder Layer x.shape after ff torch.Size([2, 20, 128]) \n",
            "Transformer after self.linear x.shape torch.Size([2, 20, 23948])\n",
            " output : torch.Size([2, 20, 23948])\n",
            " output_v : torch.Size([40, 23948])\n",
            " target_v : torch.Size([40])\n",
            " loss : <class 'torch.Tensor'> torch.Size([])\n",
            "loss : 9.222759246826172\n",
            "it: 1  | loss 9.22  | Î”w: 0.554\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# Results analysis\n",
        "# =============================================================================\n",
        "print('saving embeddings...')\n",
        "N = 3000\n",
        "np.savetxt('values.tsv', np.round(model.embeddings.weight.detach().cpu().numpy()[0:N], 2), delimiter='\\t', fmt='%1.2f')\n",
        "s = [dataset.rvocab[i] for i in range(N)]\n",
        "open('names.tsv', 'w+').write('\\n'.join(s) )\n",
        "\n",
        "\n",
        "print('end')"
      ],
      "metadata": {
        "id": "7R4odNl1iZNV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        },
        "outputId": "36b46a41-fe43-48fe-c0e0-d89eebbcb6cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "saving embeddings...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-deca236fe23e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'saving embeddings...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavetxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'values.tsv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfmt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'%1.2f'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'names.tsv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w+'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "i6fEuiJsiayR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}